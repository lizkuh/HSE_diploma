{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4b774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/root/HSE_diploma/\")\n",
    "sys.path.append(\"/root/HSE_diploma/evaluator/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c28024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 26 12:18:25 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\r\n",
      "| 30%   25C    P8    15W / 350W |      1MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# t2c_concode_220428_v38_plusoneepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a26c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/HSE_diploma/prompter/templates/\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import load_model_tokenizer_from_pretrained, draw_metrics_compare_with_glue\n",
    "from ParamsIterator import ParamsIterator\n",
    "from EvaluateTestSet import EvaluateTestSet\n",
    "import pandas as pd\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4369c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_model = \"yahma/llama-7b-hf\"\n",
    "# experiment_name = \"t2c_concode_220428_v38_plusoneepoch\"\n",
    "\n",
    "experiment_name = \"/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/\"\n",
    "default_model = json.load(open(\"/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/experiment_config.json\", \"r\"))['default_model']\n",
    "params_iteration = {\"temperature\": [1.0],\n",
    "                    \"max_new_tokens\": [300]#None, 20, 30, 35, 40, 50, 60, 70, 80, 90, 100] + [45, 47, 49, 51, 53, 55]\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6614c5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yahma/llama-7b-hf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd57c17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/',\n",
       " {'temperature': [1.0], 'max_new_tokens': [300]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name, params_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a83cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20463487c52e495fabb912e3d0aa3f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model_tokenizer_from_pretrained(default_model = default_model, \n",
    "                                                        experiment_name = experiment_name\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849eeed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id 0\n",
      "tokenizer.eos_token_id 2\n",
      "tokenizer.bos_token_id 1\n",
      "tokenizer.eos_token_id 2\n",
      "model.config.pad_token_id 0\n",
      "model.config.eos_token_id 2\n",
      "model.config.bos_token_id 1\n",
      "model.config.eos_token_id 2\n",
      "trainable params: 0 || all params: 7543721984 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "def verbose_model_tokenizer(model, tokenizer):\n",
    "    print(\"tokenizer.pad_token_id\", tokenizer.pad_token_id)\n",
    "    print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "    print(\"tokenizer.bos_token_id\", tokenizer.bos_token_id)\n",
    "    print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "\n",
    "    print(\"model.config.pad_token_id\", model.config.pad_token_id)\n",
    "    print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "    print(\"model.config.bos_token_id\", model.config.bos_token_id)\n",
    "    print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "verbose_model_tokenizer(model, tokenizer)\n",
    "\n",
    "# tokenizer.pad_token_id 0\n",
    "# tokenizer.eos_token_id 2\n",
    "# tokenizer.bos_token_id 1\n",
    "# tokenizer.eos_token_id 2\n",
    "# model.config.pad_token_id 0\n",
    "# model.config.eos_token_id 2\n",
    "# model.config.bos_token_id 1\n",
    "# model.config.eos_token_id 2\n",
    "# trainable params: 0 || all params: 6746804224 || trainable%: 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeb449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def get_metric_res(model, tokenizer, params_iteration, experiment_name):\n",
    "    res = []\n",
    "    for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n",
    "        evaluator = EvaluateTestSet(generation_config = GenerationConfig(**generation_config_dict\n",
    "                                                                        ),\n",
    "                                    \n",
    "                                    #fn_test_data = \"temp/t2c_answers.json\",\n",
    "                                    #fn_etalon = \"temp/answers.json\"\n",
    "                                   )\n",
    "\n",
    "        metric_res = evaluator.evaluate(model=model, \n",
    "                                        tokenizer=tokenizer                                        \n",
    "                                       )\n",
    "        for key, val in generation_config_dict.items():\n",
    "            assert key not in metric_res\n",
    "            metric_res[key] = val\n",
    "\n",
    "        metric_res['experiment_name'] = experiment_name\n",
    "        print(generation_config_dict, metric_res)\n",
    "\n",
    "        res.append(metric_res)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6d34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = model.config.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40d9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da30e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import GenerationConfig\n",
    "\n",
    "# from datetime import datetime\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import tqdm\n",
    "\n",
    "# from T2CEvaluator import T2CEvaluator\n",
    "\n",
    "# from prompter import generate_test_prompt, get_response\n",
    "# # prompter = Prompter()\n",
    "\n",
    "# # def generate_test_prompt(data_point):\n",
    "# #     #assert 'output' not in data_point or data_point['output']==''\n",
    "# #     if \"input\" in data_point and data_point[\"input\"]:\n",
    "# #         return prompter.generate_prompt(instruction = data_point[\"instruction\"],\n",
    "# #                                         input = data_point[\"input\"],\n",
    "# #                                         #label = ''#data_point[\"output\"]\n",
    "# #                                        )\n",
    "# #     else:\n",
    "# #         return prompter.generate_prompt(instruction = data_point[\"instruction\"],\n",
    "# #                                         #input = None,\n",
    "# #                                         #label = ''#data_point[\"output\"]\n",
    "# #                                        )\n",
    "   \n",
    "# # def generate_test_prompt(data_point, train = False):\n",
    "# #     # To decrease expectations of results :)\n",
    "# #     assert train == False\n",
    "# #     # sorry about the formatting disaster gotta move fast\n",
    "# #     if data_point[\"input\"]:\n",
    "# #         return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "# # ### Instruction:\n",
    "# # {data_point[\"instruction\"]}\n",
    "\n",
    "# # ### Input:\n",
    "# # {data_point[\"input\"]}\n",
    "\n",
    "# # ### Response:\n",
    "# # {data_point[\"output\"] if train else ''}\"\"\"\n",
    "# #     else:\n",
    "# #         return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# # ### Instruction:\n",
    "# # {data_point[\"instruction\"]}\n",
    "\n",
    "# # ### Response:\n",
    "# # {data_point[\"output\"] if train else ''}\"\"\"\n",
    "\n",
    "\n",
    "# class EvaluateTestSet:\n",
    "#     def __init__(self, \n",
    "#                  generation_config = GenerationConfig(max_new_tokens = 128), \n",
    "#                  fn_test_data = \"../data/t2c_answers.json\",\n",
    "#                  fn_etalon = \"/root/data/answers.json\",\n",
    "#                  batch_size = 10,\n",
    "#                  verbose = False\n",
    "#                 ):\n",
    "#         self.generation_config = generation_config\n",
    "        \n",
    "#         self.fn_test_data = fn_test_data\n",
    "#         self.fn_etalon = fn_etalon\n",
    "        \n",
    "#         self.batch_size = batch_size\n",
    "#         self.verbose = verbose\n",
    "       \n",
    "#     def preprocess(self, s):\n",
    "#         #ToDo rewrite it using Promt Template\n",
    "#         s = get_response(s)\n",
    "#         #s = s.split('### Response:\\n')[-1]\n",
    "#         s = s.replace('\\n', '  ')\n",
    "#         s = s.replace('<unk>', \" \")\n",
    "#         s = ' '.join(s.split(' ')[:100])\n",
    "#         while '  ' in s:\n",
    "#             s = s.replace('  ', ' ')\n",
    "\n",
    "#         if len(s) > 0 and s[0] == ' ':\n",
    "#             s = s[1:]\n",
    "        \n",
    "#         if self.verbose:\n",
    "#             print(s)\n",
    "        \n",
    "#         return s\n",
    "\n",
    "#     def clean_results(self, res_list):\n",
    "#         predict_list = []\n",
    "#         for s in tqdm.tqdm(res_list):\n",
    "#             predict_list.append(self.preprocess(s))\n",
    "#         return predict_list\n",
    "    \n",
    "#     def get_raw_results(self, model, tokenizer, prompts):\n",
    "#         batch_size = self.batch_size\n",
    "#         generation_config = self.generation_config\n",
    "        \n",
    "#         res_list = []\n",
    "#         n = math.ceil(len(prompts)/batch_size)\n",
    "#         n = 1\n",
    "#         for ind in tqdm.tqdm(range(n)):\n",
    "#             current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "#             if self.verbose:\n",
    "#                 print(ind * batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "\n",
    "#             tokenized_inputs = tokenizer(list(current_prompts), \n",
    "#                                          padding=True, \n",
    "#                                          truncation=True, \n",
    "#                                          return_tensors=\"pt\"\n",
    "#                                         ).to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 full_output = model.generate(\n",
    "#                     **tokenized_inputs,\n",
    "#                     generation_config=generation_config\n",
    "#                 )\n",
    "\n",
    "#             res_list.extend(tokenizer.batch_decode(full_output, skip_special_tokens=False))\n",
    "#             self.res_list = res_list\n",
    "#         return res_list\n",
    "    \n",
    "#     def save_results(self, predict_list):\n",
    "#         output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "#         fn_output = \"/root/results/%s\"%output_filename\n",
    "        \n",
    "#         res = '\\n'.join([i if i!='' else '-' for i in predict_list])\n",
    "#         open(fn_output, \"w+\", encoding='utf-8').write(res)\n",
    "#         return fn_output\n",
    "    \n",
    "#     def evaluate(self, model, tokenizer):\n",
    "#         model.eval()\n",
    "#         assert model.training == False\n",
    "\n",
    "#         lst = json.load(open(self.fn_test_data, 'rb'))\n",
    "#         inputs = lst# [lst[0]]\n",
    "#         # instruction = 'Combine the question and answer into an image caption as succinctly as possible. Be sure to include the phrase \"a photo of\". Do not draw false conclusions.'\n",
    "#         # inputs = ['Is this a baseball game? yes', 'Is this a baseball game? no']\n",
    "#         prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "#         prompts = np.array(prompts)\n",
    "        \n",
    "#         res_list = self.get_raw_results(model = model, \n",
    "#                                         tokenizer = tokenizer,\n",
    "#                                         prompts = prompts)\n",
    "        \n",
    "#         model.train()\n",
    "#         assert model.training == True\n",
    "        \n",
    "#         predict_list = self.clean_results(res_list)\n",
    "        \n",
    "#         self.fn_output = self.save_results(predict_list)\n",
    "        \n",
    "#         t2c_evaluator = T2CEvaluator()\n",
    "#         metric_res = t2c_evaluator.calculate_metrics(fn_answers = self.fn_etalon, \n",
    "#                                                      fn_predictions = self.fn_output\n",
    "#                                                      )\n",
    "#         return metric_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d36c0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd614338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5059/272965761.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214a124cdb4a48e582aebcdebea0ccf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:58<08:47, 58.61s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [01:53<07:31, 56.47s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [02:53<06:45, 57.88s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [03:52<05:50, 58.35s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [04:50<04:51, 58.31s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [05:41<03:43, 55.92s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [06:38<02:48, 56.22s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [07:36<01:53, 56.82s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [08:34<00:57, 57.04s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [09:33<00:00, 57.31s/it]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 44002.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.03065082934202577, 'brevity_penalty': 1.0, 'ratio': 1.3741956241956241, 'translation_length': 4271, 'reference_length': 3108, 'precisions_0': 0.07279962546816479, 'precisions_1': 0.037152444870565675, 'precisions_2': 0.022347740667976426, 'precisions_3': 0.01460221550855992, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = None\n",
    "res = []\n",
    "for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n",
    "    evaluator = EvaluateTestSet(generation_config = GenerationConfig(**generation_config_dict\n",
    "                                                                    ),\n",
    "\n",
    "                                #fn_test_data = \"temp/t2c_answers.json\",\n",
    "                                #fn_etalon = \"temp/answers.json\"\n",
    "                               )\n",
    "\n",
    "    metric_res = evaluator.evaluate(model=model, \n",
    "                                    tokenizer=tokenizer                                        \n",
    "                                   )\n",
    "    for key, val in generation_config_dict.items():\n",
    "        assert key not in metric_res\n",
    "        metric_res[key] = val\n",
    "\n",
    "    metric_res['experiment_name'] = experiment_name\n",
    "    print(generation_config_dict, metric_res)\n",
    "\n",
    "    res.append(metric_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5910ab9",
   "metadata": {},
   "source": [
    "{'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.03065082934202577, 'brevity_penalty': 1.0, 'ratio': 1.3741956241956241, 'translation_length': 4271, 'reference_length': 3108, 'precisions_0': 0.07279962546816479, 'precisions_1': 0.037152444870565675, 'precisions_2': 0.022347740667976426, 'precisions_3': 0.01460221550855992, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v38_plustwoepoch/'}from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc4a3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5005/1950585116.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b004c0c318c04cf7abebe758020dc4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:57<08:37, 57.52s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [01:51<07:21, 55.18s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [02:48<06:34, 56.30s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [03:45<05:39, 56.54s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [04:42<04:42, 56.51s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [05:32<03:37, 54.34s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [06:28<02:44, 54.87s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [07:24<01:50, 55.46s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [08:21<00:55, 55.90s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [09:18<00:00, 55.88s/it]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 45095.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.03065082934202577, 'brevity_penalty': 1.0, 'ratio': 1.3741956241956241, 'translation_length': 4271, 'reference_length': 3108, 'precisions_0': 0.07279962546816479, 'precisions_1': 0.037152444870565675, 'precisions_2': 0.022347740667976426, 'precisions_3': 0.01460221550855992, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v38_plustwoepoch/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# {'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.03065082934202577, 'brevity_penalty': 1.0, 'ratio': 1.3741956241956241, 'translation_length': 4271, 'reference_length': 3108, 'precisions_0': 0.07279962546816479, 'precisions_1': 0.037152444870565675, 'precisions_2': 0.022347740667976426, 'precisions_3': 0.01460221550855992, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v38_plustwoepoch/'}from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ec8edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4967/1950585116.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f27156de14bba9e6b08de44a33a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:13<02:04, 13.84s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [01:14<05:29, 41.14s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [01:34<03:41, 31.67s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [01:56<02:47, 27.92s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [02:13<02:00, 24.09s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [02:31<01:27, 21.78s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:41<00:54, 18.04s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [03:39<01:01, 30.79s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [03:55<00:26, 26.23s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [04:09<00:00, 24.91s/it]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 64369.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.3165820184021644, 'brevity_penalty': 0.7845045406807699, 'ratio': 0.8046975546975547, 'translation_length': 2501, 'reference_length': 3108, 'precisions_0': 0.6342925659472423, 'precisions_1': 0.46253122398001667, 'precisions_2': 0.34317984361424847, 'precisions_3': 0.2633969118982743, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v38_plusoneepoch/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0247e8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07db72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3716/1950585116.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e96de81a0347e2a657efecc6ace578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:13<01:59, 13.33s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [01:13<05:25, 40.71s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [01:22<03:05, 26.45s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [01:28<01:50, 18.43s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:42<01:23, 16.79s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:59<01:06, 16.71s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:08<00:43, 14.42s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [03:07<00:57, 28.62s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [03:29<00:26, 26.26s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [03:36<00:00, 21.65s/it]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 73365.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.27863233355465167, 'brevity_penalty': 0.6950607457027734, 'ratio': 0.7332689832689833, 'translation_length': 2279, 'reference_length': 3108, 'precisions_0': 0.6416666666666667, 'precisions_1': 0.4596330275229358, 'precisions_2': 0.3382988947621336, 'precisions_3': 0.2588294651866801, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v37/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63000d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3586/1950585116.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a0bb68057443d4bfc27d4298fe118f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:12<01:56, 13.00s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [01:11<05:16, 39.61s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [01:20<02:59, 25.67s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [01:26<01:47, 17.87s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:39<01:20, 16.20s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:55<01:04, 16.10s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:04<00:41, 13.89s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [03:01<00:54, 27.39s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [03:21<00:25, 25.14s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [03:28<00:00, 20.84s/it]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 72178.70it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 300, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.27863233355465167, 'brevity_penalty': 0.6950607457027734, 'ratio': 0.7332689832689833, 'translation_length': 2279, 'reference_length': 3108, 'precisions_0': 0.6416666666666667, 'precisions_1': 0.4596330275229358, 'precisions_2': 0.3382988947621336, 'precisions_3': 0.2588294651866801, 'max_new_tokens': 300, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v37/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0064eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EvaluateTestSet' object has no attribute 'res_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_list\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EvaluateTestSet' object has no attribute 'res_list'"
     ]
    }
   ],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7d7e6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'String function ( ) { return baseZNode + \"srini_string\" + IDENTITIES_ZNODE_NAME ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list = [get_response(s) for s in evaluator.res_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae319fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb35342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3481/3839643825.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25de0ed65dd6462eab125fe94188fd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/1 [00:08<?, ?it/s]\u001b[A\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res3 = get_metric_res(model=model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     params_iteration=params_iteration,\n",
    "                     experiment_name=experiment_name\n",
    "                    )\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717b51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompter import prompter, get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a71f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"### Response:\n",
    "TernaryBool function ( ) { return canBeInstantiated ( ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
    "===\n",
    "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Generate java code\n",
    "interpret a short as its binary form \n",
    "\n",
    "### Input:\n",
    " PlaceHolder placeHolder \n",
    "\n",
    " long asLong \n",
    " int toInt \n",
    " String toBinaryString \n",
    " String toBinaryString \n",
    " String toBinaryString \n",
    " byte[] fromInt \n",
    " byte[] fromLong\n",
    "\n",
    "### Response:\n",
    "short function ( final short arg0 ) { return ( short ) ( arg0 & 0xFFFF ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efa0b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "TernaryBool function ( ) { return canBeInstantiated ( ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "===\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate java code\n",
      "interpret a short as its binary form \n",
      "\n",
      "### Input:\n",
      " PlaceHolder placeHolder \n",
      "\n",
      " long asLong \n",
      " int toInt \n",
      " String toBinaryString \n",
      " String toBinaryString \n",
      " String toBinaryString \n",
      " byte[] fromInt \n",
      " byte[] fromLong\n",
      "\n",
      "### Response:\n",
      "short function ( final short arg0 ) { return ( short ) ( arg0 & 0xFFFF ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### Response: TernaryBool function ( ) { return canBeInstantiated ( ) ; }</s> === '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = EvaluateTestSet()\n",
    "o.preprocess(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2f4619f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "TernaryBool function ( ) { return canBeInstantiated ( ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "===\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate java code\n",
      "interpret a short as its binary form \n",
      "\n",
      "### Input:\n",
      " PlaceHolder placeHolder \n",
      "\n",
      " long asLong \n",
      " int toInt \n",
      " String toBinaryString \n",
      " String toBinaryString \n",
      " String toBinaryString \n",
      " byte[] fromInt \n",
      " byte[] fromLong\n",
      "\n",
      "### Response:\n",
      "short function ( final short arg0 ) { return ( short ) ( arg0 & 0xFFFF ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'short function ( final short arg0 ) { return ( short ) ( arg0 & 0xFFFF ) ; }</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfcbb640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1969/3839643825.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for generation_config_dict in tqdm_notebook(ParamsIterator(params_iteration=params_iteration)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da2942de5404d04b42ed2316347c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:09<01:22,  9.13s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [00:38<02:47, 20.95s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [00:46<01:44, 14.89s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [00:53<01:12, 12.01s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:07<01:03, 12.65s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:27<01:00, 15.21s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [01:38<00:41, 13.73s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [01:50<00:26, 13.38s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [02:02<00:12, 12.91s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [02:15<00:00, 13.54s/it]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 34314.85it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 200, 'temperature': 1.0} {'EM': 0.0, 'BLEU': 0.0006187833664486746, 'brevity_penalty': 0.30601594809335797, 'ratio': 0.45785070785070786, 'translation_length': 1423, 'reference_length': 3108, 'precisions_0': 0.03651685393258427, 'precisions_1': 0.0007552870090634441, 'precisions_2': 0.0007710100231303007, 'precisions_3': 0.0007861635220125787, 'max_new_tokens': 200, 'temperature': 1.0, 'experiment_name': '/root/experiments/t2c_concode_220428_v34/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'EM': 0.0,\n",
       "  'BLEU': 0.0006187833664486746,\n",
       "  'brevity_penalty': 0.30601594809335797,\n",
       "  'ratio': 0.45785070785070786,\n",
       "  'translation_length': 1423,\n",
       "  'reference_length': 3108,\n",
       "  'precisions_0': 0.03651685393258427,\n",
       "  'precisions_1': 0.0007552870090634441,\n",
       "  'precisions_2': 0.0007710100231303007,\n",
       "  'precisions_3': 0.0007861635220125787,\n",
       "  'max_new_tokens': 200,\n",
       "  'temperature': 1.0,\n",
       "  'experiment_name': '/root/experiments/t2c_concode_220428_v34/'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7c0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
