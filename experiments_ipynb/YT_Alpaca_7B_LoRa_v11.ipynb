{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60ac0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All to folder\n",
    "## generate prompt\n",
    "# !ls data\n",
    "# import time\n",
    "# time.sleep(60*30)\n",
    "\n",
    "# Try to do:\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c4ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM as LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer as LLaMATokenizer\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from EvaluateTestSet import EvaluateTestSet\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "def init_lora_model_and_tokenizer(default_model,\n",
    "                             LORA_R,\n",
    "                             LORA_ALPHA,\n",
    "                             LORA_DROPOUT\n",
    "                            ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    model = LLaMAForCausalLM.from_pretrained(\n",
    "    default_model,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = LLaMATokenizer.from_pretrained(\n",
    "        default_model, add_eos_token=True\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "class MyCustomCallback(TensorBoardCallback):\n",
    "    #log_bleu_steps_factor = 5\n",
    "    bleu_generation_max_new_tokens = 30\n",
    "    bleu_fn_test_data = \"temp/t2c_answers.json\"\n",
    "    bleu_fn_etalon = \"temp/answers.json\"\n",
    "    log_step = 0\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        super().on_log(args, state, control, logs=logs, **kwargs)\n",
    "        #print(\"kwargs\", len(kwargs), kwargs.keys())\n",
    "        if self.tb_writer is not None:\n",
    "            #print(state)\n",
    "            #print(state.global_step)\n",
    "            #print(self.log_step)\n",
    "            if (self.log_step % self.log_bleu_steps_factor ==0):\n",
    "                model = kwargs['model']\n",
    "                tokenizer = kwargs['tokenizer']\n",
    "                \n",
    "                model.eval()\n",
    "                assert not model.training\n",
    "                generation_config = GenerationConfig(max_new_tokens = self.bleu_generation_max_new_tokens,\n",
    "                                                     # min_new_tokens = 5,\n",
    "                                                     temperature = 1.0\n",
    "                                                    )\n",
    "                print(\"generation_config:\", generation_config)\n",
    "                evaluator = EvaluateTestSet(generation_config = generation_config,\n",
    "                                            fn_test_data = self.bleu_fn_test_data,\n",
    "                                            fn_etalon = self.bleu_fn_etalon,\n",
    "                                            batch_size = 1\n",
    "                                       )\n",
    "\n",
    "                metric_res = evaluator.evaluate(model=model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                               )\n",
    "                model.train()\n",
    "                assert model.training\n",
    "                print(metric_res)\n",
    "                for key, val in metric_res.items():\n",
    "                    #add \"custom/something\"\n",
    "                    self.tb_writer.add_scalar(key, val, state.global_step)\n",
    "                self.tb_writer.flush()\n",
    "            self.log_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a58e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"/root/experiments_config/\"\n",
    "EXPERIMENTS_PATH = \"/root/experiments/\"\n",
    "experiment_name = \"t2c_concode_220428_v19\"\n",
    "# t2c_concode_220428_v18.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc88f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_config_path = os.path.join(CONFIG_PATH, experiment_name + \"_config.json\")\n",
    "experiment_config = json.load(open(current_config_path, \"r\"))\n",
    "\n",
    "assert experiment_config['experiment_name'] == experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eff1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config['resume_from_checkpoint'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50bb0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert experiment_config['resume_from_checkpoint'] == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc63c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert experiment_config['experiment_name'] == experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f52cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment_path = os.path.join(EXPERIMENTS_PATH, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf204b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/experiments/t2c_concode_220428_v19’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7a3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(experiment_config, open(current_experiment_path + \\\n",
    "                                  \"/experiment_config.json\", \n",
    "                                  \"w+\"\n",
    "                                 )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946d5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(MyCustomCallback, \"log_bleu_steps_factor\", experiment_config['log_bleu_steps_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e33b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyCustomCallback.log_bleu_steps_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5bab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e34cc35c7c4bbc9cfaf7c86e3251cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3ac2744fedc77f2f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4902dc0e91bc482dba6a1133bcbc905b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = init_lora_model_and_tokenizer(default_model = experiment_config[\"default_model\"],\n",
    "                                                 LORA_R = experiment_config[\"LORA_R\"],\n",
    "                                                 LORA_ALPHA = experiment_config[\"LORA_ALPHA\"],\n",
    "                                                 LORA_DROPOUT = experiment_config[\"LORA_DROPOUT\"]\n",
    "                                                )\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", \n",
    "                    data_files = {\"train\": experiment_config[\"fn_train_dataset\"],\n",
    "                                  \"eval\":  experiment_config[\"fn_eval_dataset\"]\n",
    "                                 }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e83f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_val = LLaMATokenizer.from_pretrained(\n",
    "    experiment_config['default_model'], add_eos_token=True\n",
    ")\n",
    "tokenizer_val.pad_token_id = 0  # unk. we want this to be different from the eos token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a58aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config[\"logging_steps\"] = 1\n",
    "# experiment_config[\"eval_steps\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415d4e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 't2c_concode_220428_v19',\n",
       " 'fn_train_dataset': '/root/data/t2c_train.json',\n",
       " 'fn_eval_dataset': '/root/data/t2c_answers.json',\n",
       " 'default_model': 'decapoda-research/llama-7b-hf',\n",
       " 'MICRO_BATCH_SIZE': 2,\n",
       " 'BATCH_SIZE': 10,\n",
       " 'EPOCHS': 2,\n",
       " 'LEARNING_RATE': 0.0002,\n",
       " 'CUTOFF_LEN': 256,\n",
       " 'LORA_R': 16,\n",
       " 'LORA_ALPHA': 16,\n",
       " 'LORA_DROPOUT': 0.05,\n",
       " 'warmup_steps': 200,\n",
       " 'fp16': True,\n",
       " 'logging_steps': 10,\n",
       " 'eval_steps': 100,\n",
       " 'evaluation_strategy': 'steps',\n",
       " 'save_total_limit': 1,\n",
       " 'save_strategy': 'steps',\n",
       " 'save_steps': 500,\n",
       " 'seed': 42,\n",
       " 'logging_strategy': 'steps',\n",
       " 'report_to': 'tensorboard',\n",
       " 'mlm': False,\n",
       " 'truncation': True,\n",
       " 'padding': 'max_length',\n",
       " 'config_use_cache': False,\n",
       " 'resume_from_checkpoint': False,\n",
       " 'bleu_batch_size': 5,\n",
       " 'GRADIENT_ACCUMULATION_STEPS': 5,\n",
       " 'log_bleu_steps_factor': 50}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6708fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config[\"resume_from_checkpoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1229c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ipynb/prompter/templates/\n"
     ]
    }
   ],
   "source": [
    "from prompter import Prompter\n",
    "prompter = Prompter()\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    if \"input\" in data_point and data_point[\"input\"]:\n",
    "        return prompter.generate_prompt(instruction = data_point[\"instruction\"],\n",
    "                                        input = data_point[\"input\"],\n",
    "                                        label = data_point[\"output\"]\n",
    "                                       )\n",
    "    else:\n",
    "        return prompter.generate_prompt(instruction = data_point[\"instruction\"],\n",
    "                                        #input = None,\n",
    "                                        label = data_point[\"output\"]\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4af6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce5420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8518' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8518/20000 9:45:22 < 13:09:14, 0.24 it/s, Epoch 0.85/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.137300</td>\n",
       "      <td>1.274150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.007200</td>\n",
       "      <td>1.166116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.016400</td>\n",
       "      <td>1.136088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.128393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.930400</td>\n",
       "      <td>1.124290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>1.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.975800</td>\n",
       "      <td>1.114139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.937100</td>\n",
       "      <td>1.114745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.909300</td>\n",
       "      <td>1.110912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>1.109662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.939100</td>\n",
       "      <td>1.108066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>1.101894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>1.106151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>1.102036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>1.104635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.933400</td>\n",
       "      <td>1.099551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>1.097040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>1.098051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.923400</td>\n",
       "      <td>1.097430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.891500</td>\n",
       "      <td>1.094895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>1.096034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>1.098776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>1.094251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.936600</td>\n",
       "      <td>1.092899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.928900</td>\n",
       "      <td>1.093748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.843200</td>\n",
       "      <td>1.092242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.942600</td>\n",
       "      <td>1.093893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>1.094329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.942800</td>\n",
       "      <td>1.095656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>1.090761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>1.091895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>1.093036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>1.089532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.876300</td>\n",
       "      <td>1.088017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.803800</td>\n",
       "      <td>1.094627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.864800</td>\n",
       "      <td>1.088594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>1.089060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>1.085691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.940400</td>\n",
       "      <td>1.085263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>1.084951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>1.087337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.805700</td>\n",
       "      <td>1.085344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>1.089683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>1.084441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>1.083433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>1.085380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>1.084899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.803800</td>\n",
       "      <td>1.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>1.087579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.841900</td>\n",
       "      <td>1.084408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.860200</td>\n",
       "      <td>1.084247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.880200</td>\n",
       "      <td>1.084291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>1.081895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>1.085987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>1.086208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.887300</td>\n",
       "      <td>1.086561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>1.086574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>1.087683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.872500</td>\n",
       "      <td>1.088471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>1.084646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>1.084426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>1.083213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.860400</td>\n",
       "      <td>1.081916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.918600</td>\n",
       "      <td>1.082353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>1.082183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>1.088856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.843300</td>\n",
       "      <td>1.083860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.830700</td>\n",
       "      <td>1.082778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>1.079511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.816700</td>\n",
       "      <td>1.083423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.801200</td>\n",
       "      <td>1.079229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>1.080439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.793700</td>\n",
       "      <td>1.080543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.861700</td>\n",
       "      <td>1.079184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>1.083456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>1.080696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>1.079723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>1.081720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>1.084807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>1.081292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.077789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.774200</td>\n",
       "      <td>1.078788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>1.081620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.766200</td>\n",
       "      <td>1.079978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.759600</td>\n",
       "      <td>1.080931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|███████████████████████████████████████████| 30/30 [01:52<00:00,  3.75s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 147860.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.0025209454334184232, 'brevity_penalty': 0.34897191978151204, 'ratio': 0.48714810281517745, 'translation_length': 398, 'reference_length': 817, 'precisions_0': 0.05263157894736842, 'precisions_1': 0.005420054200542005, 'precisions_2': 0.0029498525073746312, 'precisions_3': 0.003236245954692557}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:45<00:00,  3.52s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 162780.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.2107181604972387, 'brevity_penalty': 0.7059683881507222, 'ratio': 0.7417380660954712, 'translation_length': 606, 'reference_length': 817, 'precisions_0': 0.5304777594728172, 'precisions_1': 0.36568457538994803, 'precisions_2': 0.24314442413162707, 'precisions_3': 0.16827852998065765}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:46<00:00,  3.56s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 155729.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.21512116815531265, 'brevity_penalty': 0.6694008551670413, 'ratio': 0.7135862913096696, 'translation_length': 583, 'reference_length': 817, 'precisions_0': 0.5941780821917808, 'precisions_1': 0.40974729241877256, 'precisions_2': 0.25763358778625955, 'precisions_3': 0.1700404858299595}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:47<00:00,  3.57s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 157878.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.21019114338503475, 'brevity_penalty': 0.6742205795704415, 'ratio': 0.7172582619339045, 'translation_length': 586, 'reference_length': 817, 'precisions_0': 0.606473594548552, 'precisions_1': 0.40754039497307004, 'precisions_2': 0.24667931688804554, 'precisions_3': 0.15492957746478872}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:46<00:00,  3.55s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 167996.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.19085050051921126, 'brevity_penalty': 0.6980943605403788, 'ratio': 0.7356181150550796, 'translation_length': 601, 'reference_length': 817, 'precisions_0': 0.5465116279069767, 'precisions_1': 0.34440559440559443, 'precisions_2': 0.2140221402214022, 'precisions_3': 0.138671875}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:47<00:00,  3.57s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 166661.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.19054981686442318, 'brevity_penalty': 0.6434460411594718, 'ratio': 0.6940024479804161, 'translation_length': 567, 'reference_length': 817, 'precisions_0': 0.5915492957746479, 'precisions_1': 0.3828996282527881, 'precisions_2': 0.23968565815324164, 'precisions_3': 0.14166666666666666}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:46<00:00,  3.56s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 34962.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.22481652330257174, 'brevity_penalty': 0.6564757230932565, 'ratio': 0.7037943696450428, 'translation_length': 575, 'reference_length': 817, 'precisions_0': 0.6232638888888888, 'precisions_1': 0.4194139194139194, 'precisions_2': 0.2810077519379845, 'precisions_3': 0.18724279835390947}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:39<00:00,  3.31s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 142663.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.20150458369452562, 'brevity_penalty': 0.5835609347268835, 'ratio': 0.6499388004895961, 'translation_length': 531, 'reference_length': 817, 'precisions_0': 0.6466165413533834, 'precisions_1': 0.43227091633466136, 'precisions_2': 0.2775423728813559, 'precisions_3': 0.1832579185520362}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:43<00:00,  3.46s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 51995.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.2141325804835005, 'brevity_penalty': 0.6467131914862885, 'ratio': 0.6964504283965728, 'translation_length': 569, 'reference_length': 817, 'precisions_0': 0.5912280701754385, 'precisions_1': 0.3962962962962963, 'precisions_2': 0.27058823529411763, 'precisions_3': 0.18958333333333333}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:45<00:00,  3.53s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 39101.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.21495728540586068, 'brevity_penalty': 0.6220534596927131, 'ratio': 0.6780905752753978, 'translation_length': 554, 'reference_length': 817, 'precisions_0': 0.6144144144144145, 'precisions_1': 0.42095238095238097, 'precisions_2': 0.28484848484848485, 'precisions_3': 0.1935483870967742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:45<00:00,  3.51s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 162150.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.22743628522356701, 'brevity_penalty': 0.6483443410015097, 'ratio': 0.6976744186046512, 'translation_length': 570, 'reference_length': 817, 'precisions_0': 0.5936952714535902, 'precisions_1': 0.40850277264325324, 'precisions_2': 0.29158512720156554, 'precisions_3': 0.21413721413721415}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:46<00:00,  3.54s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 158474.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.22151808863409636, 'brevity_penalty': 0.6885900046167743, 'ratio': 0.7282741738066095, 'translation_length': 595, 'reference_length': 817, 'precisions_0': 0.5771812080536913, 'precisions_1': 0.392226148409894, 'precisions_2': 0.2630597014925373, 'precisions_3': 0.17984189723320157}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:44<00:00,  3.48s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 169352.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.21397737878200795, 'brevity_penalty': 0.6020708956175393, 'ratio': 0.6634026927784578, 'translation_length': 542, 'reference_length': 817, 'precisions_0': 0.6132596685082873, 'precisions_1': 0.4249512670565302, 'precisions_2': 0.2919254658385093, 'precisions_3': 0.2097130242825607}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:44<00:00,  3.48s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 163202.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.21556123221735057, 'brevity_penalty': 0.5919969883585302, 'ratio': 0.6560587515299877, 'translation_length': 536, 'reference_length': 817, 'precisions_0': 0.6163873370577281, 'precisions_1': 0.4339250493096647, 'precisions_2': 0.29979035639413, 'precisions_3': 0.21923937360178972}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:45<00:00,  3.53s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 156115.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.22244336904208384, 'brevity_penalty': 0.6154173979975659, 'ratio': 0.6731946144430845, 'translation_length': 550, 'reference_length': 817, 'precisions_0': 0.6442831215970962, 'precisions_1': 0.43378119001919385, 'precisions_2': 0.29327902240325865, 'precisions_3': 0.20824295010845986}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:33<00:00,  3.11s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 172368.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.1913594547539591, 'brevity_penalty': 0.5148132218282434, 'ratio': 0.6009791921664627, 'translation_length': 491, 'reference_length': 817, 'precisions_0': 0.6605691056910569, 'precisions_1': 0.4523809523809524, 'precisions_2': 0.2986111111111111, 'precisions_3': 0.21393034825870647}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:36<00:00,  3.23s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 50655.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.2055215090695389, 'brevity_penalty': 0.5235213745836479, 'ratio': 0.6070991432068543, 'translation_length': 496, 'reference_length': 817, 'precisions_0': 0.6559356136820925, 'precisions_1': 0.46466809421841543, 'precisions_2': 0.32036613272311215, 'precisions_3': 0.24324324324324326}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:43<00:00,  3.45s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 50758.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.20623565837695024, 'brevity_penalty': 0.57678561653526, 'ratio': 0.6450428396572827, 'translation_length': 527, 'reference_length': 817, 'precisions_0': 0.6268939393939394, 'precisions_1': 0.42570281124497994, 'precisions_2': 0.28846153846153844, 'precisions_3': 0.21232876712328766}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [01:39<00:00,  3.30s/it]\n",
      "100%|███████████████████████████████████████| 30/30 [00:00<00:00, 170039.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.19307813252370135, 'brevity_penalty': 0.5477365891588007, 'ratio': 0.6242350061199511, 'translation_length': 510, 'reference_length': 817, 'precisions_0': 0.6301369863013698, 'precisions_1': 0.43451143451143454, 'precisions_2': 0.2860310421286031, 'precisions_3': 0.19714964370546317}\n"
     ]
    }
   ],
   "source": [
    "# def generate_prompt(data_point):\n",
    "#     # sorry about the formatting disaster gotta move fast\n",
    "#     if data_point[\"input\"]:\n",
    "#         return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# {data_point[\"instruction\"]}\n",
    "# ### Input:\n",
    "# {data_point[\"input\"]}\n",
    "# ### Response:\n",
    "# {data_point[\"output\"]}\"\"\"\n",
    "#     else:\n",
    "#         return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# {data_point[\"instruction\"]}\n",
    "# ### Response:\n",
    "# {data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "data = data.shuffle().map(\n",
    "    lambda data_point: tokenizer(\n",
    "        generate_prompt(data_point),\n",
    "        truncation=experiment_config[\"truncation\"],\n",
    "        max_length=experiment_config[\"CUTOFF_LEN\"],\n",
    "        padding=experiment_config[\"padding\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_val,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data['eval'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=experiment_config[\"MICRO_BATCH_SIZE\"],\n",
    "        gradient_accumulation_steps=experiment_config[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
    "        warmup_steps=experiment_config[\"warmup_steps\"],\n",
    "        num_train_epochs=experiment_config[\"EPOCHS\"],\n",
    "        learning_rate=experiment_config[\"LEARNING_RATE\"],\n",
    "        fp16=experiment_config[\"fp16\"],\n",
    "        logging_steps=experiment_config[\"logging_steps\"],        \n",
    "        evaluation_strategy = experiment_config['evaluation_strategy'],\n",
    "        eval_steps=experiment_config[\"eval_steps\"],\n",
    "        output_dir=current_experiment_path,#\"lora-alpaca\",\n",
    "        save_total_limit=experiment_config[\"save_total_limit\"],\n",
    "        save_strategy = experiment_config[\"save_strategy\"],\n",
    "        \n",
    "        save_steps = experiment_config[\"save_steps\"],\n",
    "        seed=experiment_config[\"seed\"],\n",
    "        logging_dir=current_experiment_path,\n",
    "        logging_strategy=experiment_config[\"logging_strategy\"],\n",
    "        report_to=experiment_config[\"report_to\"]\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                               mlm=experiment_config[\"mlm\"]\n",
    "                                                              ),\n",
    "    callbacks = [MyCustomCallback]\n",
    ")\n",
    "model.config.use_cache = experiment_config[\"config_use_cache\"]\n",
    "# print(len(trainer.optimizer.state['found_inf_per_device']))\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=experiment_config[\"resume_from_checkpoint\"])\n",
    "\n",
    "model.save_pretrained(current_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665c1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(current_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a8f14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay          45G   34G   12G  74% /\r\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "392d638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\r\n",
      "drwxr-xr-x  2 root root   36 May  9 21:46 .\r\n",
      "drwxr-xr-x 12 root root 4.0K May  9 21:46 ..\r\n",
      "-rw-r--r--  1 root root  793 May  9 21:48 experiment_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97279e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf {current_experiment_path}/checkpoint-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feedd63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cbe2cb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b2d438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683669322.621859    events.out.tfevents.1683669322.8d048d63ed1a.14248.0\r\n",
      "1683669322.6291256   events.out.tfevents.1683669322.8d048d63ed1a.14248.2\r\n",
      "adapter_config.json  experiment_config.json\r\n",
      "adapter_model.bin    metrics.csv\r\n",
      "checkpoint-20000\r\n"
     ]
    }
   ],
   "source": [
    "!rm /root/experiments/t2c_concode_220428_v19/checkpoint-20000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1960ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
