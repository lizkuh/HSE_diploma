{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf700c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # prepare dataset in correct format\n",
    "# dev_dataset = [json.loads(s) for s in open(\"/root/data/dev.json\", \"r\").read().split('\\n')[:-1]]\n",
    "# input_list_of_dict = []\n",
    "# for obj in dev_dataset:\n",
    "#     instruction = obj['nl'].split(\"concode_field_sep\")[0]\n",
    "#     input = 'concode_field_sep'.join(obj['nl'].split(\"concode_field_sep\")[1:])\n",
    "#     input = input.replace('concode_elem_sep', '\\n')\n",
    "#     input = input.replace('concode_field_sep', '\\n')\n",
    "#     assert '\\n' not in obj['code']\n",
    "#     input_list_of_dict.append({\"instruction\": f\"Generate java code\\n{instruction}\",\n",
    "#                                \"input\": input,\n",
    "#                                \"output\": obj['code']\n",
    "#                               }\n",
    "#                              )\n",
    "\n",
    "# json.dump(input_list_of_dict, open(\"/root/data/final_dev.json\", 'w+'))\n",
    "# # json.dump('\\n'.join([s['output'] for s in input_list_of_dict]), open(\"/root/data/final_dev.txt\", 'w+'))\n",
    "# open(\"/root/data/final_dev.txt\", 'w+').write('\\n'.join([s['output'] for s in input_list_of_dict]))\n",
    "\n",
    "# open('/root/data/final_answers_jsonl.json', 'w+').\\\n",
    "# write(\n",
    "#     '\\n'.join([json.dumps({\"code\": obj['output']})\n",
    "#                for obj in input_list_of_dict\n",
    "#               ]\n",
    "#              )\n",
    "# )\n",
    "#\n",
    "# open(\"/root/data/final_answers_jsonl.json\", \"r\").read().count('\\n'),\\\n",
    "# open(\"/root/data/final_dev.json\", \"r\").read().count('\\n'),\\\n",
    "# open(\"/root/data/final_dev.txt\", \"r\").read().count('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482902f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10037862",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"t2c_concode_220428_v33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d268ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers.json\t\t  reference_corpus.txt\tt2c_dev.json\r\n",
      "dev.json\t\t  t2c_10000train.json\tt2c_test.json\r\n",
      "final_answers_jsonl.json  t2c_1000train.json\tt2c_train.json\r\n",
      "final_dev.json\t\t  t2c_10answers.json\r\n",
      "final_dev.txt\t\t  t2c_answers.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445d5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/root/HSE_diploma/\")\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "EXPERIMENT_PATH = \"/root/experiments/\"\n",
    "ARTIFACTS_PATH = \"/root/temperary_results/\"\n",
    "\n",
    "df_experiments = pd.read_csv(\"all_experiments.csv\").set_index(\"experiment_name_short\")\n",
    "\n",
    "line = df_experiments[df_experiments['exp_name'] == exp_name]\n",
    "assert(len(line)==1)\n",
    "line = line.iloc[0]\n",
    "line = line.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc56e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b57be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiments.loc[[\"9\", \"10\", \"11\", \"12\", \"13\"], \"default_model\"] = [\"decapoda-research/llama-7b-hf\"]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1454c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>adapter_model.bin</th>\n",
       "      <th>experiment_config.json</th>\n",
       "      <th>adapter_config.json</th>\n",
       "      <th>fn_config</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>fn_train_dataset</th>\n",
       "      <th>default_model</th>\n",
       "      <th>MICRO_BATCH_SIZE</th>\n",
       "      <th>BATCH_SIZE</th>\n",
       "      <th>...</th>\n",
       "      <th>bleu_batch_size</th>\n",
       "      <th>GRADIENT_ACCUMULATION_STEPS</th>\n",
       "      <th>fn_eval_dataset</th>\n",
       "      <th>log_bleu_steps_factor</th>\n",
       "      <th>eval_steps</th>\n",
       "      <th>evaluation_strategy</th>\n",
       "      <th>load_best_model_at_end</th>\n",
       "      <th>epoch</th>\n",
       "      <th>num_virtual_tokens</th>\n",
       "      <th>mapping_hidden_dim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment_name_short</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t2c_concode_220428_v9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t2c_concode_220428_v10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t2c_concode_220428_v11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t2c_concode_220428_v12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t2c_concode_220428_v13</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     exp_name  adapter_model.bin   \n",
       "experiment_name_short                                              \n",
       "9                       t2c_concode_220428_v9               True  \\\n",
       "10                     t2c_concode_220428_v10               True   \n",
       "11                     t2c_concode_220428_v11               True   \n",
       "12                     t2c_concode_220428_v12               True   \n",
       "13                     t2c_concode_220428_v13               True   \n",
       "\n",
       "                       experiment_config.json  adapter_config.json   \n",
       "experiment_name_short                                                \n",
       "9                                       False                 True  \\\n",
       "10                                      False                 True   \n",
       "11                                      False                 True   \n",
       "12                                      False                 True   \n",
       "13                                      False                 True   \n",
       "\n",
       "                                                               fn_config   \n",
       "experiment_name_short                                                      \n",
       "9                      /root/experiments/experiments/t2c_concode_2204...  \\\n",
       "10                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "11                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "12                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "13                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "\n",
       "                      experiment_name fn_train_dataset   \n",
       "experiment_name_short                                    \n",
       "9                                 NaN              NaN  \\\n",
       "10                                NaN              NaN   \n",
       "11                                NaN              NaN   \n",
       "12                                NaN              NaN   \n",
       "13                                NaN              NaN   \n",
       "\n",
       "                                       default_model  MICRO_BATCH_SIZE   \n",
       "experiment_name_short                                                    \n",
       "9                      decapoda-research/llama-7b-hf               NaN  \\\n",
       "10                     decapoda-research/llama-7b-hf               NaN   \n",
       "11                     decapoda-research/llama-7b-hf               NaN   \n",
       "12                     decapoda-research/llama-7b-hf               NaN   \n",
       "13                     decapoda-research/llama-7b-hf               NaN   \n",
       "\n",
       "                       BATCH_SIZE  ...  bleu_batch_size   \n",
       "experiment_name_short              ...                    \n",
       "9                             NaN  ...              NaN  \\\n",
       "10                            NaN  ...              NaN   \n",
       "11                            NaN  ...              NaN   \n",
       "12                            NaN  ...              NaN   \n",
       "13                            NaN  ...              NaN   \n",
       "\n",
       "                       GRADIENT_ACCUMULATION_STEPS  fn_eval_dataset   \n",
       "experiment_name_short                                                 \n",
       "9                                              NaN              NaN  \\\n",
       "10                                             NaN              NaN   \n",
       "11                                             NaN              NaN   \n",
       "12                                             NaN              NaN   \n",
       "13                                             NaN              NaN   \n",
       "\n",
       "                       log_bleu_steps_factor  eval_steps  evaluation_strategy   \n",
       "experiment_name_short                                                           \n",
       "9                                        NaN         NaN                  NaN  \\\n",
       "10                                       NaN         NaN                  NaN   \n",
       "11                                       NaN         NaN                  NaN   \n",
       "12                                       NaN         NaN                  NaN   \n",
       "13                                       NaN         NaN                  NaN   \n",
       "\n",
       "                       load_best_model_at_end epoch  num_virtual_tokens   \n",
       "experiment_name_short                                                     \n",
       "9                                         NaN   NaN                 NaN  \\\n",
       "10                                        NaN   NaN                 NaN   \n",
       "11                                        NaN   NaN                 NaN   \n",
       "12                                        NaN   NaN                 NaN   \n",
       "13                                        NaN   NaN                 NaN   \n",
       "\n",
       "                       mapping_hidden_dim  \n",
       "experiment_name_short                      \n",
       "9                                     NaN  \n",
       "10                                    NaN  \n",
       "11                                    NaN  \n",
       "12                                    NaN  \n",
       "13                                    NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_experiments[df_experiments['experiment_config.json']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a88c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiments[df_experiments[]]\n",
    "# df_experiments['default_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60967c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiments\n",
    "# os.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e9268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = line[\"experiment_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf74ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already here\n"
     ]
    }
   ],
   "source": [
    "# Loading files to remote host if they are not exist\n",
    "if not os.path.exists(os.path.join(EXPERIMENT_PATH, experiment_name)):\n",
    "    print(\"loading files\")\n",
    "    #!mkdir {os.path.join(EXPERIMENT_PATH, experiment_name)}\n",
    "    os.system(f\"mkdir {os.path.join(EXPERIMENT_PATH, experiment_name)}\")\n",
    "    for filename in ['adapter_model.bin', 'experiment_config.json', 'adapter_config.json']:\n",
    "        file_from = f\"/root/experiments/experiments/{experiment_name}/{filename}\"\n",
    "        file_to   = f\"/root/experiments/{experiment_name}/{filename}\"\n",
    "        print(file_from, '->', file_to)\n",
    "        #!scp -i ~/.ssh/master_hetzner root@65.108.123.219:{file_from} {file_to} \n",
    "        os.system(f\"scp -i ~/.ssh/master_hetzner root@65.108.123.219:{file_from} {file_to} \")\n",
    "else:\n",
    "    print(\"File already here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a92e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 33M\r\n",
      "drwxr-xr-x  2 root root  88 May 26 16:00 .\r\n",
      "drwxr-xr-x 10 root root 279 May 26 20:52 ..\r\n",
      "-rw-r--r--  1 root root 359 May 26 21:58 adapter_config.json\r\n",
      "-rw-r--r--  1 root root 33M May 26 21:58 adapter_model.bin\r\n",
      "-rw-r--r--  1 root root 815 May 26 21:58 experiment_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/experiments/{exp_name}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d5e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb31c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_model= yahma/llama-7b-hf\n"
     ]
    }
   ],
   "source": [
    "# experiment_name = \"/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/\"\n",
    "experiment_config = None\n",
    "default_model = line['default_model']\n",
    "\n",
    "if line['experiment_config.json']:\n",
    "    experiment_config = json.load(open(f\"{EXPERIMENT_PATH}/{experiment_name}/experiment_config.json\", \n",
    "                                           \"r\"\n",
    "                                          )\n",
    "                                     )\n",
    "    \n",
    "    default_model = json.load(open(f\"{EXPERIMENT_PATH}/{experiment_name}/experiment_config.json\", \n",
    "                                   \"r\"\n",
    "                                  )\n",
    "                             )['default_model']\n",
    "\n",
    "# params_iteration = {\"temperature\": [1.0],\n",
    "#                     \"max_new_tokens\": [300]#None, 20, 30, 35, 40, 50, 60, 70, 80, 90, 100] + [45, 47, 49, 51, 53, 55]\n",
    "#                    }\n",
    "\n",
    "print(\"default_model=\", default_model)\n",
    "# print(\"params_iteration=\", params_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4ed51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM as LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer as LLaMATokenizer\n",
    "\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276cc47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b631835a2848f9a816db97db5610fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LLaMAForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = default_model,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "        model = model,\n",
    "        model_id = os.path.join(EXPERIMENT_PATH, experiment_name),\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(pretrained_model_name_or_path = default_model)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "tokenizer.padding_side = \"left\"\n",
    "# not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46a72db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id 0\n",
      "tokenizer.eos_token_id 2\n",
      "tokenizer.bos_token_id 1\n",
      "tokenizer.eos_token_id 2\n",
      "model.config.pad_token_id 0\n",
      "model.config.eos_token_id 2\n",
      "model.config.bos_token_id 1\n",
      "model.config.eos_token_id 2\n",
      "trainable params: 0 || all params: 6746804224 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "def verbose_model_tokenizer(model, tokenizer):\n",
    "    print(\"tokenizer.pad_token_id\", tokenizer.pad_token_id)\n",
    "    print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "    print(\"tokenizer.bos_token_id\", tokenizer.bos_token_id)\n",
    "    print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "\n",
    "    print(\"model.config.pad_token_id\", model.config.pad_token_id)\n",
    "    print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "    print(\"model.config.bos_token_id\", model.config.bos_token_id)\n",
    "    print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "    if hasattr(model, \"print_trainable_parameters\"):\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "verbose_model_tokenizer(model, tokenizer)\n",
    "\n",
    "# tokenizer.pad_token_id 0\n",
    "# tokenizer.eos_token_id 2\n",
    "# tokenizer.bos_token_id 1\n",
    "# tokenizer.eos_token_id 2\n",
    "# model.config.pad_token_id 0\n",
    "# model.config.eos_token_id 2\n",
    "# model.config.bos_token_id 1\n",
    "# model.config.eos_token_id 2\n",
    "# trainable params: 0 || all params: 6746804224 || trainable%: 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ed4c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/HSE_diploma/prompter/templates/\n"
     ]
    }
   ],
   "source": [
    "from prompter import generate_test_prompt, get_response\n",
    "from transformers import GenerationConfig\n",
    "import math\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdb722d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_dict = {\"temperature\": 1.0,\n",
    "                          #\"penalty_alpha\": 2,\n",
    "                          \"max_new_tokens\": 500\n",
    "                         }\n",
    "\n",
    "generation_config = GenerationConfig(**generation_config_dict)\n",
    "\n",
    "fn_test_data = \"/root/data/final_dev.json\"\n",
    "# fn_test_data = \"../data/t2c_answers.json\"\n",
    "# fn_etalon = \"/root/data/answers.json\"\n",
    "batch_size = 10\n",
    "verbose = False\n",
    "\n",
    "assert model.training == False\n",
    "\n",
    "lst = json.load(open(fn_test_data, 'rb'))\n",
    "inputs = lst\n",
    "\n",
    "prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "prompts = np.array(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7018e400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Generate java code\\nactually walks the bag to make sure the count is correct and resets the running total ',\n",
       " 'input': ' Object _current \\n int _total \\n DefaultMapBag _parent \\n Map _map \\n int _mods \\n Iterator _support \\n boolean add \\n boolean add \\n Object next \\n boolean containsAll \\n boolean containsAll \\n void clear \\n boolean isEmpty \\n boolean hasNext \\n void remove \\n boolean remove \\n boolean remove \\n Map getMap \\n int modCount \\n boolean contains \\n Iterator iterator \\n boolean removeAll \\n int size \\n boolean addAll \\n int hashCode \\n boolean equals \\n Object[] toArray \\n Object[] toArray \\n Set uniqueSet \\n void setMap \\n String toString \\n int getCount \\n List extractList \\n boolean retainAll \\n boolean retainAll',\n",
       " 'output': 'int function ( ) { _total = extractList ( ) . size ( ) ; return _total ; }'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52eae30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate java code\\nactually walks the bag to make sure the count is correct and resets the running total \\n\\n### Input:\\n Object _current \\n int _total \\n DefaultMapBag _parent \\n Map _map \\n int _mods \\n Iterator _support \\n boolean add \\n boolean add \\n Object next \\n boolean containsAll \\n boolean containsAll \\n void clear \\n boolean isEmpty \\n boolean hasNext \\n void remove \\n boolean remove \\n boolean remove \\n Map getMap \\n int modCount \\n boolean contains \\n Iterator iterator \\n boolean removeAll \\n int size \\n boolean addAll \\n int hashCode \\n boolean equals \\n Object[] toArray \\n Object[] toArray \\n Set uniqueSet \\n void setMap \\n String toString \\n int getCount \\n List extractList \\n boolean retainAll \\n boolean retainAll\\n\\n### Response:\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0819d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      " 24%|██████████▎                               | 49/200 [16:23<33:03, 13.14s/it]"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "n = math.ceil(len(prompts)/batch_size)\n",
    "\n",
    "for ind in tqdm.tqdm(range(n)):\n",
    "    current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "    if verbose:\n",
    "        print(ind * batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "\n",
    "    tokenized_inputs = tokenizer(list(current_prompts), \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 return_tensors=\"pt\"\n",
    "                                ).to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        full_output = model.generate(\n",
    "            **tokenized_inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "    res_list.extend(tokenizer.batch_decode(full_output, \n",
    "                                           skip_special_tokens=False\n",
    "                                          )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49109d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {\"real_config\": experiment_config,\n",
    "            \"meta_config\": line,\n",
    "            \"generation_config\": generation_config_dict,\n",
    "            \"fn_test_data\": fn_test_data,\n",
    "#             \"fn_etalon\": fn_etalon,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"verbose\": verbose,\n",
    "            \"prompts\": list(prompts),\n",
    "            \"res_list\": res_list\n",
    "           }\n",
    "\n",
    "fn_output = ARTIFACTS_PATH +\\\n",
    "            \"predictions\" +\\\n",
    "            exp_name+\"_\"+str(datetime.now()).split('.')[0].replace(' ', '-')\n",
    "print(fn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(res_dict, \n",
    "          open(fn_output+'.json', \"w+\")\n",
    "         )\n",
    "\n",
    "pickle.dump(res_dict,\n",
    "            open(fn_output+'.pickle', \"wb\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    #ToDo rewrite it using Promt Template\n",
    "    s = get_response(s)\n",
    "    if \"</s>\" in s:\n",
    "        s = s.split('</s>')[0]\n",
    "    if \"### Input\" in s:\n",
    "        s = s.split(\"### Input\")[0]\n",
    "    try:\n",
    "        assert \"<unk>\" not in s\n",
    "        assert \"\\n\" not in s\n",
    "        assert \"  \" not in s\n",
    "    except Exception as e:\n",
    "        print(\"Something wrong with\")\n",
    "        print(s)\n",
    "        raise e\n",
    "#     #s = s.split('### Response:\\n')[-1]\n",
    "#     #s = s.replace('\\n', '  ')\n",
    "#     s = s.replace('<unk>', \" \")\n",
    "#     s = ' '.join(s.split(' ')[:100])\n",
    "#     while '  ' in s:\n",
    "#         s = s.replace('  ', ' ')\n",
    "\n",
    "#     if len(s) > 0 and s[0] == ' ':\n",
    "#         s = s[1:]\n",
    "\n",
    "#     if self.verbose:\n",
    "#         print(s)\n",
    "\n",
    "    return s\n",
    "# preprocess(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [preprocess(s) for s in res_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0264a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "open(fn_output+\".txt\", 'w+').write('\\n'.join(predictions_list))\n",
    "# 9321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e08414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_results = fn_output+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c721f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = fn_output+\".txt\"\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c594330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ARTIFACTS_PATH)\n",
    "!ls {ARTIFACTS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65036d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# \"/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-16:46:49.txt\"\n",
    "def run_evaluation_of_metrics(fn = fn_results):\n",
    "    #fn_refference = \"/root/data/reference_corpus.txt\"\n",
    "    #fn_answers = \"/root/data/answers.json\"\n",
    "    \n",
    "    fn_refference = \"/root/data/final_dev.txt\"\n",
    "    fn_answers = \"/root/data/final_answers_jsonl.json\"\n",
    "    open(\"/root/HSE_diploma/load_all_experiments/res.txt\", \"w+\").write('')\n",
    "    print(f\"/root/HSE_diploma/evaluateFromCodeXGlue/;python3 calc_code_bleu.py --refs {fn_refference} --hyp $fn --lang java --params 0.25,0.25,0.25,0.25\")\n",
    "    !cd /root/HSE_diploma/evaluateFromCodeXGlue/;python3 calc_code_bleu.py --refs {fn_refference} --hyp $fn --lang java --params 0.25,0.25,0.25,0.25 > /root/HSE_diploma/load_all_experiments/res.txt\n",
    "    !cd /root/HSE_diploma/evaluateFromCodeXGlue/;python3 calculate_bleu/evaluator.py -a={fn_answers} -p=$fn  >> /root/HSE_diploma/load_all_experiments/res.txt\n",
    "\n",
    "\n",
    "def parse_metrics(fn_metrics = \"res.txt\"):\n",
    "    s = open('res.txt', \"r\").read()\n",
    "\n",
    "    BLEU = re.findall(\"BLEU: \\d+\\.\\d+\", s)\n",
    "    EM = re.findall(\"EM: \\d+\\.\\d+\", s)\n",
    "    CodeBLEU = re.findall(\"CodeBLEU score:  \\d+\\.\\d+\", s)\n",
    "\n",
    "    assert len(BLEU) == len(EM) == len(CodeBLEU) == 1\n",
    "    BLEU = float(BLEU[0].split(' ')[-1])\n",
    "    EM = float(EM[0].split(' ')[-1])\n",
    "    CodeBLEU = float(CodeBLEU[0].split(' ')[-1])\n",
    "\n",
    "    ngram_match = float(re.findall(\"ngram match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "    weighted_ngram_match = float(re.findall(\"weighted ngram match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "    syntax_match = float(re.findall(\"syntax_match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "    dataflow_match = float(re.findall(\"dataflow_match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "\n",
    "    precisions = re.findall(\"precisions:\\s+.+\\n\", s)[0].split(':  ')[-1][:-1]\n",
    "    bp = float(re.findall(\"bp:  \\d+\\.\\d+\", s)[0].split(' ')[-1])\n",
    "    ratio = float(re.findall(\"ratio:  \\d+\\.\\d+\", s)[0].split(' ')[-1])\n",
    "    translation_length = int(re.findall(\"translation_length:.*\\n\", s)[0].split(' ')[-1][:-1])\n",
    "    reference_length = int(re.findall(\"reference_length:.*\\n\", s)[0].split(' ')[-1][:-1])\n",
    "\n",
    "    # weighted ngram match: 0.2792407600329364, \n",
    "    # syntax_match: 0.3445378151260504, \n",
    "    # dataflow_match: 0.2908777969018933\n",
    "    metrics = {\"BLEU\": BLEU,\n",
    "               \"EM\": EM,\n",
    "               \"CodeBLEU\": CodeBLEU,\n",
    "               \"ngram_match\": ngram_match,\n",
    "               \"weighted_ngram_match\": weighted_ngram_match,\n",
    "               \"syntax_match\": syntax_match,\n",
    "               \"dataflow_match\": dataflow_match,\n",
    "               \"precisions\": precisions,\n",
    "               \"bp\": bp,\n",
    "               \"ratio\": ratio,\n",
    "               \"translation_length\": translation_length,\n",
    "               \"reference_length\": reference_length,\n",
    "               \"raw\": s\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "run_evaluation_of_metrics(fn = fn_results)\n",
    "final_metrics = parse_metrics()\n",
    "\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1b299241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/HSE_diploma/evaluateFromCodeXGlue/;python3 calc_code_bleu.py --refs /root/data/final_dev.txt --hyp $fn --lang java --params 0.25,0.25,0.25,0.25\n",
      "INFO:__main__:BLEU: 18.27, EM: 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': 18.27,\n",
       " 'EM': 10.0,\n",
       " 'CodeBLEU': 0.2310302252826384,\n",
       " 'ngram_match': 0.1807804463258241,\n",
       " 'weighted_ngram_match': 0.23934755329629998,\n",
       " 'syntax_match': 0.2795031055900621,\n",
       " 'dataflow_match': 0.22448979591836735,\n",
       " 'precisions': '[0.875968992248062, 0.6470588235294118, 0.44036697247706424, 0.30303030303030304]',\n",
       " 'bp': 0.34830125444308524,\n",
       " 'ratio': 0.4866920152091255,\n",
       " 'translation_length': 128,\n",
       " 'reference_length': 263,\n",
       " 'raw': 'ngram match: 0.1807804463258241, weighted ngram match: 0.23934755329629998, syntax_match: 0.2795031055900621, dataflow_match: 0.22448979591836735\\nCodeBLEU score:  0.2310302252826384\\nBLEU: 18.27, EM: 10.0\\nprecisions:  [0.875968992248062, 0.6470588235294118, 0.44036697247706424, 0.30303030303030304]\\nbp:  0.34830125444308524\\nratio:  0.4866920152091255\\ntranslation_length:  128\\nreference_length:  263\\n'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ecb4b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict['final_metrics'] = final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bbcdd0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(res_dict, \n",
    "          open(fn_output+'_final_metrics.json', \"w+\")\n",
    "         )\n",
    "\n",
    "pickle.dump(res_dict,\n",
    "            open(fn_output+'_final_metrics.pickle', \"wb\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b9cea028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-22:34:18'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ce77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77244c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
