{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d288e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"t2c_concode_220428_v33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc4456a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers.json\t      t2c_1000train.json  t2c_dev.json\r\n",
      "reference_corpus.txt  t2c_10answers.json  t2c_test.json\r\n",
      "t2c_10000train.json   t2c_answers.json\t  t2c_train.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be982d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/root/HSE_diploma/\")\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "EXPERIMENT_PATH = \"/root/experiments/\"\n",
    "ARTIFACTS_PATH = \"/root/temperary_results/\"\n",
    "\n",
    "df_experiments = pd.read_csv(\"all_experiments.csv\").set_index(\"experiment_name_short\")\n",
    "\n",
    "line = df_experiments[df_experiments['exp_name'] == exp_name]\n",
    "assert(len(line)==1)\n",
    "line = line.iloc[0]\n",
    "line = line.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d4ee0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24006e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiments.loc[[\"9\", \"10\", \"11\", \"12\", \"13\"], \"default_model\"] = [\"decapoda-research/llama-7b-hf\"]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a0b8ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>adapter_model.bin</th>\n",
       "      <th>experiment_config.json</th>\n",
       "      <th>adapter_config.json</th>\n",
       "      <th>fn_config</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>fn_train_dataset</th>\n",
       "      <th>default_model</th>\n",
       "      <th>MICRO_BATCH_SIZE</th>\n",
       "      <th>BATCH_SIZE</th>\n",
       "      <th>...</th>\n",
       "      <th>bleu_batch_size</th>\n",
       "      <th>GRADIENT_ACCUMULATION_STEPS</th>\n",
       "      <th>fn_eval_dataset</th>\n",
       "      <th>log_bleu_steps_factor</th>\n",
       "      <th>eval_steps</th>\n",
       "      <th>evaluation_strategy</th>\n",
       "      <th>load_best_model_at_end</th>\n",
       "      <th>epoch</th>\n",
       "      <th>num_virtual_tokens</th>\n",
       "      <th>mapping_hidden_dim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment_name_short</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t2c_concode_220428_v9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t2c_concode_220428_v10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t2c_concode_220428_v11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t2c_concode_220428_v12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t2c_concode_220428_v13</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/root/experiments/experiments/t2c_concode_2204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>decapoda-research/llama-7b-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     exp_name  adapter_model.bin   \n",
       "experiment_name_short                                              \n",
       "9                       t2c_concode_220428_v9               True  \\\n",
       "10                     t2c_concode_220428_v10               True   \n",
       "11                     t2c_concode_220428_v11               True   \n",
       "12                     t2c_concode_220428_v12               True   \n",
       "13                     t2c_concode_220428_v13               True   \n",
       "\n",
       "                       experiment_config.json  adapter_config.json   \n",
       "experiment_name_short                                                \n",
       "9                                       False                 True  \\\n",
       "10                                      False                 True   \n",
       "11                                      False                 True   \n",
       "12                                      False                 True   \n",
       "13                                      False                 True   \n",
       "\n",
       "                                                               fn_config   \n",
       "experiment_name_short                                                      \n",
       "9                      /root/experiments/experiments/t2c_concode_2204...  \\\n",
       "10                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "11                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "12                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "13                     /root/experiments/experiments/t2c_concode_2204...   \n",
       "\n",
       "                      experiment_name fn_train_dataset   \n",
       "experiment_name_short                                    \n",
       "9                                 NaN              NaN  \\\n",
       "10                                NaN              NaN   \n",
       "11                                NaN              NaN   \n",
       "12                                NaN              NaN   \n",
       "13                                NaN              NaN   \n",
       "\n",
       "                                       default_model  MICRO_BATCH_SIZE   \n",
       "experiment_name_short                                                    \n",
       "9                      decapoda-research/llama-7b-hf               NaN  \\\n",
       "10                     decapoda-research/llama-7b-hf               NaN   \n",
       "11                     decapoda-research/llama-7b-hf               NaN   \n",
       "12                     decapoda-research/llama-7b-hf               NaN   \n",
       "13                     decapoda-research/llama-7b-hf               NaN   \n",
       "\n",
       "                       BATCH_SIZE  ...  bleu_batch_size   \n",
       "experiment_name_short              ...                    \n",
       "9                             NaN  ...              NaN  \\\n",
       "10                            NaN  ...              NaN   \n",
       "11                            NaN  ...              NaN   \n",
       "12                            NaN  ...              NaN   \n",
       "13                            NaN  ...              NaN   \n",
       "\n",
       "                       GRADIENT_ACCUMULATION_STEPS  fn_eval_dataset   \n",
       "experiment_name_short                                                 \n",
       "9                                              NaN              NaN  \\\n",
       "10                                             NaN              NaN   \n",
       "11                                             NaN              NaN   \n",
       "12                                             NaN              NaN   \n",
       "13                                             NaN              NaN   \n",
       "\n",
       "                       log_bleu_steps_factor  eval_steps  evaluation_strategy   \n",
       "experiment_name_short                                                           \n",
       "9                                        NaN         NaN                  NaN  \\\n",
       "10                                       NaN         NaN                  NaN   \n",
       "11                                       NaN         NaN                  NaN   \n",
       "12                                       NaN         NaN                  NaN   \n",
       "13                                       NaN         NaN                  NaN   \n",
       "\n",
       "                       load_best_model_at_end epoch  num_virtual_tokens   \n",
       "experiment_name_short                                                     \n",
       "9                                         NaN   NaN                 NaN  \\\n",
       "10                                        NaN   NaN                 NaN   \n",
       "11                                        NaN   NaN                 NaN   \n",
       "12                                        NaN   NaN                 NaN   \n",
       "13                                        NaN   NaN                 NaN   \n",
       "\n",
       "                       mapping_hidden_dim  \n",
       "experiment_name_short                      \n",
       "9                                     NaN  \n",
       "10                                    NaN  \n",
       "11                                    NaN  \n",
       "12                                    NaN  \n",
       "13                                    NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_experiments[df_experiments['experiment_config.json']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "647b8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiments[df_experiments[]]\n",
    "# df_experiments['default_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a681dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiments\n",
    "# os.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba0ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = line[\"experiment_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8363592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "/root/experiments/experiments/t2c_concode_220428_v33/adapter_model.bin -> /root/experiments/t2c_concode_220428_v33/adapter_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/experiments/t2c_concode_220428_v33’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/experiments/experiments/t2c_concode_220428_v33/experiment_config.json -> /root/experiments/t2c_concode_220428_v33/experiment_config.json\n",
      "/root/experiments/experiments/t2c_concode_220428_v33/adapter_config.json -> /root/experiments/t2c_concode_220428_v33/adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "# Loading files to remote host if they are not exist\n",
    "if not os.path.exists(os.path.join(EXPERIMENT_PATH, experiment_name)):\n",
    "    print(\"loading files\")\n",
    "    #!mkdir {os.path.join(EXPERIMENT_PATH, experiment_name)}\n",
    "    os.system(f\"mkdir {os.path.join(EXPERIMENT_PATH, experiment_name)}\")\n",
    "    for filename in ['adapter_model.bin', 'experiment_config.json', 'adapter_config.json']:\n",
    "        file_from = f\"/root/experiments/experiments/{experiment_name}/{filename}\"\n",
    "        file_to   = f\"/root/experiments/{experiment_name}/{filename}\"\n",
    "        print(file_from, '->', file_to)\n",
    "        #!scp -i ~/.ssh/master_hetzner root@65.108.123.219:{file_from} {file_to} \n",
    "        os.system(f\"scp -i ~/.ssh/master_hetzner root@65.108.123.219:{file_from} {file_to} \")\n",
    "else:\n",
    "    print(\"File already here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4606951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 33M\r\n",
      "drwxr-xr-x  2 root root   88 May 26 16:00 .\r\n",
      "drwxr-xr-x 11 root root 4.0K May 26 15:59 ..\r\n",
      "-rw-r--r--  1 root root  359 May 26 16:00 adapter_config.json\r\n",
      "-rw-r--r--  1 root root  33M May 26 16:00 adapter_model.bin\r\n",
      "-rw-r--r--  1 root root  815 May 26 16:00 experiment_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/experiments/{exp_name}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34ac5c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5b3e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_model= yahma/llama-7b-hf\n"
     ]
    }
   ],
   "source": [
    "# experiment_name = \"/root/experiments/t2c_concode_220428_v38_plustwoepoch_test_copy/\"\n",
    "experiment_config = None\n",
    "default_model = line['default_model']\n",
    "\n",
    "if line['experiment_config.json']:\n",
    "    experiment_config = json.load(open(f\"{EXPERIMENT_PATH}/{experiment_name}/experiment_config.json\", \n",
    "                                           \"r\"\n",
    "                                          )\n",
    "                                     )\n",
    "    \n",
    "    default_model = json.load(open(f\"{EXPERIMENT_PATH}/{experiment_name}/experiment_config.json\", \n",
    "                                   \"r\"\n",
    "                                  )\n",
    "                             )['default_model']\n",
    "\n",
    "# params_iteration = {\"temperature\": [1.0],\n",
    "#                     \"max_new_tokens\": [300]#None, 20, 30, 35, 40, 50, 60, 70, 80, 90, 100] + [45, 47, 49, 51, 53, 55]\n",
    "#                    }\n",
    "\n",
    "print(\"default_model=\", default_model)\n",
    "# print(\"params_iteration=\", params_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312deac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM as LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer as LLaMATokenizer\n",
    "\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53facf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b3b91041e84cae8653f903774479ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LLaMAForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = default_model,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "        model = model,\n",
    "        model_id = os.path.join(EXPERIMENT_PATH, experiment_name),\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(pretrained_model_name_or_path = default_model)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "tokenizer.padding_side = \"left\"\n",
    "# not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1582db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id 0\n",
      "tokenizer.eos_token_id 2\n",
      "tokenizer.bos_token_id 1\n",
      "tokenizer.eos_token_id 2\n",
      "model.config.pad_token_id 0\n",
      "model.config.eos_token_id 2\n",
      "model.config.bos_token_id 1\n",
      "model.config.eos_token_id 2\n",
      "trainable params: 0 || all params: 6746804224 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "def verbose_model_tokenizer(model, tokenizer):\n",
    "    print(\"tokenizer.pad_token_id\", tokenizer.pad_token_id)\n",
    "    print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "    print(\"tokenizer.bos_token_id\", tokenizer.bos_token_id)\n",
    "    print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "\n",
    "    print(\"model.config.pad_token_id\", model.config.pad_token_id)\n",
    "    print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "    print(\"model.config.bos_token_id\", model.config.bos_token_id)\n",
    "    print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "    if hasattr(model, \"print_trainable_parameters\"):\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "verbose_model_tokenizer(model, tokenizer)\n",
    "\n",
    "# tokenizer.pad_token_id 0\n",
    "# tokenizer.eos_token_id 2\n",
    "# tokenizer.bos_token_id 1\n",
    "# tokenizer.eos_token_id 2\n",
    "# model.config.pad_token_id 0\n",
    "# model.config.eos_token_id 2\n",
    "# model.config.bos_token_id 1\n",
    "# model.config.eos_token_id 2\n",
    "# trainable params: 0 || all params: 6746804224 || trainable%: 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4fc1795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/HSE_diploma/prompter/templates/\n"
     ]
    }
   ],
   "source": [
    "from prompter import generate_test_prompt, get_response\n",
    "from transformers import GenerationConfig\n",
    "import math\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca69b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_dict = {\"temperature\": 1.0,\n",
    "                          #\"penalty_alpha\": 2,\n",
    "                          \"max_new_tokens\": 500\n",
    "                         }\n",
    "\n",
    "generation_config = GenerationConfig(**generation_config_dict)\n",
    "\n",
    "fn_test_data = \"../data/t2c_answers.json\"\n",
    "fn_etalon = \"/root/data/answers.json\"\n",
    "batch_size = 10\n",
    "verbose = False\n",
    "\n",
    "assert model.training == False\n",
    "\n",
    "lst = json.load(open(fn_test_data, 'rb'))\n",
    "inputs = lst\n",
    "\n",
    "prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "prompts = np.array(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f76d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts[0]\n",
    "# model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de22675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|███████████████████████████████████████████| 10/10 [02:33<00:00, 15.39s/it]\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "n = math.ceil(len(prompts)/batch_size)\n",
    "\n",
    "for ind in tqdm.tqdm(range(n)):\n",
    "    current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "    if verbose:\n",
    "        print(ind * batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "\n",
    "    tokenized_inputs = tokenizer(list(current_prompts), \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 return_tensors=\"pt\"\n",
    "                                ).to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        full_output = model.generate(\n",
    "            **tokenized_inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "    res_list.extend(tokenizer.batch_decode(full_output, \n",
    "                                           skip_special_tokens=False\n",
    "                                          )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136243ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac4c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08def572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-20:45:26\n"
     ]
    }
   ],
   "source": [
    "res_dict = {\"real_config\": experiment_config,\n",
    "            \"meta_config\": line,\n",
    "            \"generation_config\": generation_config_dict,\n",
    "            \"fn_test_data\": fn_test_data,\n",
    "            \"fn_etalon\": fn_etalon,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"verbose\": verbose,\n",
    "            \"prompts\": list(prompts),\n",
    "            \"res_list\": res_list\n",
    "           }\n",
    "\n",
    "fn_output = ARTIFACTS_PATH +\\\n",
    "            \"predictions\" +\\\n",
    "            exp_name+\"_\"+str(datetime.now()).split('.')[0].replace(' ', '-')\n",
    "print(fn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b2d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(res_dict, \n",
    "          open(fn_output+'.json', \"w+\")\n",
    "         )\n",
    "\n",
    "pickle.dump(res_dict,\n",
    "            open(fn_output+'.pickle', \"wb\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3c2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    #ToDo rewrite it using Promt Template\n",
    "    s = get_response(s)\n",
    "    if \"</s>\" in s:\n",
    "        s = s.split('</s>')[0]\n",
    "    if \"### Input\" in s:\n",
    "        s = s.split(\"### Input\")[0]\n",
    "    try:\n",
    "        assert \"<unk>\" not in s\n",
    "        assert \"\\n\" not in s\n",
    "        assert \"  \" not in s\n",
    "    except Exception as e:\n",
    "        print(\"Something wrong with\")\n",
    "        print(s)\n",
    "        raise e\n",
    "#     #s = s.split('### Response:\\n')[-1]\n",
    "#     #s = s.replace('\\n', '  ')\n",
    "#     s = s.replace('<unk>', \" \")\n",
    "#     s = ' '.join(s.split(' ')[:100])\n",
    "#     while '  ' in s:\n",
    "#         s = s.replace('  ', ' ')\n",
    "\n",
    "#     if len(s) > 0 and s[0] == ' ':\n",
    "#         s = s[1:]\n",
    "\n",
    "#     if self.verbose:\n",
    "#         print(s)\n",
    "\n",
    "    return s\n",
    "# preprocess(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc89549",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [preprocess(s) for s in res_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16453cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9321"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(fn_output+\".txt\", 'w+').write('\\n'.join(predictions_list))\n",
    "# 9321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d505fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-20:45:26.txt\n"
     ]
    }
   ],
   "source": [
    "fn = fn_output+\".txt\"\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1caf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/temperary_results/\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:34:58.json\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:36:00.json\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:36:00.pickle\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:36:00.txt\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:46:49.json\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:46:49.pickle\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-16:46:49.txt\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-20:45:26.json\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-20:45:26.pickle\r\n",
      "predictionst2c_concode_220428_v33_2023-05-26-20:45:26.txt\r\n"
     ]
    }
   ],
   "source": [
    "print(ARTIFACTS_PATH)\n",
    "!ls {ARTIFACTS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c71cc3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\t   calculate_bleu     parser\t       utils.py\r\n",
      "bleu.py\t\t   dataflow_match.py  res.txt\t       weighted_ngram_match.py\r\n",
      "calc_code_bleu.py  keywords\t      syntax_match.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/HSE_diploma/evaluateFromCodeXGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa6232ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 25.78, EM: 18.0\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': 25.78,\n",
       " 'EM': 18.0,\n",
       " 'CodeBLEU': 0.2934164199083868,\n",
       " 'ngram_match': 0.2572881371767979,\n",
       " 'weighted_ngram_match': 0.2792407600329364,\n",
       " 'syntax_match': 0.3445378151260504,\n",
       " 'dataflow_match': 0.29259896729776247,\n",
       " 'precisions': '[0.6750727449078564, 0.5005091649694501, 0.3654876741693462, 0.27303561334087056]',\n",
       " 'bp': 0.6016942684373926,\n",
       " 'ratio': 0.6631274131274131,\n",
       " 'translation_length': 2061,\n",
       " 'reference_length': 3108,\n",
       " 'raw': 'ngram match: 0.2572881371767979, weighted ngram match: 0.2792407600329364, syntax_match: 0.3445378151260504, dataflow_match: 0.29259896729776247\\nCodeBLEU score:  0.2934164199083868\\nBLEU: 25.78, EM: 18.0\\nprecisions:  [0.6750727449078564, 0.5005091649694501, 0.3654876741693462, 0.27303561334087056]\\nbp:  0.6016942684373926\\nratio:  0.6631274131274131\\ntranslation_length:  2061\\nreference_length:  3108\\n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def run_evaluation_of_metrics(fn = \"/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-16:46:49.txt\"):\n",
    "    open(\"/root/HSE_diploma/load_all_experiments/res.txt\", \"w+\").write('')\n",
    "    !cd /root/HSE_diploma/evaluateFromCodeXGlue/;python3 calc_code_bleu.py --refs /root/data/reference_corpus.txt --hyp $fn --lang java --params 0.25,0.25,0.25,0.25 > /root/HSE_diploma/load_all_experiments/res.txt\n",
    "    !cd /root/HSE_diploma/evaluateFromCodeXGlue/;python3 calculate_bleu/evaluator.py -a=/root/data/answers.json -p=$fn  >> /root/HSE_diploma/load_all_experiments/res.txt\n",
    "\n",
    "\n",
    "def parse_metrics(fn_metrics = \"res.txt\"):\n",
    "    s = open('res.txt', \"r\").read()\n",
    "\n",
    "    BLEU = re.findall(\"BLEU: \\d+\\.\\d+\", s)\n",
    "    EM = re.findall(\"EM: \\d+\\.\\d+\", s)\n",
    "    CodeBLEU = re.findall(\"CodeBLEU score:  \\d+\\.\\d+\", s)\n",
    "\n",
    "    assert len(BLEU) == len(EM) == len(CodeBLEU) == 1\n",
    "    BLEU = float(BLEU[0].split(' ')[-1])\n",
    "    EM = float(EM[0].split(' ')[-1])\n",
    "    CodeBLEU = float(CodeBLEU[0].split(' ')[-1])\n",
    "\n",
    "    ngram_match = float(re.findall(\"ngram match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "    weighted_ngram_match = float(re.findall(\"weighted ngram match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "    syntax_match = float(re.findall(\"syntax_match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "    dataflow_match = float(re.findall(\"dataflow_match: \\d+\\.\\d+\", s)[0].split(' ')[-1]) \n",
    "\n",
    "    precisions = re.findall(\"precisions:\\s+.+\\n\", s)[0].split(':  ')[-1][:-1]\n",
    "    bp = float(re.findall(\"bp:  \\d+\\.\\d+\", s)[0].split(' ')[-1])\n",
    "    ratio = float(re.findall(\"ratio:  \\d+\\.\\d+\", s)[0].split(' ')[-1])\n",
    "    translation_length = int(re.findall(\"translation_length:.*\\n\", s)[0].split(' ')[-1][:-1])\n",
    "    reference_length = int(re.findall(\"reference_length:.*\\n\", s)[0].split(' ')[-1][:-1])\n",
    "\n",
    "    # weighted ngram match: 0.2792407600329364, \n",
    "    # syntax_match: 0.3445378151260504, \n",
    "    # dataflow_match: 0.2908777969018933\n",
    "    metrics = {\"BLEU\": BLEU,\n",
    "               \"EM\": EM,\n",
    "               \"CodeBLEU\": CodeBLEU,\n",
    "               \"ngram_match\": ngram_match,\n",
    "               \"weighted_ngram_match\": weighted_ngram_match,\n",
    "               \"syntax_match\": syntax_match,\n",
    "               \"dataflow_match\": dataflow_match,\n",
    "               \"precisions\": precisions,\n",
    "               \"bp\": bp,\n",
    "               \"ratio\": ratio,\n",
    "               \"translation_length\": translation_length,\n",
    "               \"reference_length\": reference_length,\n",
    "               \"raw\": s\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "run_evaluation_of_metrics(fn = \"/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-16:36:00.txt\")\n",
    "final_metrics = parse_metrics()\n",
    "\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "392cd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict['final_metrics'] = final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ac5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(res_dict, \n",
    "          open(fn_output+'_final_metrics.json', \"w+\")\n",
    "         )\n",
    "\n",
    "pickle.dump(res_dict,\n",
    "            open(fn_output+'_final_metrics.pickle', \"wb\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ca30b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/temperary_results/predictionst2c_concode_220428_v33_2023-05-26-20:45:26'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0707da5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 25.78,\n",
       " 'EM': 18.0,\n",
       " 'CodeBLEU': 0.29298612730941953,\n",
       " 'ngram_match': 0.2572881371767979,\n",
       " 'weighted_ngram_match': 0.2792407600329364,\n",
       " 'syntax_match': 0.3445378151260504,\n",
       " 'dataflow_match': 0.2908777969018933,\n",
       " 'precisions': '[0.6750727449078564, 0.5005091649694501, 0.3654876741693462, 0.27303561334087056]',\n",
       " 'bp': 0.6016942684373926,\n",
       " 'ratio': 0.6631274131274131,\n",
       " 'translation_length': 2061,\n",
       " 'reference_length': 3108,\n",
       " 'raw': 'ngram match: 0.2572881371767979, weighted ngram match: 0.2792407600329364, syntax_match: 0.3445378151260504, dataflow_match: 0.2908777969018933\\nCodeBLEU score:  0.29298612730941953\\nBLEU: 25.78, EM: 18.0\\nprecisions:  [0.6750727449078564, 0.5005091649694501, 0.3654876741693462, 0.27303561334087056]\\nbp:  0.6016942684373926\\nratio:  0.6631274131274131\\ntranslation_length:  2061\\nreference_length:  3108\\n'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6863a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
