{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060ffa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc67ea1873884fd1a945dc322e8cbe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "    ### Instruction:\n",
      "    Generate java code\n",
      "actually walks the bag to make sure the count is correct and resets the running total \n",
      "\n",
      "    ### Input:\n",
      "     Object _current \n",
      " int _total \n",
      " DefaultMapBag _parent \n",
      " Map _map \n",
      " int _mods \n",
      " Iterator _support \n",
      "\n",
      " boolean add \n",
      " boolean add \n",
      " Object next \n",
      " boolean containsAll \n",
      " boolean containsAll \n",
      " void clear \n",
      " boolean isEmpty \n",
      " boolean hasNext \n",
      " void remove \n",
      " boolean remove \n",
      " boolean remove \n",
      " Map getMap \n",
      " int modCount \n",
      " boolean contains \n",
      " Iterator iterator \n",
      " boolean removeAll \n",
      " int size \n",
      " boolean addAll \n",
      " int hashCode \n",
      " boolean equals \n",
      " Object[] toArray \n",
      " Object[] toArray \n",
      " Set uniqueSet \n",
      " void setMap \n",
      " String toString \n",
      " int getCount \n",
      " List extractList \n",
      " boolean retainAll \n",
      " boolean retainAll\n",
      "\n",
      "    ### Response:\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▋                                        | 1/12 [00:17<03:15, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▎                                    | 2/12 [01:08<06:08, 36.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 27 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▎                                    | 2/12 [01:17<06:29, 38.99s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "\n",
    "# from transformers import LLamaTokenizer, LLamaForCausalLM, GenerationConfig\n",
    "from transformers import LLaMAForCausalLM, LLaMATokenizer, GenerationConfig\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "def inference(default_model, \n",
    "              experiment_name,\n",
    "              test_dataset = 't2c_concode/t2c_answers.json',\n",
    "              fn_answers = \"/root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json\"\n",
    "             ):\n",
    "    # load everything\n",
    "    BASE_MODEL = default_model\n",
    "    LORA_WEIGHTS = experiment_name #\"tloen/alpaca-lora-7b\"\n",
    "\n",
    "    model = LLaMAForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        LORA_WEIGHTS,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    tokenizer = LLaMATokenizer.from_pretrained(BASE_MODEL)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    # not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # def generate_prompt(instruction, input):\n",
    "    #     return f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "\n",
    "    def generate_test_prompt(data_point, train = False):\n",
    "        # To decrease expectations of results :)\n",
    "        assert train == False\n",
    "        # sorry about the formatting disaster gotta move fast\n",
    "        if data_point[\"input\"]:\n",
    "            return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    ### Instruction:\n",
    "    {data_point[\"instruction\"]}\n",
    "\n",
    "    ### Input:\n",
    "    {data_point[\"input\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"output\"] if train else ''}\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    ### Instruction:\n",
    "    {data_point[\"instruction\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"output\"] if train else ''}\"\"\"\n",
    "\n",
    "\n",
    "    lst = json.load(open(test_dataset, 'rb'))\n",
    "    inputs = lst# [lst[0]]\n",
    "    # instruction = 'Combine the question and answer into an image caption as succinctly as possible. Be sure to include the phrase \"a photo of\". Do not draw false conclusions.'\n",
    "    # inputs = ['Is this a baseball game? yes', 'Is this a baseball game? no']\n",
    "    prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "    print(prompts[0])\n",
    "    prompts = np.array(prompts)\n",
    "    batch_size = 9\n",
    "    res_list = []\n",
    "    for ind in tqdm.tqdm(range(math.ceil(len(prompts)/batch_size))):\n",
    "        current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "        print(ind*batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "\n",
    "        tokenized_inputs = tokenizer(list(current_prompts), \n",
    "                                     padding=True, \n",
    "                                     truncation=True, \n",
    "                                     return_tensors=\"pt\"\n",
    "                                    ).to('cuda')\n",
    "\n",
    "        generation_config = GenerationConfig(max_new_tokens=128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            full_output = model.generate(\n",
    "                **tokenized_inputs,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "\n",
    "        res_list.extend(tokenizer.batch_decode(full_output, skip_special_tokens=False))\n",
    "    #     print(tokenizer.batch_decode(full_output, skip_special_tokens=False)[-1])\n",
    "\n",
    "\n",
    "        def preprocess(s, verbose = False):\n",
    "            s = s.split('### Response:\\n')[-1]\n",
    "            s = s.replace('\\n', '  ')\n",
    "            s = s.replace('<unk>', \" \")\n",
    "            s = ' '.join(s.split(' ')[:100])\n",
    "            while '  ' in s:\n",
    "                s = s.replace('  ', ' ')\n",
    "\n",
    "            if len(s) > 0 and s[0] == ' ':\n",
    "                s = s[1:]\n",
    "            \n",
    "            if verbose:\n",
    "                print(s)\n",
    "            \n",
    "            return s\n",
    "\n",
    "\n",
    "    predict_list = []\n",
    "    for s in tqdm.tqdm(res_list):\n",
    "        predict_list.append(preprocess(s))\n",
    "\n",
    "    from datetime import datetime\n",
    "    output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "    current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "    print(\"Save results at file\", current_pred_name)\n",
    "\n",
    "    with open(current_pred_name, 'w+') as f:\n",
    "        f.write('\\n'.join(predict_list))\n",
    "    #     f.write('\\n')\n",
    "\n",
    "    command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a {fn_answers} -p {current_pred_name}\"\n",
    "    result = subprocess.run(command.split(' '), \n",
    "                            stderr=subprocess.PIPE)\n",
    "    result.stderr\n",
    "    results = result.stderr.decode()\n",
    "    print(results)\n",
    "    \n",
    "    return {\"metric_string\": results,\n",
    "            \"prompts_list\": prompts,\n",
    "            \"res_list\": res_list,\n",
    "            \"predict_list\": predict_list\n",
    "           }\n",
    "\n",
    "inference(default_model = \"decapoda-research/llama-7b-hf\", \n",
    "          experiment_name = \"t2c_concode_220428_v6\"\n",
    "         )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e73405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DATA_LICENSE\r\n",
      " Dockerfile\r\n",
      " LICENSE\r\n",
      " README.md\r\n",
      "'[backup]_batch_size_changes_generate_output.ipynb'\r\n",
      " alpaca_data.json\r\n",
      " alpaca_data_cleaned_archive.json\r\n",
      " alpaca_data_gpt4.json\r\n",
      " create_t2c_dataset_and_show_max_pad_size.ipynb\r\n",
      " docker-compose.yml\r\n",
      " export_hf_checkpoint.py\r\n",
      " export_state_dict_checkpoint.py\r\n",
      " finetune-original.py\r\n",
      " finetune.py\r\n",
      " flagged\r\n",
      " generate.py\r\n",
      " img\r\n",
      " inference.py\r\n",
      " inference_v1.ipynb\r\n",
      " inference_v1_old.ipynb\r\n",
      " lengths-Copy1.ipynb\r\n",
      " lora-alpaca\r\n",
      " lora-alpaca_finetuned\r\n",
      " lora-alpaca_finetuned_gpu\r\n",
      " prompter.ipynb\r\n",
      " pyproject.toml\r\n",
      " requirements.txt\r\n",
      " res.txt\r\n",
      " t2c_concode\r\n",
      " t2c_concode_220428\r\n",
      " t2c_concode_220428_finetune_v7\r\n",
      " t2c_concode_220428_finetune_v8\r\n",
      " t2c_concode_220428_v3\r\n",
      " t2c_concode_220428_v4\r\n",
      " t2c_concode_220428_v5\r\n",
      " t2c_concode_220428_v6\r\n",
      " t2c_concode_220428_v6123\r\n",
      " tb_logs\r\n",
      " templates\r\n",
      " utils\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dee9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"t2c_concode_220428_finetune_v8\"\n",
    "# experiment_name = \"t2c_concode_220428_finetune_v7\"\n",
    "# experiment_name = \"t2c_concode_220428_v6\"\n",
    "# experiment_name = \"t2c_concode_220428_v4\"\n",
    "# experiment_name = \"t2c_concode_220428\"\n",
    "# experiment_name = \"tloen/alpaca-lora-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2446c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "\n",
    "# from transformers import LLamaTokenizer, LLamaForCausalLM, GenerationConfig\n",
    "from transformers import LLaMAForCausalLM, LLaMATokenizer, GenerationConfig\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e555675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'lora-alpaca/checkpoint-30': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah lora-alpaca/checkpoint-30\n",
    "# experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60544e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78687c2aead54394b29cdcfb5afbc406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LLaMAForCausalLM(\n",
       "      (model): LLaMAModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LLaMADecoderLayer(\n",
       "            (self_attn): LLaMAAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LLaMAMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): RMSNorm()\n",
       "            (post_attention_layernorm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "LORA_WEIGHTS = experiment_name #\"tloen/alpaca-lora-7b\"\n",
    "\n",
    "model = LLaMAForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    LORA_WEIGHTS,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(BASE_MODEL)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "tokenizer.padding_side = \"left\"\n",
    "# not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cdd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.from_pretrained(\"decapoda-research/llama-7b-hf\", \"lora-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9801b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Generate java code\n",
      "actually walks the bag to make sure the count is correct and resets the running total \n",
      "\n",
      "### Input:\n",
      " Object _current \n",
      " int _total \n",
      " DefaultMapBag _parent \n",
      " Map _map \n",
      " int _mods \n",
      " Iterator _support \n",
      "\n",
      " boolean add \n",
      " boolean add \n",
      " Object next \n",
      " boolean containsAll \n",
      " boolean containsAll \n",
      " void clear \n",
      " boolean isEmpty \n",
      " boolean hasNext \n",
      " void remove \n",
      " boolean remove \n",
      " boolean remove \n",
      " Map getMap \n",
      " int modCount \n",
      " boolean contains \n",
      " Iterator iterator \n",
      " boolean removeAll \n",
      " int size \n",
      " boolean addAll \n",
      " int hashCode \n",
      " boolean equals \n",
      " Object[] toArray \n",
      " Object[] toArray \n",
      " Set uniqueSet \n",
      " void setMap \n",
      " String toString \n",
      " int getCount \n",
      " List extractList \n",
      " boolean retainAll \n",
      " boolean retainAll\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▋                                        | 1/12 [00:54<09:55, 54.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▎                                    | 2/12 [01:48<09:02, 54.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 27 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████                                 | 3/12 [02:35<07:37, 50.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 36 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 4/12 [03:30<07:00, 52.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 45 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████▎                         | 5/12 [04:24<06:12, 53.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 54 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 6/12 [05:18<05:20, 53.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 63 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████████████████████████▋                  | 7/12 [06:12<04:27, 53.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 72 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 8/12 [07:06<03:34, 53.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 81 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████           | 9/12 [07:59<02:40, 53.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 90 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████▊       | 10/12 [08:54<01:47, 53.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 99 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████▍   | 11/12 [09:48<00:53, 53.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 108 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [10:05<00:00, 50.42s/it]\n",
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 76959.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Generate java code actually walks the bag to make sure the count is correct and resets the running total ### Input: Object _current int _total DefaultMapBag _parent Map _map int _mods Iterator _support boolean add boolean add Object next boolean containsAll boolean containsAll void clear boolean isEmpty \n",
      "### Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ##\n",
      "String _db4oVersion byte _db4oHeaderVersion String fileName String fileName String getTempPath void runDefrag void checkDatabaseFile void update int db4oMajorVersion void checkUpdatedDatabaseFile void investigateFileHeaderVersion boolean isApplicableForDb4oVersion void withDatabase void deconfigure void setUp String fileNamePrefix void assertObjectDeletion void assertObjectsAreReadable String databasePath void configureFor\n",
      "String getEncryptionKeyTypeZNode String getBaseZNode String getDisabledZNode String getKeyTypeZNode String getEncryptionKeyNoZNode String getEncryptionKeyZNode String getLockedZNode String getCreatedTimeZNode String getExpireTimeZNode String getKeyVersionZNode String getKeysZNode String getIdentitiesZNode List<String> getIdentityNames String getPrincipalNameZnode String getKdcFlagsZNode ##\n",
      "\n",
      "### Instruction: Generate java code sets the tag position if one is associated with the nls element. ### Input: Region fTagPosition String TAG_POSTFIX AccessorClassReference fAccessorClassReference boolean fIsEclipseNLS int TAG_PREFIX_LENGTH String TAG_PREFIX int fIndex int TAG_POSTFIX_LENGTH String fValue Region fPosition String getValue Region getPosition boolean isEclipseNLS \n",
      "``` UnivariateRealSolverFactory factory = new UnivariateRealSolverFactory(); UnivariateRealSolver solver = factory.getSolver(bracket, bracket, midpoint); ``` ### Example: ``` UnivariateRealSolverFactory factory = new UnivariateRealSolverFactory(); UnivariateRealSolver solver = factory.getSolver(new double[]{0, 0}, new double[]{0, 0}, 0); ``` ### Example: ``` UnivariateRealSol\n",
      "boolean gotBug1Event = false; boolean hasHierarchyEventGenerationBug = false; boolean hasKeyStrokeGenerationBug = false; boolean hasRobotMotionBug = false; boolean reportsIncorrectLockingKeyState = false; boolean fileDialogMisreportsBounds = false; boolean needsRobotVerification = false; boolean hasMenuDisableBug = false; boolean hasInputMethodInsteadOfKeyTyped = false; boolean hasMissingWindowMouseMotion = false; boolean hasEscapeGenerationB\n",
      "String getName String getName boolean isDynamic List<String> getWorkspaceSchemaPath SqlIntervalQualifier getIntervalQualifier boolean hasStar TimeUnit getEndUnit RelDataType getRowType Integer getPrecision void setSql String getSql TimeUnit getStartUnit List<FieldType> getFields Integer getScale SqlTypeName getType Boolean getIsNullable ### Output: String getName String getName boolean isDynamic \n",
      "PlaceHolder placeHolder Drawable bitmapToDrawable void closeInputStream Bitmap drawableToBitmap Bitmap byteToBitmap Bitmap getBitmapFromUrl Bitmap getBitmapFromUrl Bitmap scaleImage byte[] bitmapToByte Bitmap scaleImageTo Drawable byteToDrawable Drawable getDrawableFromUrl Drawable getDrawableFromUrl InputStream getInputStreamFromUrl InputStream getInputStreamFromUrl ### Output: PlaceHolder placeHolder Drawable bitmapToDrawable void closeInputStream \n",
      "### Instruction: Generate java code resets migration flag. should only be used in tests. ### Input: String APP_PREFS_NAME String PROFILE_PREFS_NAME_PREFIX String[] PROFILE_MIGRATIONS_0_TO_1 EnumSet<Flags> disableMigrations boolean migrationDone int PREFS_VERSION String LOGTAG String PREFS_VERSION_KEY int getVersion void migrateIfNecessary \n",
      "void f11(int I1, int I2, int I3, int I4) { int I1z = I1; int I2z = I2; int I1z = I1; int I2z = I2; int I1z = I1; int I2z = I2; int I1z = I1; int I2z = I2; int I1z = I1; int\n",
      "\n",
      "### Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ##\n",
      "### Output: ### Expected Output: ### Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Expected Error: ### Ex\n",
      "\\begin{code} \\begin{pre} \\end{code} \\end{code} \\end{code} \\end{blockquote} \\begin{code} import java.util.Arrays; import java.util.List; import java.util.stream.Collectors; public class Main { public static void main(String[] args) { List<String> list = Arrays.asList(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n",
      "public void setColorPortada(String colorPortada) { this.colorPortada = colorPortada; } public String getColorPortada() { return colorPortada; } public void setDimensiones(int dimensiones) { this.dimensiones = dimensiones; } public int getDimensiones() { return dimensiones; } public void setTitulo(String titulo) { this.titulo = titulo; } public String getTitulo() {\n",
      "IJavaElement[] codeResolveForked = { selectJavaElement, codeResolve, codeResolve, codeResolve, codeResolve, performForkedCodeResolve, canOperateOn, getStructuredSelection, getInputAsClassFile, codeResolveForked, getInput, getTypeAtOffset, codeResolveOrInputForked, getInputAsCompilationUnit, getElements, \n",
      "``` LoadPlan buildRootCollectionLoadPlan = new LoadPlan(); buildRootCollectionLoadPlan.setRootEntity(placeHolder); buildRootCollectionLoadPlan.setFetches(new ArrayList<FetchPlan>()); buildRootCollectionLoadPlan.setFetches(new ArrayList<FetchPlan>()); buildRootCollectionLoadPlan.setFetches(new ArrayList<FetchPlan>()); buildRootCollectionLoadPlan.setFetches(new ArrayList<FetchPlan>()); buildRootCollectionLoadPlan.setFetches(new ArrayList<FetchPlan>()); \n",
      "### Instruction: Generate java code compute a single body-body or\n",
      "String nameKey String name String catalogName boolean optional String nameKey String name String catalogName boolean postprocess Command getCommand boolean isOptional boolean execute void setOptional ### Example: String name = \"myName\"; String catalogName = \"myCatalog\"; boolean optional = false; String nameKey = \"myNameKey\"; CatalogSearcher searcher = new CatalogSearcher(name, catalogName, optional, nameKey); String\n",
      "ColumnDescriptor<T>[] columns ### Examples: ### Input: ColumnFileMetaData metaData Map<String,ColumnDescriptor> columnsByName Input file ColumnDescriptor[] columns long rowCount int columnCount ColumnMetaData[] getColumnMetaData ColumnMetaData getColumnMetaData ColumnMetaData getColumnMetaData long getRowCount List<ColumnMetaData> getRoots void readColumnMetaData void readColumnStarts ColumnDescriptor<T> get\n",
      "``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` \n",
      "\n",
      "Buffer outputBuffer Codec codec Buffer inputBuffer ArrayList<VideoTrack> tracks String nameFormat Format videoFormat int count int width Format fileFormat File dir int height boolean isVFRSupported Format getFileFormat boolean isEmpty void writeSamples int getTrackCount int addTrack int addVideoTrack void writeSample boolean\n",
      "boolean configured boolean useRanks double zipfExp long meanReorderIntvl RandomGenerator rand String name ZipfDistribution dist int startRank Logger log List<Integer> keyPopularity int size void setName(String name) { this.name = name; } String getName() { return name; } int getKeyForPublish() { return keyPopularity.get(0);\n",
      "List list = new ArrayList(); addScript(new Script(\"script1.js\")); addScript(new Script(\"script2.js\")); addScript(new Script(\"script3.js\")); compile(); trimWhitespace(); System.out.println(list); ### Output: List list = new ArrayList(); addScript(new Script(\"script1.js\")); addScript(new Script(\"script2.js\")); addScript(new Script(\"script3.js\")); compile(); trimWhitespace(); System.out\n",
      "\n",
      "String javaCode ### Examples: ### Instruction: Generate java code get the docker rest uri. ### Input: String address int port String host String DEFAULT_HOST String DEFAULT_UNIX_ENDPOINT int DEFAULT_PORT URI uri String dockerCertPath int defaultPort DockerHost fromEnv String address int port String host DockerHost\n",
      "### Output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ##\n",
      "DocumentBuilderFactory newInstance boolean isValidating void setAttribute Object getAttribute boolean isExpandEntityReferences DocumentBuilder newDocumentBuilder void setValidating void setIgnoringComments void setExpandEntityReferences boolean isIgnoringComments ### Output: DocumentBuilderFactory newInstance boolean isValidating void setAttribute Object getAttribute boolean isExpandEntityReferences DocumentBuilder newDocumentBuilder void setValidating void setIgnoringComments void\n",
      "int getMax = 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "void warn(String name, boolean is12, Logger logger, String FQCN) { logger.warn(name, is12); } void warn(String name, boolean is12, Logger logger, String FQCN) { logger.warn(name, is12); } void trace(String name, boolean is12, Logger logger, String FQCN) { logger.trace(name, is12); } void trace(String name, boolean is12, Logger logger, String FQCN)\n",
      "List<Element> getAllChildElemsMatching = placeHolder.getElementsByTagName(\"div\"); ### Instruction: Generate java code returns the first child element of a node that\n",
      "String getFcKey String getDisplayName List<PropertyDeclaration> getProperties void setFcKey String getResourceType void setFcLink String getLocalPortID void setResourceType void setId ProvidedPortID getId void setLocalPortID void setDisplayName ### Output: String getFcKey String getDisplayName List<PropertyDeclaration> getProperties void setFcKey String getResourceType void set\n",
      "### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: \n",
      "### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: \n",
      "SolvableGroebnerBase<C> sbb boolean debug Logger logger List<GenSolvablePolynomial<C>> leftGB ModuleList<C> leftGB boolean isTwosidedGB boolean isTwosidedGB List<GenSolvablePolynomial<C>> rightGB ModuleList<C> rightGB boolean isLeftGB boolean isLeftGB boolean isRightGB boolean isRightGB ### Output: SolvableGroebnerBase<\n",
      "### Output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ##\n",
      "``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` \n",
      "Local<C> localAbsoluteValue(Local<C> num) { Local<C> localAbs = Local.newLocal(); if (num.isZERO()) { localAbs.set(0); } else { localAbs.set(num.negate().multiply(num).subtract(1).gcd().divide(num).subtract(1).negate().multiply(num).subtract(1).gcd().divide(num).subtract(1).negate().\n",
      "String nullStrToEmpty = \"\"; Long[] transformLongArray = new Long[10]; long[] transformLongArray = new long[10]; int compare = 0; Integer[] transformIntArray = new Integer[10]; int[] transformIntArray = new int[10]; for (int i = 0; i < transformIntArray.length; i++) { transformIntArray[i] = i; } for (int i = 0; i < transformIntArray.length; i++) { transformIntArray[\n",
      "String name = \"name\"; boolean[] probes = new boolean[10]; long id = 1000; String getName() { return name; } void merge(String name, boolean[] probes, long id) { this.name = name; this.probes = probes; this.id = id; } void merge(String name, boolean[] probes, long id) { this.name = name; this.probes = probes; \n",
      "createConsumedPort createApplicationPayload createApplicationReleaseBinding createApplication createPropertyDeclaration createApplicationID createProvidedPort ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output\n",
      "\n",
      "UnivPowerSeries<C> getTAN boolean isField boolean isCommutative UnivPowerSeries<C> getSIN UnivPowerSeries<C> parse UnivPowerSeries<C> parse UnivPowerSeries<C> fromPolynomial java.math.BigInteger characteristic UnivPowerSeries<C> getEXP UnivPowerSeries<C> random UnivPowerSeries<C> random UnivPowerSeries<C> random UnivPowerSeries<C> random \n",
      "bringWorkbenchToFront(getWorkbenchWindow(), getWorkbench()); ### Output: bringWorkbenchToFront(getWorkbenchWindow(), getWorkbench()); ### Expected output: bringWorkbenchToFront(getWorkbenchWindow(), getWorkbench()); ### Expected output: bringWorkbenchToFront(getWorkbenchWindow(), getWorkbench()); ### Expected output: bringWorkbenchToFront(getWorkbenchWindow\n",
      "WildcardMatcher excludesMatcher = new WildcardMatcher(); excludesMatcher.setExcludes(excludes); excludesMatcher.setLimits(limits); excludesMatcher.setIncludes(includes); excludesMatcher.setElement(element); excludesMatcher.setExcludes(excludes); excludesMatcher.setLimits(limits); excludesMatcher.setIncludes(includes); excludesMatcher.setElement(element); excludesMatcher.setExcludes(excludes);\n",
      "String str = \"123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\n",
      "### Output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ##\n",
      "``` Map<String,JsonBall> metadata = new HashMap<>(); metadata.put(\"name\", new JsonBall(\"friendly\", \"machine\")); metadata.put(\"created\", new Date(\"2016-01-01\")); metadata.put(\"type\", new JsonBall(\"machine\", \"machine\")); metadata.put(\"ips\", new HashSet<String>()); metadata.put(\"ips\", new HashSet<String>()); metadata.put(\"metadata\", new HashMap<String,JsonBall>()); metadata.put(\"metadata\", new HashMap<String\n",
      "boolean useCriterion4 = false; ColorPolynomial<C> pi = new ColorPolynomial<C>(i, pj, j); boolean useCriterion3 = false; int n = 0; boolean toZero = false; int getPairNumber = 0; int hashCode = 0; boolean equals = false; boolean getUseCriterion3 = false; boolean isZero = false; String toString = \"\"; int compareTo = 0; boolean pairNumber = false; boolean getUseCriterion4\n",
      "### Output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ##\n",
      "\n",
      "\n",
      "### Output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ### Expected output: ##\n",
      "if (sendToConsole) { System.out.println(placeHolder); } ### Output: if (sendToConsole) { System.out.println(placeHolder); } ### Expected Output: if (sendToConsole) { System.out.println(placeHolder); } ### Expected Output: if (sendToConsole) { System.out.println(placeHolder); } ### Expected Output: if (sendToConsole) { \n",
      "String[] getSkipDays = new String[skipDays.size()]; for (int i = 0; i < skipDays.size(); i++) { getSkipDays[i] = skipDays.get(i); } String[] findSkipDays = new String[skipDays.size()]; for (int i = 0; i < skipDays.size(); i++) { findSkipDays[i] = skipDays.get(i); } String[] findSkipHours = new String[skipHours.\n",
      "### Instruction: Generate java code getter for property ` name '. ### Input: CoreMessageLogger LOG boolean containsJoinFetchedCollection Fetch bagJoinFetch String name boolean containsJoinFetchedBag Map<String,Fetch> fetches Map<String,Fetch> getFetches boolean isContainsJoinFetchedBag void addFetch void addFetch void addFetch Fetch getFetchByRole boolean isContainsJoinFetchedCollection ### Response:\n",
      "\n",
      "GenVector<C> random GenVector<C> random GenVector<C> random GenVector<C> random GenVector<C> fromList int hashCode boolean equals GenVector<C> fromInteger GenVector<C> fromInteger String toString GenVector<C> copy GenVector<C> parse GenVector<C> parse ### Output: GenVector<C> random GenVector<C> random GenVector<C> random \n",
      "List<ListBlock> getListBlocks() { List<ListBlock> listBlocks = new ArrayList<ListBlock>(); for (int i = 0; i < getNumberOfLists(); i++) { listBlocks.add(getListBlock()); } return listBlocks; } ListBlock getListBlock() { ListBlock listBlock = new ListBlock(); for (int i = 0; i < getNumberOfListBlocks(); i++) { \n",
      "### Instruction: Generate java code this method is invoked at the server before the response is executed, but before the response has been formulated ###\n",
      "### Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ##\n",
      "\n",
      "### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: ### Output: \n",
      "``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` ``` \n",
      "String getJavaCodeString String getWorkingSetLabel String getURLPart String getJavaElementName String getFilePattern String getResourceName String getVersionName String getFileName ### Examples: ### Instruction: Generate java code returns the label of a path. ### Input: String CODE_DELIMITERS String FILE_PATTERN_DELIMITERS String URL_DELIMITERS String getJavaCodeString String getWork\n",
      "### Instruction: Generate java code answer the condition monitor ### Input: ConditionMonitor INSTANCE void add void add void removeAll int\n",
      "long getIterations = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "GenMatrix<C> randomUpper = new GenMatrix<C>(rows, cols, randomUpper); GenMatrix<C> randomLower = new GenMatrix<C>(rows, cols, randomLower); GenMatrix<C> product = new GenMatrix<C>(rows, cols, randomUpper, randomLower); GenMatrix<C> fromList = new GenMatrix<C>(rows, cols, fromList); boolean isField = false; boolean isCommutative = false; GenMatrix<C> parse = new GenMatrix<C>(rows, cols, parse); Gen\n",
      "Archive<?> createDeployment = new JarDeployment(); createDeployment.setName(\"test.jar\"); createDeployment.setClassPathManifestAttribute(\"test.jar\"); createDeployment.setDeploymentDirectory(new File(\"test.war\")); createDeployment.setDeploymentName(\"test.war\"); createDeployment.setEarName(\"test.ear\"); createDeployment.setEarVersion(\"1.0\"); createDeployment.setJarName(\"test.jar\"); createDeployment.\n",
      "\n",
      "### Output: ### Error: ### Exception: ### Stack Trace: ### Log: ### Log Status: ### Log Message: ### Log Object: ### Log No Header: ### Log Stack Trace: ### Log No Header: ### Log No Header: ### Log No Header: ### Log No Header: ### Log No Header: ### Log No Header: \n",
      "BigInteger nresults = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "String javaCode = \"import java.io.IOException; import java.util.HashMap; import java.util.Map; import java.util.Map.Entry; import java.util.Set; import java.util.TreeSet; import java.util.Vector; import java.util.regex.Pattern; import java.util.regex.Matcher; import java.util.regex.PatternSyntaxException; import java.util.regex.PatternSyntaxException; import java.util.regex.Pattern; import java.util.regex.Matcher; import java.util.regex.Pattern; import\n",
      "if (locator.widget(ui, placeHolder) == null) return \"widget not showing\"; ### Output: if (locator.widget(ui, placeHolder) == null) return \"widget not showing\"; ### Output: if (locator.widget(ui, placeHolder) == null) return \"widget not showing\"; ### Output: if (locator.widget(ui, placeHolder) == null) return \"widget not showing\"; ### Output: if (loc\n",
      "VectorXZ add(VectorXZ a, VectorXZ b) { return a.add(b); } VectorXZ xyzFunction(VectorXZ a, VectorXZ b) { return a.xyzFunction(b); } VectorXZ invert(VectorXZ a) { return a.invert(); } VectorXZ subtract(VectorXZ a, VectorXZ b) { return a.subtract(b); } double dot(VectorXZ a, VectorXZ b\n",
      "void readRGBs555to24(byte[] byteBuf) { int rgb555 = 0; for (int i = 0; i < byteBuf.length; i++) { rgb555 = rgb555 << 1; rgb555 = rgb555 | byteBuf[i]; } rgb555 = rgb555 << 1; rgb555 = rgb555 | 0\n",
      "\n",
      "\n",
      "double derivative = 0; if (isStrictlyIncreasing) { for (int i = 0; i < n; i++) { derivative += polynomials[i].getCoefficient(i) * value; } } else { for (int i = 0; i < n; i++) { derivative += polynomials[i].getCoefficient(i) * value; } } return derivative; ### Output: \n",
      "public class Closure { public static Closure getInstance(long serialVersionUID) { throw new UnsupportedOperationException(); } } \n",
      "``` public class Atom { public int getLeftType() { return type; } public int getRightType() { return type; } public int getType() { return type; } public int getTypeLimits() { \n",
      "getEmpiricalDistribution(DIGEST_MODE, filePointer, CONSTANT_MODE, mode, mu, empiricalDistribution, valuesFileURL, sigma, randomData, REPLAY_MODE, EXPONENTIAL_MODE, GAUSSIAN_MODE, UNIFORM_MODE); getEmpiricalDistribution(DIGEST_MODE, filePointer, CONSTANT_MODE, mode, mu, empiricalDistribution, valuesFileURL, sigma, randomData, REPLAY_MODE, EXPONENTIAL_MODE, G\n",
      "doublelistiterator doublelistiterator(ListIterator _iterator) { if (_iterator == null) { return null; } else { return new doublelistiterator(_iterator); } } doublelistiterator doublelistiterator(doublelistiterator _iterator) { if (_iterator == null) { \n",
      "importPingErCountryRegionFile(countryLookup, pingErCountry, pingErRegion, serialVersionUID, log, pingErCountryRegions, pingErCountryRegionFilename) importPingErCountryRegionFile(countryLookup, pingErCountry, pingErRegion, serialVersionUID, log, pingErCountryRegions, pingErCountryRegionFilename) importPingErCountryRegionFile(countryLookup, pingErCountry, pingErRegion, serialVersionUID, log, pingErCountryRegions, pingErCountryRegionFilename) importPingEr\n",
      "String[] parseArgumentList(String[] args) { String[] parsedArgs = new String[args.length]; for (int i = 0; i < args.length; i++) { parsedArgs[i] = args[i].replace(\"\\\\\", \"\\\\\\\\\"); } return parsedArgs; } String unescapeBrackets(String s) { StringBuffer sb = new StringBuffer(); for (int i = 0; i < s.length(); i++) \n",
      "\n",
      "String getKdcPort String getKdcRealm int checkGetKdcTcpPort int getKdcUdpPort int checkGetKdcUdpPort boolean allowTcp boolean allowUdp KdcConfig getKdcConfig String getKdcHost BackendConfig getBackendConfig ### Explanation: ``` public class KdcClient { public static void main(String[] args) { KdcConfig kdcConfig = new KdcConfig(); BackendConfig\n",
      "\n",
      "### Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ### Expected Output: ##\n",
      "java.math.BigInteger val = new java.math.BigInteger(val); ModInteger ring = new ModInteger(ring); ModInteger MIREM = new ModInteger(MIREM); java.math.BigInteger getSymmetricVal = new java.math.BigInteger(getSymmetricVal); int MISIGN = new int(MISIGN); ModInteger gcd = new ModInteger(gcd); ModInteger sum = new ModInteger(sum); ModInteger MINEG = new ModInteger(MINEG); ModInteger MISUM\n",
      "PartyIdentification32 createParty RemittanceInformation5 createRmtInf_struct XMLGregorianCalendar createXMLGregorianCalendarDate RemittanceInformation5 createRmtInf BranchAndFinancialInstitutionIdentification4 createFinInstnId XMLGregorianCalendar createXMLGregorianCalendar AmountType3Choice createAmount ### Instruction: Generate java code unambiguous identification of a account ### Input: Pattern bicRegex PartyIdent\n",
      "### Instruction: Generate java code creates an unmarshaller from the sardineutil #jaxb_context. note : the unmarshaller is not thread safe, so it must be created for every request. ### Input: JAXBContext JAXB_CONTEXT String CUSTOM_NAMESPACE_URI String DEFAULT_NAMESPACE_URI String DEFAULT_NAMESPACE_PREFIX List<ThreadLocal<SimpleDateFormat>> DATETIME_FORMATS String CUSTOM_NAME\n",
      "byte[] encode = new byte[encodingTable.length]; byte[] decode = new byte[decodingTable.length]; for (int i = 0; i < encodingTable.length; i++) { encode[i] = (byte) (encodingTable[i] & 0xFF); } for (int i = 0; i < decodingTable.length; i++) { decode[i] = (byte) (decodingTable[i] & 0xFF); } int encodedSize = computeEncodedSize(\n",
      "String getMessage(int param) { String message = getMessage(); message = message.replace(\"1\", Integer.toString(param)); return message; } String getMessage() { String message = getLanguageMessages().get(\"1\"); if (message == null) { message = getDefaultLanguage().get(\"1\"); } return message; } String getMessage() { \n",
      "\n",
      "``` if (_constructor.isAccessible()) { if (_constructor.getDeclaringClass().isAssignableFrom(_args[0].getClass())) { if (_constructor.getDeclaringClass().isAssignableFrom(_args[1].getClass())) { if (_constructor.getDeclaringClass().isAssignableFrom(_args[2].getClass())) { if (_constructor.getDeclaringClass().isAssignableFrom(_args[3].getClass())) { if (_constructor.getDeclaringClass\n",
      "PlaceHolder placeHolder long asLong int toInt String toBinaryString String toBinaryString String toBinaryString byte[] fromInt byte[] fromLong ### Output: PlaceHolder placeHolder long asLong int toInt String toBinaryString String toBinaryString String toBinaryString byte[] fromInt byte[] fromLong ### Output: PlaceHolder placeHolder \n",
      "/root/alpaca-lora/t2c_concode/results/2023-04-30-14_56_57.txt\n",
      "CompletedProcess(args=['python', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py', '-a', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json', '-p', '/root/alpaca-lora/t2c_concode/results/2023-04-30-14_56_57.txt'], returncode=0, stderr=b'INFO:__main__:BLEU: 0.29, EM: 0.0\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "# def generate_prompt(instruction, input):\n",
    "#     return f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "\n",
    "def generate_test_prompt(data_point, train = False):\n",
    "    # To decrease expectations of results :)\n",
    "    assert train == False\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"] if train else ''}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"] if train else ''}\"\"\"\n",
    "\n",
    "\n",
    "lst = json.load(open('t2c_concode/t2c_answers.json', 'rb'))\n",
    "inputs = lst# [lst[0]]\n",
    "# instruction = 'Combine the question and answer into an image caption as succinctly as possible. Be sure to include the phrase \"a photo of\". Do not draw false conclusions.'\n",
    "# inputs = ['Is this a baseball game? yes', 'Is this a baseball game? no']\n",
    "prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "print(prompts[0])\n",
    "prompts = np.array(prompts)\n",
    "batch_size = 9\n",
    "res_list = []\n",
    "for ind in tqdm.tqdm(range(math.ceil(len(prompts)/batch_size))):\n",
    "    current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "    print(ind*batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "    \n",
    "    tokenized_inputs = tokenizer(list(current_prompts), \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 return_tensors=\"pt\"\n",
    "                                ).to('cuda')\n",
    "\n",
    "    generation_config = GenerationConfig(max_new_tokens=128)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        full_output = model.generate(\n",
    "            **tokenized_inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "    res_list.extend(tokenizer.batch_decode(full_output, skip_special_tokens=False))\n",
    "#     print(tokenizer.batch_decode(full_output, skip_special_tokens=False)[-1])\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.split('### Response:\\n')[-1]\n",
    "    s = s.replace('\\n', '  ')\n",
    "    s = s.replace('<unk>', \" \")\n",
    "    s = ' '.join(s.split(' ')[:100])\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "    \n",
    "    if len(s) > 0 and s[0] == ' ':\n",
    "        s = s[1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "for s in tqdm.tqdm(res_list):\n",
    "    predict_list.append(preprocess(s))\n",
    "    \n",
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)\n",
    "\n",
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')\n",
    "\n",
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00828eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['python', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py', '-a', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json', '-p', '/root/alpaca-lora/t2c_concode/results/2023-04-30-14_56_57.txt'], returncode=0, stderr=b'INFO:__main__:BLEU: 0.29, EM: 0.0\\n')\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f877f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay          45G   25G   21G  56% /\r\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "552aa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "res = open(\"/root/alpaca-lora/t2c_concode/results/2023-04-30-11_27_46.txt\", \"r\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bbb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d32ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08dd0ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['python', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py', '-a', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json', '-p', '/root/alpaca-lora/t2c_concode/results/2023-04-30-13_38_56.txt'], returncode=0, stderr=b'INFO:__main__:BLEU: 19.67, EM: 10.0\\n')\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.split('### Response:\\n')[-1]\n",
    "    s = s.replace('\\n', '  ')\n",
    "    s = s.replace('<unk>', \" \")\n",
    "    s = ' '.join(s.split(' ')[:100])\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "    \n",
    "    if len(s) > 0 and s[0] == ' ':\n",
    "        s = s[1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "for s in tqdm.tqdm(res_list):\n",
    "    predict_list.append(preprocess(s))\n",
    "    \n",
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)\n",
    "\n",
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')\n",
    "\n",
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b42aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py', '-a', '/root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json', '-p', '/root/alpaca-lora/t2c_concode/results/2023-04-30-13_38_56.txt'], returncode=0, stderr=b'INFO:__main__:BLEU: 19.67, EM: 10.0\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4f0a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/alpaca-lora/t2c_concode/results/2023-04-29-18_08_01.txt\n",
      "INFO:__main__:BLEU: 19.67, EM: 10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)\n",
    "\n",
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')\n",
    "\n",
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edea50",
   "metadata": {},
   "source": [
    "# len(res_list)\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbdf65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = s.split('### Response:\\n')[-1]\n",
    "    s = s.replace('\\n', '  ')\n",
    "    s = s.replace('<unk>', \" \")\n",
    "    s = ' '.join(s.split(' ')[:100])\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "    \n",
    "    if len(s) > 0 and s[0] == ' ':\n",
    "        s = s[1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "for s in tqdm.tqdm(res_list):\n",
    "    predict_list.append(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([i.split(\"### Response:\\n\")[-1]=='' for i in prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst[0])\n",
    "print(prompts[0])\n",
    "print(predict_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a20e356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Integer function ( ) { return null == intervalQualifier ? null : intervalQualifier . getFractionalSecondPrecisionPreservingDefault ( ) ; }'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[8]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89c4ebf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Integer function ( ) { return precision ; } '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f5f497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'byte [ ] function ( Drawable arg0 ) { return bitmapToByte ( drawableToBitmap ( arg0 ) ) ; } '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978dc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d86571",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_pred_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_pred_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(command\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m      4\u001b[0m                         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[1;32m      5\u001b[0m result\u001b[38;5;241m.\u001b[39mstderr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_pred_name' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089b8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 53.9, EM: 21.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a80d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 0.29, EM: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "456530b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 0.27, EM: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f8035e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b468338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 23.70 GiB total capacity; 13.80 GiB already allocated; 2.91 GiB free; 19.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig(max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m     full_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(full_output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:731\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 731\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1406\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1401\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1402\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1403\u001b[0m         )\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2201\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2201\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2209\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:770\u001b[0m, in \u001b[0;36mLLaMAForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    767\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    782\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:619\u001b[0m, in \u001b[0;36mLLaMAModel.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    613\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    614\u001b[0m         hidden_states,\n\u001b[1;32m    615\u001b[0m         attention_mask,\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:316\u001b[0m, in \u001b[0;36mLLaMADecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    313\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:253\u001b[0m, in \u001b[0;36mLLaMAAttention.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    250\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(attn_weights, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin))\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    254\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1845\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 23.70 GiB total capacity; 13.80 GiB already allocated; 2.91 GiB free; 19.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompts = [generate_prompt(instruction=inp['instruction'], \n",
    "#                            input=inp['input']) \n",
    "#            for inp in inputs\n",
    "#           ]\n",
    "tokenized_inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_output = model.generate(\n",
    "        **tokenized_inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "print(tokenizer.batch_decode(full_output, skip_special_tokens=False)[-1])\n",
    "\n",
    "#     # same tokenized input, but we index just into the parts for the second input\n",
    "#     single_output = model.generate(\n",
    "#         **{k: v[1:] for k, v in tokenized_inputs.items()},\n",
    "#         generation_config=generation_config\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137008ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results  t2c_answers.json  t2c_dev.json  t2c_test.json\tt2c_train.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls t2c_concode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5448783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t2c_answers.json</th>\n",
       "      <th>t2c_train.json</th>\n",
       "      <th>t2c_test.json</th>\n",
       "      <th>t2c_dev.json</th>\n",
       "      <th>t2c_1000train.json</th>\n",
       "      <th>t2c_10000train.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t2c_answers.json</th>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_train.json</th>\n",
       "      <td>10</td>\n",
       "      <td>71262</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>981</td>\n",
       "      <td>9138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_test.json</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_dev.json</th>\n",
       "      <td>99</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>1935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_1000train.json</th>\n",
       "      <td>0</td>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>981</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_10000train.json</th>\n",
       "      <td>0</td>\n",
       "      <td>9138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "      <td>9138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     t2c_answers.json  t2c_train.json  t2c_test.json   \n",
       "t2c_answers.json                   99              10              0  \\\n",
       "t2c_train.json                     10           71262              0   \n",
       "t2c_test.json                       0               0              1   \n",
       "t2c_dev.json                       99             114              0   \n",
       "t2c_1000train.json                  0             981              0   \n",
       "t2c_10000train.json                 0            9138              0   \n",
       "\n",
       "                     t2c_dev.json  t2c_1000train.json  t2c_10000train.json  \n",
       "t2c_answers.json               99                   0                    0  \n",
       "t2c_train.json                114                 981                 9138  \n",
       "t2c_test.json                   0                   0                    0  \n",
       "t2c_dev.json                 1935                   0                    0  \n",
       "t2c_1000train.json              0                 981                  225  \n",
       "t2c_10000train.json             0                 225                 9138  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dct = {}\n",
    "for filename in os.listdir(\"t2c_concode\"):\n",
    "    if filename.endswith('.json'):\n",
    "        dct[filename] = json.load(open(f\"t2c_concode/{filename}\"))\n",
    "    \n",
    "df = {}\n",
    "keys = list(dct.keys())\n",
    "for key1 in keys:\n",
    "    df[key1] = {}\n",
    "    for key2 in keys:\n",
    "        lst1 = [i['output'] for i in dct[key2]]\n",
    "        lst2 = [i['output'] for i in dct[key1]]\n",
    "\n",
    "        df[key1][key2] = len(set(lst1)&set(lst2))\n",
    "\n",
    "pd.DataFrame(df)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb96d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dct.keys()\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6da2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "lst = set([i['output'] for i in dct['t2c_dev.json']])\n",
    "\n",
    "lst_new = [i for i in dct['t2c_train.json'] if i['output'] not in lst]\n",
    "lst_new = shuffle(lst_new)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b970b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(lst_new, open(\"t2c_concode/t2c_10000train.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87c9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(lst_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338296f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96168ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
