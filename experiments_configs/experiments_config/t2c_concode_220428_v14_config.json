{"experiment_name": "t2c_concode_220428_v14", "fn_train_dataset": "/root/data/t2c_train.json", "default_model": "decapoda-research/llama-7b-hf", "MICRO_BATCH_SIZE": 2, "BATCH_SIZE": 10, "EPOCHS": 10, "LEARNING_RATE": 0.0002, "CUTOFF_LEN": 256, "LORA_R": 4, "LORA_ALPHA": 16, "LORA_DROPOUT": 0.05, "warmup_steps": 200, "fp16": true, "logging_steps": 10, "save_total_limit": 1, "save_strategy": "steps", "save_steps": 100, "seed": 42, "logging_strategy": "steps", "report_to": "tensorboard", "mlm": false, "truncation": true, "padding": "max_length", "config_use_cache": false, "resume_from_checkpoint": false, "bleu_batch_size": 5, "GRADIENT_ACCUMULATION_STEPS": 5}