{"experiment_name": "t2c_concode_220428_v33", "fn_train_dataset": "/root/data/t2c_train.json", "fn_eval_dataset": "/root/data/t2c_answers.json", "default_model": "yahma/llama-7b-hf", "MICRO_BATCH_SIZE": 2, "BATCH_SIZE": 10, "EPOCHS": 2, "LEARNING_RATE": 0.0002, "CUTOFF_LEN": 256, "LORA_R": 16, "LORA_ALPHA": 16, "LORA_DROPOUT": 0.05, "warmup_steps": 200, "fp16": true, "logging_steps": 10, "eval_steps": 100, "evaluation_strategy": "steps", "save_total_limit": 1, "save_strategy": "steps", "save_steps": 500, "seed": 42, "logging_strategy": "steps", "report_to": "tensorboard", "mlm": false, "truncation": true, "padding": "max_length", "config_use_cache": false, "resume_from_checkpoint": false, "bleu_batch_size": 5, "GRADIENT_ACCUMULATION_STEPS": 5, "log_bleu_steps_factor": 50, "load_best_model_at_end": false}