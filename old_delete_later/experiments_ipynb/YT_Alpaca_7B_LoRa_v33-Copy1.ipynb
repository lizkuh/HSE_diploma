{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ac0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All to folder\n",
    "## generate prompt\n",
    "# !ls data\n",
    "# import time\n",
    "# time.sleep(60*30)\n",
    "\n",
    "# Try to do:\n",
    "# torch.cuda.empty_cache()\n",
    "# yahma/llama-7b-hf\n",
    "# decapoda-research/llama-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c4ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ipynb/prompter/templates/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM as LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer as LLaMATokenizer\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from EvaluateTestSet import EvaluateTestSet\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "def init_lora_model_and_tokenizer(default_model,\n",
    "                             LORA_R,\n",
    "                             LORA_ALPHA,\n",
    "                             LORA_DROPOUT\n",
    "                            ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    model = LLaMAForCausalLM.from_pretrained(\n",
    "    default_model,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = LLaMATokenizer.from_pretrained(\n",
    "        default_model, add_eos_token=True\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "class MyCustomCallback(TensorBoardCallback):\n",
    "    #log_bleu_steps_factor = 5\n",
    "    bleu_generation_max_new_tokens = 30\n",
    "    bleu_fn_test_data = \"temp/t2c_answers.json\"\n",
    "    bleu_fn_etalon = \"temp/answers.json\"\n",
    "    log_step = 0\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        super().on_log(args, state, control, logs=logs, **kwargs)\n",
    "        #print(\"kwargs\", len(kwargs), kwargs.keys())\n",
    "        if self.tb_writer is not None:\n",
    "            #print(state)\n",
    "            #print(state.global_step)\n",
    "            #print(self.log_step)\n",
    "            if (self.log_step % self.log_bleu_steps_factor ==0):\n",
    "                model = kwargs['model']\n",
    "                tokenizer = kwargs['tokenizer']\n",
    "                \n",
    "                model.eval()\n",
    "                assert not model.training\n",
    "                generation_config = GenerationConfig(max_new_tokens = self.bleu_generation_max_new_tokens,\n",
    "                                                     # min_new_tokens = 5,\n",
    "                                                     temperature = 1.0\n",
    "                                                    )\n",
    "                print(\"generation_config:\", generation_config)\n",
    "                evaluator = EvaluateTestSet(generation_config = generation_config,\n",
    "                                            fn_test_data = self.bleu_fn_test_data,\n",
    "                                            fn_etalon = self.bleu_fn_etalon,\n",
    "                                            batch_size = 1\n",
    "                                       )\n",
    "\n",
    "                metric_res = evaluator.evaluate(model=model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                               )\n",
    "                model.train()\n",
    "                assert model.training\n",
    "                print(metric_res)\n",
    "                for key, val in metric_res.items():\n",
    "                    #add \"custom/something\"\n",
    "                    self.tb_writer.add_scalar(key, val, state.global_step)\n",
    "                self.tb_writer.flush()\n",
    "            self.log_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6cd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Prompter import prompter\n",
    "# from prompter import Prompter\n",
    "# Prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c982ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# A dedicated helper to manage templates and prompt building.\n",
    "# \"\"\"\n",
    "\n",
    "# import json\n",
    "# import os.path as osp\n",
    "# from typing import Union\n",
    "\n",
    "\n",
    "# class Prompter(object):\n",
    "#     __slots__ = (\"template\", \"_verbose\")\n",
    "#     __file__ = 'prompter/fds'\n",
    "#     path = '/'.join(__file__.split('/')[:-1]) + '/' + 'templates/'\n",
    "#     def __init__(self, \n",
    "#                  template_name: str = \"\", \n",
    "#                  verbose: bool = False, \n",
    "#                  bos_token = None,\n",
    "#                  eos_token = None\n",
    "#                 ):\n",
    "#         print(self.path)\n",
    "#         self._verbose = verbose\n",
    "#         self._test = \"test\"\n",
    "#         print(bos_token)\n",
    "        \n",
    "#         self.bos_token = bos_token\n",
    "#         self.bos_token = bos_token\n",
    "#         self.eos_token = eos_token\n",
    "        \n",
    "#         if not template_name:\n",
    "#             # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "#             template_name = \"alpaca\"\n",
    "#         file_name = osp.join(self.path, f\"{template_name}.json\")\n",
    "#         if not osp.exists(file_name):\n",
    "#             raise ValueError(f\"Can't read {file_name}\")\n",
    "#         with open(file_name) as fp:\n",
    "#             self.template = json.load(fp)\n",
    "#         if self._verbose:\n",
    "#             print(\n",
    "#                 f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "#             )\n",
    "\n",
    "#     def generate_prompt(\n",
    "#         self,\n",
    "#         instruction: str,\n",
    "#         input: Union[None, str] = None,\n",
    "#         label: Union[None, str] = None,\n",
    "#     ) -> str:\n",
    "#         # returns the full prompt from instruction and optional input\n",
    "#         # if a label (=response, =output) is provided, it's also appended.\n",
    "#         if input:\n",
    "#             res = self.template[\"prompt_input\"].format(\n",
    "#                 instruction=instruction, input=input\n",
    "#             )\n",
    "#         else:\n",
    "#             res = self.template[\"prompt_no_input\"].format(\n",
    "#                 instruction=instruction\n",
    "#             )\n",
    "#         if label:\n",
    "#             res = f\"{res}{label}\"\n",
    "#         if self._verbose:\n",
    "#             print(res)\n",
    "#         if self.bos_token is not None:\n",
    "#             res = self.bos_token + res\n",
    "#         if self.eos_token is not None:\n",
    "#             res = res + self.eos_token\n",
    "        \n",
    "#         return res\n",
    "\n",
    "#     def get_response(self, output: str) -> str:\n",
    "#         return output.split(self.template[\"response_split\"])[1].strip()\n",
    "    \n",
    "# Prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79baedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat prompter/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81b76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list\n",
    "# !pip install bitsandbytes==0.37.2\n",
    "# experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a58e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"/root/experiments_configs/\"\n",
    "EXPERIMENTS_PATH = \"/root/experiments/\"\n",
    "experiment_name = \"t2c_concode_220428_v33\"\n",
    "# t2c_concode_220428_v18.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51baf7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2c_concode_220428_v14_config.json  t2c_concode_220428_v22_config.json\r\n",
      "t2c_concode_220428_v15_config.json  t2c_concode_220428_v30_config.json\r\n",
      "t2c_concode_220428_v16_config.json  t2c_concode_220428_v31.json\r\n",
      "t2c_concode_220428_v18_config.json  t2c_concode_220428_v31_config.json\r\n",
      "t2c_concode_220428_v19_config.json  t2c_concode_220428_v32_config.json\r\n",
      "t2c_concode_220428_v20_config.json  t2c_concode_220428_v33_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/experiments_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc88f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_config_path = os.path.join(CONFIG_PATH, experiment_name + \"_config.json\")\n",
    "experiment_config = json.load(open(current_config_path, \"r\"))\n",
    "\n",
    "assert experiment_config['experiment_name'] == experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eff1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config['resume_from_checkpoint'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50bb0a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m experiment_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume_from_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert experiment_config['resume_from_checkpoint'] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dc63c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert experiment_config['experiment_name'] == experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f52cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment_path = os.path.join(EXPERIMENTS_PATH, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf204b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/experiments/t2c_concode_220428_v33’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d7a3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(experiment_config, open(current_experiment_path + \\\n",
    "                                  \"/experiment_config.json\", \n",
    "                                  \"w+\"\n",
    "                                 )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "946d5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(MyCustomCallback, \"log_bleu_steps_factor\", experiment_config['log_bleu_steps_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e33b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyCustomCallback.log_bleu_steps_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2fbc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config[ \"fn_train_dataset\"] =  \"/root/data/t2c_1000train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e5bab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb6645df13b4c278bac5a104647ff0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-49f8fd5e1df47661/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fecb1b99ad34786afa58bcfc1b53d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = init_lora_model_and_tokenizer(default_model = experiment_config[\"default_model\"],\n",
    "                                                 LORA_R = experiment_config[\"LORA_R\"],\n",
    "                                                 LORA_ALPHA = experiment_config[\"LORA_ALPHA\"],\n",
    "                                                 LORA_DROPOUT = experiment_config[\"LORA_DROPOUT\"]\n",
    "                                                )\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", \n",
    "                    data_files = {\"train\": experiment_config[\"fn_train_dataset\"],\n",
    "                                  \"eval\":  experiment_config[\"fn_eval_dataset\"]\n",
    "                                 }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da190bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.id_to_piece(1)  ## '<s>', which is the bos token for LLaMa\n",
    "# sp_model.id_to_piece(2)  ## '</s>', which is the eos token for LLaMa\n",
    "# sp_model.id_to_piece(-1)  ## Throws: IndexError: piece id is out of range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb912c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id = model.config.pad_token_id\n",
    "# tokenizer.eos_token_id = model.config.eos_token_id\n",
    "# model.config.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8991685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.pad_token_id = model.conf\n",
    "# tokenizer.pad_token_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5362bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # padding_side = 'left'\n",
    "# from transformers import LlamaTokenizer as LLaMATokenizer\n",
    "\n",
    "# tokenizer = LLaMATokenizer.from_pretrained(experiment_config[\"default_model\"])\n",
    "\n",
    "# # model.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "# # tokenizer.eos_token_id = 1\n",
    "# # tokenizer.bos_token_id = 2\n",
    "# # 0  # unk\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# # # not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "# # model.config.bos_token_id = 1\n",
    "# # model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3db7caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa19c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.eos_token_id = 7\n",
    "# tokenizer.bos_token_id = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f84325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.pad_token_id = tokenizer.pad_token_id = 0\n",
    "# tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d450fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"sdlfkjjsdf\"+\"<0x04>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dc1a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tokenizers.processors import TemplateProcessing\n",
    "# # tokenizer.post_processor = TemplateProcessing(\n",
    "# #     single=f\"[BOS] $A {tokenizer.eos_token}\",\n",
    "# #     special_tokens=[(\"[BOS]\", 7), (tokenizer.eos_token, 8)],\n",
    "# # )\n",
    "# model = LLaMAForCausalLM.from_pretrained(\n",
    "#     experiment_config[\"default_model\"],\n",
    "#     load_in_8bit=True,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22c4214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use_fast=False\n",
    "# # model.config.eos_token_id, model.config.pad_token_id, \n",
    "# # model.config.unk_token_id\n",
    "# # model.config.eos_token_id,\\\n",
    "# # model.config.unk_token_id\n",
    "\n",
    "# model.parameters#.config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8764125",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "piece id is out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1217\u001b[0m, in \u001b[0;36mSpecialTokensMixin.pad_token_id\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;129m@pad_token_id\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_token_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m-> 1217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:903\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_decoder[ids]\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:129\u001b[0m, in \u001b[0;36mLlamaTokenizer._convert_id_to_token\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:1045\u001b[0m, in \u001b[0;36m_batchnize.<locals>._batched_func\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1043\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [_func(\u001b[38;5;28mself\u001b[39m, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1045\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:1038\u001b[0m, in \u001b[0;36m_batchnize.<locals>._func\u001b[0;34m(v, n)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func\u001b[39m(v, n):\n\u001b[1;32m   1037\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(n) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mpiece_size()):\n\u001b[0;32m-> 1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiece id is out of range.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1039\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(v, n)\n",
      "\u001b[0;31mIndexError\u001b[0m: piece id is out of range."
     ]
    }
   ],
   "source": [
    "# tokenizer.pad_token_id = model.config.pad_token_id = 0\n",
    "# tokenizer.eos_token_id = model.config.eos_token_id\n",
    "# tokenizer.bos_token_id = model.config.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f460f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'padding_side': 'left'} not recognized.\n",
      "Keyword arguments {'padding_side': 'left'} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   319, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184, 23184,\n",
       "         23184, 23184, 23184, 23184, 23184, 23184],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     2,   350, 14388, 14388, 14388, 14388,\n",
       "         14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388,\n",
       "         14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388,\n",
       "         14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388,\n",
       "         14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388, 14388,\n",
       "         14388, 14388, 14388, 14388, 14388, 29933]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer(['A'*10000, \"B\"*100], \n",
    "#           return_tensors='pt',         \n",
    "#           truncation=experiment_config[\"truncation\"],\n",
    "#           max_length=experiment_config[\"CUTOFF_LEN\"],\n",
    "#           padding=experiment_config[\"padding\"], \n",
    "#           padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac6bdbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id 0\n",
      "tokenizer.eos_token_id 0\n",
      "tokenizer.bos_token_id 0\n",
      "tokenizer.eos_token_id 0\n",
      "model.config.pad_token_id -1\n",
      "model.config.eos_token_id 1\n",
      "model.config.bos_token_id 0\n",
      "model.config.eos_token_id 1\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer.pad_token_id\", tokenizer.pad_token_id)\n",
    "print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "print(\"tokenizer.bos_token_id\", tokenizer.bos_token_id)\n",
    "print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "\n",
    "print(\"model.config.pad_token_id\", model.config.pad_token_id)\n",
    "print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "print(\"model.config.bos_token_id\", model.config.bos_token_id)\n",
    "print(\"model.config.eos_token_id\", model.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3810c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id 0\n",
      "tokenizer.eos_token_id 2\n",
      "tokenizer.bos_token_id 1\n",
      "tokenizer.sep_token_id None\n",
      "tokenizer.unk_token_id 0\n",
      "model.config.pad_token_id 0\n",
      "model.config.eos_token_id 2\n",
      "model.config.bos_token_id 1\n",
      "model.config.sep_token_id None\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer.pad_token_id\", tokenizer.pad_token_id)\n",
    "print(\"tokenizer.eos_token_id\", tokenizer.eos_token_id)\n",
    "print(\"tokenizer.bos_token_id\", tokenizer.bos_token_id)\n",
    "print(\"tokenizer.sep_token_id\", tokenizer.sep_token_id)\n",
    "print(\"tokenizer.unk_token_id\", tokenizer.unk_token_id)\n",
    "\n",
    "print(\"model.config.pad_token_id\", model.config.pad_token_id)\n",
    "print(\"model.config.eos_token_id\", model.config.eos_token_id)\n",
    "print(\"model.config.bos_token_id\", model.config.bos_token_id)\n",
    "print(\"model.config.sep_token_id\", model.config.sep_token_id)\n",
    "# print(\"model.config.unk_token_id\", model.config.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f378003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config['default_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "493ab75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55448c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_val = LLaMATokenizer.from_pretrained(experiment_config['default_model'])\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "#model.config.bos_token_id = 1\n",
    "#model.config.eos_token_id = 2\n",
    "\n",
    "# tokenizer_val.padding_side = \"left\"\n",
    "# not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "#raise ValueError(\"Change this\")\n",
    "# model.config.bos_token_id = 1\n",
    "# model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e83f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_val = LLaMATokenizer.from_pretrained(\n",
    "    experiment_config['default_model'], add_eos_token=True\n",
    ")\n",
    "\n",
    "tokenizer_val.padding_side = \"left\"\n",
    "# tokenizer_val.pad_token_id = 0  # unk. we want this to be different from the eos token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284973fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0a65da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_val.pad_token_id None\n",
      "tokenizer_val.eos_token_id 2\n",
      "tokenizer_val.bos_token_id 1\n",
      "tokenizer_val.sep_token_id None\n",
      "tokenizer_val.unk_token_id 0\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer_val.pad_token_id\", tokenizer_val.pad_token_id)\n",
    "print(\"tokenizer_val.eos_token_id\", tokenizer_val.eos_token_id)\n",
    "print(\"tokenizer_val.bos_token_id\", tokenizer_val.bos_token_id)\n",
    "print(\"tokenizer_val.sep_token_id\", tokenizer_val.sep_token_id)\n",
    "print(\"tokenizer_val.unk_token_id\", tokenizer_val.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a58aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config[\"logging_steps\"] = 1\n",
    "# experiment_config[\"eval_steps\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "415d4e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 't2c_concode_220428_v32',\n",
       " 'fn_train_dataset': '/root/data/t2c_train.json',\n",
       " 'fn_eval_dataset': '/root/data/t2c_answers.json',\n",
       " 'default_model': 'decapoda-research/llama-7b-hf',\n",
       " 'MICRO_BATCH_SIZE': 2,\n",
       " 'BATCH_SIZE': 10,\n",
       " 'EPOCHS': 2,\n",
       " 'LEARNING_RATE': 0.0002,\n",
       " 'CUTOFF_LEN': 256,\n",
       " 'LORA_R': 1024,\n",
       " 'LORA_ALPHA': 1024,\n",
       " 'LORA_DROPOUT': 0.005,\n",
       " 'warmup_steps': 200,\n",
       " 'fp16': True,\n",
       " 'logging_steps': 10,\n",
       " 'eval_steps': 100,\n",
       " 'evaluation_strategy': 'steps',\n",
       " 'save_total_limit': 1,\n",
       " 'save_strategy': 'steps',\n",
       " 'save_steps': 100,\n",
       " 'seed': 42,\n",
       " 'logging_strategy': 'steps',\n",
       " 'report_to': 'tensorboard',\n",
       " 'mlm': False,\n",
       " 'truncation': True,\n",
       " 'padding': 'max_length',\n",
       " 'config_use_cache': False,\n",
       " 'resume_from_checkpoint': False,\n",
       " 'bleu_batch_size': 5,\n",
       " 'GRADIENT_ACCUMULATION_STEPS': 5,\n",
       " 'log_bleu_steps_factor': 50,\n",
       " 'load_best_model_at_end': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_config\n",
    "# {'experiment_name': 't2c_concode_220428_v19',\n",
    "#  'fn_train_dataset': '/root/data/t2c_train.json',\n",
    "#  'fn_eval_dataset': '/root/data/t2c_answers.json',\n",
    "#  'default_model': 'decapoda-research/llama-7b-hf',\n",
    "#  'MICRO_BATCH_SIZE': 2,\n",
    "#  'BATCH_SIZE': 10,\n",
    "#  'EPOCHS': 2,\n",
    "#  'LEARNING_RATE': 0.0002,\n",
    "#  'CUTOFF_LEN': 256,\n",
    "#  'LORA_R': 16,\n",
    "#  'LORA_ALPHA': 16,\n",
    "#  'LORA_DROPOUT': 0.05,\n",
    "#  'warmup_steps': 200,\n",
    "#  'fp16': True,\n",
    "#  'logging_steps': 10,\n",
    "#  'eval_steps': 100,\n",
    "#  'evaluation_strategy': 'steps',\n",
    "#  'save_total_limit': 1,\n",
    "#  'save_strategy': 'steps',\n",
    "#  'save_steps': 500,\n",
    "#  'seed': 42,\n",
    "#  'logging_strategy': 'steps',\n",
    "#  'report_to': 'tensorboard',\n",
    "#  'mlm': False,\n",
    "#  'truncation': True,\n",
    "#  'padding': 'max_length',\n",
    "#  'config_use_cache': False,\n",
    "#  'resume_from_checkpoint': False,\n",
    "#  'bleu_batch_size': 5,\n",
    "#  'GRADIENT_ACCUMULATION_STEPS': 5,\n",
    "#  'log_bleu_steps_factor': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6708fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config[\"resume_from_checkpoint\"]\n",
    "# prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1229c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ipynb/prompter/templates/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Prompter' object has no attribute 'bos_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prompter\n\u001b[0;32m----> 2\u001b[0m prompter \u001b[38;5;241m=\u001b[39m \u001b[43mPrompter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(data_point):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_point \u001b[38;5;129;01mand\u001b[39;00m data_point[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/ipynb/prompter/__init__.py:21\u001b[0m, in \u001b[0;36mPrompter.__init__\u001b[0;34m(self, template_name, verbose, bos_token, eos_token)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m=\u001b[39m verbose\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_token \u001b[38;5;241m=\u001b[39m bos_token\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token \u001b[38;5;241m=\u001b[39m eos_token\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template_name:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Enforce the default here, so the constructor can be called with '' and will not break.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Prompter' object has no attribute 'bos_token'"
     ]
    }
   ],
   "source": [
    "from prompter import Prompter\n",
    "prompter = Prompter()\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    if \"input\" in data_point and data_point[\"input\"]:\n",
    "        return prompter.generate_prompt(instruction = data_point[\"instruction\"],\n",
    "                                        input = data_point[\"input\"],\n",
    "                                        label = data_point[\"output\"]\n",
    "                                       )\n",
    "    else:\n",
    "        return prompter.generate_prompt(instruction = data_point[\"instruction\"],\n",
    "                                        #input = None,\n",
    "                                        label = data_point[\"output\"]\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f894b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b0ca33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n1\\n\\n### Response:\\n2'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prompt({\"instruction\": \"1\",\n",
    "                 \"output\": \"2\" \n",
    "                }\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc4af6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 256, 'max_length')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment_config[\"resume_from_checkpoint\"] =\n",
    "experiment_config[\"truncation\"],\\\n",
    "experiment_config[\"CUTOFF_LEN\"],\\\n",
    "experiment_config[\"padding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce5420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='657' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  657/20000 48:52 < 24:03:28, 0.22 it/s, Epoch 0.07/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.041800</td>\n",
       "      <td>1.150982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>1.142533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>1.147014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.953700</td>\n",
       "      <td>1.146060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>1.145460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.529800</td>\n",
       "      <td>1.878569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/30 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:53<00:00,  3.78s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 156115.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.0009689912275490802, 'brevity_penalty': 0.10351901120544502, 'ratio': 0.30599755201958384, 'translation_length': 250, 'reference_length': 817, 'precisions_0': 0.05976095617529881, 'precisions_1': 0.004524886877828055, 'precisions_2': 0.005025125628140704, 'precisions_3': 0.005649717514124294}\n",
      "generation_config: GenerationConfig {\n",
      "  \"max_new_tokens\": 30,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:51<00:00,  3.71s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 146825.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 0.17230523072562803, 'brevity_penalty': 0.6677909732374923, 'ratio': 0.7123623011015912, 'translation_length': 582, 'reference_length': 817, 'precisions_0': 0.5540308747855918, 'precisions_1': 0.33815551537070526, 'precisions_2': 0.19120458891013384, 'precisions_3': 0.12373225152129817}\n"
     ]
    }
   ],
   "source": [
    "# def generate_prompt(data_point):\n",
    "#     # sorry about the formatting disaster gotta move fast\n",
    "#     if data_point[\"input\"]:\n",
    "#         return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# {data_point[\"instruction\"]}\n",
    "# ### Input:\n",
    "# {data_point[\"input\"]}\n",
    "# ### Response:\n",
    "# {data_point[\"output\"]}\"\"\"\n",
    "#     else:\n",
    "#         return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# {data_point[\"instruction\"]}\n",
    "# ### Response:\n",
    "# {data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "data = data.shuffle().map(\n",
    "    lambda data_point: tokenizer(\n",
    "        generate_prompt(data_point),\n",
    "        truncation=experiment_config[\"truncation\"],\n",
    "        max_length=experiment_config[\"CUTOFF_LEN\"],\n",
    "        padding=experiment_config[\"padding\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_val,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data['eval'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=experiment_config[\"MICRO_BATCH_SIZE\"],\n",
    "        gradient_accumulation_steps=experiment_config[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
    "        warmup_steps=experiment_config[\"warmup_steps\"],\n",
    "        num_train_epochs=experiment_config[\"EPOCHS\"],\n",
    "        learning_rate=experiment_config[\"LEARNING_RATE\"],\n",
    "        fp16=experiment_config[\"fp16\"],\n",
    "        logging_steps=experiment_config[\"logging_steps\"],        \n",
    "        evaluation_strategy = experiment_config['evaluation_strategy'],\n",
    "        eval_steps=experiment_config[\"eval_steps\"],\n",
    "        output_dir=current_experiment_path,#\"lora-alpaca\",\n",
    "        save_total_limit=experiment_config[\"save_total_limit\"],\n",
    "        save_strategy = experiment_config[\"save_strategy\"],\n",
    "        \n",
    "        save_steps = experiment_config[\"save_steps\"],\n",
    "        seed=experiment_config[\"seed\"],\n",
    "        logging_dir=current_experiment_path,\n",
    "        logging_strategy=experiment_config[\"logging_strategy\"],\n",
    "        report_to=experiment_config[\"report_to\"],\n",
    "        load_best_model_at_end = experiment_config[\"load_best_model_at_end\"]\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                               mlm=experiment_config[\"mlm\"]\n",
    "                                                              ),\n",
    "    callbacks = [MyCustomCallback]\n",
    ")\n",
    "model.config.use_cache = experiment_config[\"config_use_cache\"]\n",
    "# print(len(trainer.optimizer.state['found_inf_per_device']))\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=experiment_config[\"resume_from_checkpoint\"])\n",
    "\n",
    "model.save_pretrained(current_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67d4ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97522ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/experiments/t2c_concode_220428_v32'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_experiment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d9aa6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684180819.057514    checkpoint-20000\r\n",
      "1684180819.0648775   events.out.tfevents.1684180819.8d048d63ed1a.18376.0\r\n",
      "adapter_config.json  events.out.tfevents.1684180819.8d048d63ed1a.18376.2\r\n",
      "adapter_model.bin    experiment_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efa33d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa25528",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "839d4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(current_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac663873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 't2c_concode_220428_v22',\n",
       " 'fn_train_dataset': '/root/data/t2c_train.json',\n",
       " 'fn_eval_dataset': '/root/data/t2c_answers.json',\n",
       " 'default_model': 'decapoda-research/llama-7b-hf',\n",
       " 'MICRO_BATCH_SIZE': 2,\n",
       " 'BATCH_SIZE': 10,\n",
       " 'EPOCHS': 2,\n",
       " 'LEARNING_RATE': 0.0002,\n",
       " 'CUTOFF_LEN': 256,\n",
       " 'LORA_R': 64,\n",
       " 'LORA_ALPHA': 64,\n",
       " 'LORA_DROPOUT': 0.05,\n",
       " 'warmup_steps': 200,\n",
       " 'fp16': True,\n",
       " 'logging_steps': 10,\n",
       " 'eval_steps': 100,\n",
       " 'evaluation_strategy': 'steps',\n",
       " 'save_total_limit': 1,\n",
       " 'save_strategy': 'steps',\n",
       " 'save_steps': 500,\n",
       " 'seed': 42,\n",
       " 'logging_strategy': 'steps',\n",
       " 'report_to': 'tensorboard',\n",
       " 'mlm': False,\n",
       " 'truncation': True,\n",
       " 'padding': 'max_length',\n",
       " 'config_use_cache': False,\n",
       " 'resume_from_checkpoint': False,\n",
       " 'bleu_batch_size': 5,\n",
       " 'GRADIENT_ACCUMULATION_STEPS': 5,\n",
       " 'log_bleu_steps_factor': 50}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "665c1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(current_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e72383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fe786fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2c_concode_220428_v12\tt2c_concode_220428_v16\tt2c_concode_220428_v22\r\n",
      "t2c_concode_220428_v13\tt2c_concode_220428_v18\tt2c_concode_220428_v30\r\n",
      "t2c_concode_220428_v14\tt2c_concode_220428_v19\tt2c_concode_220428_v9\r\n",
      "t2c_concode_220428_v15\tt2c_concode_220428_v20\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "220a459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14G\t/root/\r\n"
     ]
    }
   ],
   "source": [
    "# !du -hs /root/ | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97e5c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t/root/.cache/huggingface/hub\r\n"
     ]
    }
   ],
   "source": [
    "# !du -lahS /root/\n",
    "# !du -hs /root/.cache/huggingface/* | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8940a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68a3752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /root/experiments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a8f14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay          45G   17G   29G  37% /\r\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d411735",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /root/experiments/t2c_concode_220428_v20/checkpoint-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /root/experiments/t2c_concode_220428_v20/checkpoint-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /root/experiments/t2c_concode_220428_v19/checkpoint-20000/\n",
    "# !df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97279e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf {current_experiment_path}/checkpoint-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a22c54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay          45G   45G  3.6M 100% /\r\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "731023f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683303090.6023178   adapter_model.bin\r\n",
      "1683303090.6101534   events.out.tfevents.1683303090.8d048d63ed1a.5752.0\r\n",
      "1683303146.4560587   events.out.tfevents.1683303090.8d048d63ed1a.5752.2\r\n",
      "1683303146.4694548   events.out.tfevents.1683303146.8d048d63ed1a.5752.4\r\n",
      "1683303388.633225    events.out.tfevents.1683303146.8d048d63ed1a.5752.6\r\n",
      "1683303388.645383    events.out.tfevents.1683303388.8d048d63ed1a.5752.10\r\n",
      "1683303483.1163416   events.out.tfevents.1683303388.8d048d63ed1a.5752.8\r\n",
      "1683303483.1291792   events.out.tfevents.1683303483.8d048d63ed1a.5752.12\r\n",
      "1683303652.147785    events.out.tfevents.1683303483.8d048d63ed1a.5752.14\r\n",
      "1683303652.1612782   events.out.tfevents.1683303652.8d048d63ed1a.5752.16\r\n",
      "1683303676.8182936   events.out.tfevents.1683303652.8d048d63ed1a.5752.18\r\n",
      "1683303676.8320808   events.out.tfevents.1683303676.8d048d63ed1a.5752.20\r\n",
      "1683303700.7354565   events.out.tfevents.1683303676.8d048d63ed1a.5752.22\r\n",
      "1683303700.7492273   events.out.tfevents.1683303700.8d048d63ed1a.5752.24\r\n",
      "1683303738.1971772   events.out.tfevents.1683303700.8d048d63ed1a.5752.26\r\n",
      "1683303738.208674    events.out.tfevents.1683303738.8d048d63ed1a.5752.28\r\n",
      "1683304244.2201474   events.out.tfevents.1683303738.8d048d63ed1a.5752.30\r\n",
      "1683304244.2324886   events.out.tfevents.1683304244.8d048d63ed1a.6142.0\r\n",
      "1683304469.9929264   events.out.tfevents.1683304244.8d048d63ed1a.6142.2\r\n",
      "1683304470.0023499   events.out.tfevents.1683304469.8d048d63ed1a.6142.4\r\n",
      "1683305248.9343088   events.out.tfevents.1683304469.8d048d63ed1a.6142.6\r\n",
      "1683305248.9410791   events.out.tfevents.1683305248.8d048d63ed1a.6275.0\r\n",
      "1683305916.0134802   events.out.tfevents.1683305248.8d048d63ed1a.6275.2\r\n",
      "1683305916.023698    events.out.tfevents.1683305916.8d048d63ed1a.6427.0\r\n",
      "1683306963.8205214   events.out.tfevents.1683305916.8d048d63ed1a.6427.2\r\n",
      "1683306963.8250442   events.out.tfevents.1683306963.8d048d63ed1a.6747.0\r\n",
      "adapter_config.json  events.out.tfevents.1683306963.8d048d63ed1a.6747.2\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/experiments/t2c_concode_220428_v13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dccce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay          45G   24G   22G  54% /\r\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07e61405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay          45G   24G   22G  54% /\r\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa1098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
