{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001efbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc67ea1873884fd1a945dc322e8cbe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "    ### Instruction:\n",
      "    Generate java code\n",
      "actually walks the bag to make sure the count is correct and resets the running total \n",
      "\n",
      "    ### Input:\n",
      "     Object _current \n",
      " int _total \n",
      " DefaultMapBag _parent \n",
      " Map _map \n",
      " int _mods \n",
      " Iterator _support \n",
      "\n",
      " boolean add \n",
      " boolean add \n",
      " Object next \n",
      " boolean containsAll \n",
      " boolean containsAll \n",
      " void clear \n",
      " boolean isEmpty \n",
      " boolean hasNext \n",
      " void remove \n",
      " boolean remove \n",
      " boolean remove \n",
      " Map getMap \n",
      " int modCount \n",
      " boolean contains \n",
      " Iterator iterator \n",
      " boolean removeAll \n",
      " int size \n",
      " boolean addAll \n",
      " int hashCode \n",
      " boolean equals \n",
      " Object[] toArray \n",
      " Object[] toArray \n",
      " Set uniqueSet \n",
      " void setMap \n",
      " String toString \n",
      " int getCount \n",
      " List extractList \n",
      " boolean retainAll \n",
      " boolean retainAll\n",
      "\n",
      "    ### Response:\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▋                                        | 1/12 [00:17<03:15, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▎                                    | 2/12 [01:08<06:08, 36.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 27 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▎                                    | 2/12 [01:17<06:29, 38.99s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "\n",
    "# from transformers import LLamaTokenizer, LLamaForCausalLM, GenerationConfig\n",
    "from transformers import LLaMAForCausalLM, LLaMATokenizer, GenerationConfig\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "def inference(default_model, \n",
    "              experiment_name,\n",
    "              test_dataset = 't2c_concode/t2c_answers.json',\n",
    "              fn_answers = \"/root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json\"\n",
    "             ):\n",
    "    # load everything\n",
    "    BASE_MODEL = default_model\n",
    "    LORA_WEIGHTS = experiment_name #\"tloen/alpaca-lora-7b\"\n",
    "\n",
    "    model = LLaMAForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        LORA_WEIGHTS,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    tokenizer = LLaMATokenizer.from_pretrained(BASE_MODEL)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    # not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # def generate_prompt(instruction, input):\n",
    "    #     return f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "\n",
    "    def generate_test_prompt(data_point, train = False):\n",
    "        # To decrease expectations of results :)\n",
    "        assert train == False\n",
    "        # sorry about the formatting disaster gotta move fast\n",
    "        if data_point[\"input\"]:\n",
    "            return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    ### Instruction:\n",
    "    {data_point[\"instruction\"]}\n",
    "\n",
    "    ### Input:\n",
    "    {data_point[\"input\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"output\"] if train else ''}\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    ### Instruction:\n",
    "    {data_point[\"instruction\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"output\"] if train else ''}\"\"\"\n",
    "\n",
    "\n",
    "    lst = json.load(open(test_dataset, 'rb'))\n",
    "    inputs = lst# [lst[0]]\n",
    "    # instruction = 'Combine the question and answer into an image caption as succinctly as possible. Be sure to include the phrase \"a photo of\". Do not draw false conclusions.'\n",
    "    # inputs = ['Is this a baseball game? yes', 'Is this a baseball game? no']\n",
    "    prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "    print(prompts[0])\n",
    "    prompts = np.array(prompts)\n",
    "    batch_size = 9\n",
    "    res_list = []\n",
    "    for ind in tqdm.tqdm(range(math.ceil(len(prompts)/batch_size))):\n",
    "        current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "        print(ind*batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "\n",
    "        tokenized_inputs = tokenizer(list(current_prompts), \n",
    "                                     padding=True, \n",
    "                                     truncation=True, \n",
    "                                     return_tensors=\"pt\"\n",
    "                                    ).to('cuda')\n",
    "\n",
    "        generation_config = GenerationConfig(max_new_tokens=128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            full_output = model.generate(\n",
    "                **tokenized_inputs,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "\n",
    "        res_list.extend(tokenizer.batch_decode(full_output, skip_special_tokens=False))\n",
    "    #     print(tokenizer.batch_decode(full_output, skip_special_tokens=False)[-1])\n",
    "\n",
    "\n",
    "        def preprocess(s, verbose = False):\n",
    "            s = s.split('### Response:\\n')[-1]\n",
    "            s = s.replace('\\n', '  ')\n",
    "            s = s.replace('<unk>', \" \")\n",
    "            s = ' '.join(s.split(' ')[:100])\n",
    "            while '  ' in s:\n",
    "                s = s.replace('  ', ' ')\n",
    "\n",
    "            if len(s) > 0 and s[0] == ' ':\n",
    "                s = s[1:]\n",
    "            \n",
    "            if verbose:\n",
    "                print(s)\n",
    "            \n",
    "            return s\n",
    "\n",
    "\n",
    "    predict_list = []\n",
    "    for s in tqdm.tqdm(res_list):\n",
    "        predict_list.append(preprocess(s))\n",
    "\n",
    "    from datetime import datetime\n",
    "    output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "    current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "    print(\"Save results at file\", current_pred_name)\n",
    "\n",
    "    with open(current_pred_name, 'w+') as f:\n",
    "        f.write('\\n'.join(predict_list))\n",
    "    #     f.write('\\n')\n",
    "\n",
    "    command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a {fn_answers} -p {current_pred_name}\"\n",
    "    result = subprocess.run(command.split(' '), \n",
    "                            stderr=subprocess.PIPE)\n",
    "    result.stderr\n",
    "    results = result.stderr.decode()\n",
    "    print(results)\n",
    "    \n",
    "    return {\"metric_string\": results,\n",
    "            \"prompts_list\": prompts,\n",
    "            \"res_list\": res_list,\n",
    "            \"predict_list\": predict_list\n",
    "           }\n",
    "\n",
    "inference(default_model = \"decapoda-research/llama-7b-hf\", \n",
    "          experiment_name = \"t2c_concode_220428_v6\"\n",
    "         )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf3bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"t2c_concode_220428_v6\"\n",
    "# experiment_name = \"t2c_concode_220428_v4\"\n",
    "# experiment_name = \"t2c_concode_220428\"\n",
    "# experiment_name = \"tloen/alpaca-lora-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd3a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "\n",
    "# from transformers import LLamaTokenizer, LLamaForCausalLM, GenerationConfig\n",
    "from transformers import LLaMAForCausalLM, LLaMATokenizer, GenerationConfig\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e46f2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'lora-alpaca/checkpoint-30': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah lora-alpaca/checkpoint-30\n",
    "# experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c81594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc153328c88422ea71044973579402c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LLaMAForCausalLM(\n",
       "      (model): LLaMAModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LLaMADecoderLayer(\n",
       "            (self_attn): LLaMAAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LLaMAMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): RMSNorm()\n",
       "            (post_attention_layernorm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "LORA_WEIGHTS = experiment_name #\"tloen/alpaca-lora-7b\"\n",
    "\n",
    "model = LLaMAForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    LORA_WEIGHTS,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(BASE_MODEL)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "tokenizer.padding_side = \"left\"\n",
    "# not sure how necessary this part is, not sure if tloen/alpaca-lora-7b was even trained with EOS and BOS tokens\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b3e1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.from_pretrained(\"decapoda-research/llama-7b-hf\", \"lora-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c52142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Generate java code\n",
      "actually walks the bag to make sure the count is correct and resets the running total \n",
      "\n",
      "### Input:\n",
      " Object _current \n",
      " int _total \n",
      " DefaultMapBag _parent \n",
      " Map _map \n",
      " int _mods \n",
      " Iterator _support \n",
      "\n",
      " boolean add \n",
      " boolean add \n",
      " Object next \n",
      " boolean containsAll \n",
      " boolean containsAll \n",
      " void clear \n",
      " boolean isEmpty \n",
      " boolean hasNext \n",
      " void remove \n",
      " boolean remove \n",
      " boolean remove \n",
      " Map getMap \n",
      " int modCount \n",
      " boolean contains \n",
      " Iterator iterator \n",
      " boolean removeAll \n",
      " int size \n",
      " boolean addAll \n",
      " int hashCode \n",
      " boolean equals \n",
      " Object[] toArray \n",
      " Object[] toArray \n",
      " Set uniqueSet \n",
      " void setMap \n",
      " String toString \n",
      " int getCount \n",
      " List extractList \n",
      " boolean retainAll \n",
      " boolean retainAll\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▋                                        | 1/12 [00:19<03:38, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▎                                    | 2/12 [01:01<05:25, 32.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 27 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████                                 | 3/12 [01:20<03:59, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 36 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 4/12 [01:41<03:12, 24.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 45 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████▎                         | 5/12 [02:33<03:59, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 54 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 6/12 [03:17<03:46, 37.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 63 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████████████████████████▋                  | 7/12 [03:51<03:02, 36.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 72 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 8/12 [04:20<02:16, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 81 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████           | 9/12 [04:47<01:35, 31.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 90 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████▊       | 10/12 [05:14<01:00, 30.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 99 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████▍   | 11/12 [05:52<00:32, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 108 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [05:57<00:00, 29.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "# def generate_prompt(instruction, input):\n",
    "#     return f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "\n",
    "def generate_test_prompt(data_point, train = False):\n",
    "    # To decrease expectations of results :)\n",
    "    assert train == False\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"] if train else ''}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"] if train else ''}\"\"\"\n",
    "\n",
    "\n",
    "lst = json.load(open('t2c_concode/t2c_answers.json', 'rb'))\n",
    "inputs = lst# [lst[0]]\n",
    "# instruction = 'Combine the question and answer into an image caption as succinctly as possible. Be sure to include the phrase \"a photo of\". Do not draw false conclusions.'\n",
    "# inputs = ['Is this a baseball game? yes', 'Is this a baseball game? no']\n",
    "prompts = [generate_test_prompt(inp) for inp in inputs]\n",
    "print(prompts[0])\n",
    "prompts = np.array(prompts)\n",
    "batch_size = 9\n",
    "res_list = []\n",
    "for ind in tqdm.tqdm(range(math.ceil(len(prompts)/batch_size))):\n",
    "    current_prompts = prompts[ind*batch_size: (ind+1)*batch_size]\n",
    "    print(ind*batch_size, (ind+1)*batch_size, len(current_prompts))\n",
    "    \n",
    "    tokenized_inputs = tokenizer(list(current_prompts), \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 return_tensors=\"pt\"\n",
    "                                ).to('cuda')\n",
    "\n",
    "    generation_config = GenerationConfig(max_new_tokens=128)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        full_output = model.generate(\n",
    "            **tokenized_inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "    res_list.extend(tokenizer.batch_decode(full_output, skip_special_tokens=False))\n",
    "#     print(tokenizer.batch_decode(full_output, skip_special_tokens=False)[-1])\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.split('### Response:\\n')[-1]\n",
    "    s = s.replace('\\n', '  ')\n",
    "    s = s.replace('<unk>', \" \")\n",
    "    s = ' '.join(s.split(' ')[:100])\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "    \n",
    "    if len(s) > 0 and s[0] == ' ':\n",
    "        s = s[1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "for s in tqdm.tqdm(res_list):\n",
    "    predict_list.append(preprocess(s))\n",
    "    \n",
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)\n",
    "\n",
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')\n",
    "\n",
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d67f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44275083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 29698.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int function ( ) { int loc0 \n",
      "void function ( boolean arg0 ) { fStatementsRecovery = arg0 ; } \n",
      "boolean function ( ) { return true ; } \n",
      "String function ( ) { return getIdentitiesZNode ( ) ; } \n",
      "int function ( final C \n",
      "void function ( Region arg0 ) { if ( fTagPosition == null ) { fTagPosition = arg0 ; } else { fTagPosition. setRegion ( arg0 ) ; } }\n",
      "double function ( double arg0, double... arg1 ) { return factory. solve ( arg0, arg1 ) ; } \n",
      "boolean function ( ) { return hasEscapeGenerationBug ( ) ; } \n",
      "\n",
      "byte [ ] function ( Drawable arg0 ) { InputStream loc0 = getInputStreamFromUrl ( arg0. createInputStream ( ) ) ; byte [ ] loc1 = new byte [ loc0. getByteCount ( ) ] ; loc0. read ( loc1, 0, loc0. getByteCount ( ) ) ; return loc1 ; } \n",
      "void function ( ) { migrationDone = false ; } \n",
      "void function ( int arg0, int arg1 ) { System. out. println ( \"srini_string\" + arg0 + \"srini_string\" + arg1 ) ; } \n",
      "Dialog function ( Context arg0 \n",
      "BaseColor function ( ) { return strokeColor ; } \n",
      "void function ( ) { when ( this. securityConfiguration ( ). getApplicationPolicy ( ) ) { String loc0 = this. securityConfiguration ( ). getApplicationPolicy ( ) ; assertNotNull ( loc0 ) ; } } \n",
      "void function ( Predicate [ ] arg0, int arg1 ) { int loc0 = arg1 ; while ( loc0 > 0 ) { Predicate [ ] loc1 = Arrays. asList ( arg0 ). remove ( 0 ) ; arg0 [ 0 ] = loc1 ; arg0 [ loc0 - 1 ] = null ; arg0 [ loc0 ] = loc1 ; loc0 -- ; } }\n",
      "int function ( ) { setMarcador ( 0 ) ; return this. marcador ; } \n",
      "IJavaElement function ( final IStructuredSelection arg0 \n",
      "LoadPlan function ( EntityManager arg0, String arg1, Collection<Query<?>> arg2, Collection<Query<?>> arg3 ) { return buildLoadPlan ( arg0, arg1, arg2, arg3, LoadPlan.LOAD_ROOT_COLLECTION ) ; }\n",
      "double function ( int arg0, double arg1 ) { MathVector loc0 = new MathVector ( arg1 ) ; double loc1 = BodyFactory. get_body_data ( arg0 ). pos_get_length ( loc0 ) ; return ( loc1 ) ; } \n",
      "void function ( String arg0 ) { this. catalogName = arg0 ; } \n",
      "Iterator < T > function ( String arg0 ) { for ( ColumnDescriptor loc0 : columnsByName. get ( arg0 ) ) { return loc0. columns. iterator ( ) ; } return null ; } \n",
      "void function ( ) { boolean loc0 = syncResult. getIsSoftError ( ) ; if ( loc0 ) { \n",
      "IMember function ( IHandle arg0 ) { return ( IMember ) TypeFactory. getInstance ( fOldType ). createInstance ( arg0. toReference ( ) ) ; } \n",
      "int function ( ) { return codec. getSampleDuration ( ) ; } \n",
      "void function ( ) { double loc0 = dist. getDistributionCurve ( ) ; int loc1 = size ; int loc2 = Math \n",
      "List function ( ) { return list ; } \n",
      "\n",
      "String function ( ) { return uri. getUri ( ). toString ( ) ; } \n",
      "Getcontentlanguage function ( ) { return getContentLanguage ( ) ; } \n",
      "boolean function ( ) { return this. coalescing ; } \n",
      "int function ( ) { return available + leased + pending ; } \n",
      "void function ( String arg0, String arg1 ) { logger ( error ( ), arg0 ). warn ( arg1 ) ; } \n",
      "Element function ( Node arg0, String arg1 ) { Element loc0 = getFirstChild ( arg0, arg1 ) ; if ( loc0 == null ) { return null ; } return loc0 ; }\n",
      "String function ( ) { return fcLink ; } \n",
      "\n",
      "void function ( int arg0 ) { this. i = arg0 ; } \n",
      "SolvableGroebnerBase < C > function ( ) { return sbb ; } \n",
      "void function ( boolean arg0 ) { fResolveBindings = arg0 ; } \n",
      "void function ( KeyEvent arg0 ) { Event loc0 = createClickEvent ( arg0. getCode ( ), arg0. getLocation ( ) ) ; EventManager. fireEvent ( arg0. getTarget ( ), loc0. getEventType ( ), loc0. getTarget ( ) ) ; } \n",
      "int function ( int arg0 ) { int loc0 = Math. abs ( arg0 ) ; return loc0 == 0? 1 : ( loc0 % 2 ) ; } \n",
      "int function ( Object arg0, Object arg1 ) { int loc0 = 0 ; while ( loc0 == 0 ) { if ( arg0 == null ) { return 0 ; } if ( arg1 == null ) { return 1 ; } loc0 = ( arg0. class. equals ( arg1. class ) ) ; if ( loc0 == 0 ) { loc0 = ( arg0. hashCode ( ) == arg1. hashCode ( ) ) ; } if ( loc0!= 0 ) { return loc0 ; } } }\n",
      "void function ( boolean [ ] arg0 ) { this. probes = arg0 ; } \n",
      "ProvidedPortID function ( QName arg0 ) { return new ProvidedPortID ( arg0 ) ; } \n",
      "void function ( ISourceFileLocator arg0 ) { delegates. add ( arg0 ) ; } \n",
      "UnivPowerSeries < C > function ( java. math. BigInteger arg0 ) { UnivPowerSeries < C > loc0 = getZERO ( ) ; for ( int loc1 = 0 ; loc1 < arg0. length ( ) ; loc1 ++ ) { loc0 = loc0. set ( arg0. getInt ( loc1 ), coFac. getRing ( ) ) ; } return loc0 ; } \n",
      "void function ( IShell arg0 ) { Platform.runLater ( new Runnable () { public void ( ) { arg0. getDisplay ( ). flush ( ) ; } } ) ; } \n",
      "Limit function ( String arg0, String... arg1 ) { Limit loc0 = new Limit ( arg0, arg1 ) ; limits. add ( loc0 ) ; return loc0 ; } \n",
      "void function ( ) { char loc0 = ch ; hasNext = false ; } \n",
      "void function ( ) { try { JAXBContext context = JAXBContext. create \n",
      "String function ( String arg0 ) { return \"srini_string\" + arg0 + \"srini_string\" ; } \n",
      "void function ( boolean arg0 ) { this. useCriterion4 = arg0 ; } \n",
      "void function ( ClassDoc arg0, String arg1 ) { Set < ClassDoc > loc0 = cmap. get ( arg0 ) ; if ( loc0 == null ) { loc0 = new HashSet < ClassDoc > ( ) ; cmap. put ( arg0, loc0 ) ; } loc0. add ( new ClassDoc ( arg0. name ( ), arg0. full_name ( ), arg0. source_file ( ), arg0 ) )\n",
      "Property function ( ) { return property ; } \n",
      "void function ( final double arg0 ) { if ( lookupEnabled ) { myNode. lookup ( arg0 ) ; } } \n",
      "double [ ] function ( ) { double [ ] loc0 = new double [ receivedMsg ] ; for ( int loc1 = 0 ; loc1 < receivedMsg ; loc1 ++ ) { loc0 [ loc1 ] = event. receivedMessages [ loc1 ] ; } return loc0 ; } \n",
      "void function ( java. lang.Throwable arg0, String arg1 ) { org. log. error ( arg1, arg0 ) ; } \n",
      "Item [ ] function ( ) { return ( Item [ ] ) items ; } \n",
      "String function ( ) { return name ; } \n",
      "Float function ( ) { Float loc0 = ( Float ) memory. get ( BOOKMARK_TREE ) ; if ( loc0 == null ) { loc0 = Float. parseFloat ( pageSize. getWidth ( ) ) ; memory. put ( BOOKMARK_TREE, loc0 ) ; } return loc0 ; }\n",
      "GenVector < C > function ( ) { return ZERO ; } \n",
      "void function ( int arg0 ) \n",
      "void function ( final String arg0, final HttpRequest arg1, final HttpResponse arg2 ) { } \n",
      "void function ( String arg0, String [ ] arg1, Map < String, Object > arg2 ) { if ( arg1!= null && arg1. length > 0 ) { for ( String loc0 : arg1 ) { writeParameter ( arg0, loc0, \n",
      "List < String > function ( File arg0 ) { return getNamespaces ( arg0, null ) ; } \n",
      "void function ( List < Integer > arg0 ) { for ( Integer loc0 : arg0 ) { this. taskIdsQueue. add ( loc0 ) ; } notEmpty. signal ( ) ; } \n",
      "void function ( \n",
      "String function ( Arguments arg0, String arg1 ) { Label loc0 = Label. parse ( arg1, true ) ; if ( loc0 == null ) { return null ; } return loc0. getLabelFor ( arg0. agentId ( ), arg0. jobLabel ( ) ) ; }\n",
      "void function ( Object arg0 ) { INSTANCE. set ( arg0 ) ; } \n",
      "long function ( ) { return ours ; } \n",
      "GenMatrix < C > function ( int arg0 ) { return GenMatrix < C >. fromLocal ( new GenMatrix < C > ( arg0, blocksize, blocksize, ZERO ) ) ; } \n",
      "void function ( String arg0, String arg1 ) { TestUtil. assertClassAvailable ( arg0, arg1, TestUtil. classLocation ( arg1 ) ) ; } \n",
      "void function ( ) { if ( mapper == null ) { mapper = new HashMap < String, BaseFontParameters ( ) > ( ) ; } String loc0 = \"srini_string \n",
      "void function ( String arg0, Object arg1 ) { log ( arg0, arg1, null ) ; } \n",
      "void function ( BigInteger arg0 ) { this. nresults = arg0 ; } \n",
      "void \n",
      "boolean function ( IWidget arg0 ) { return ui. getValue ( locator. lookup ( arg0 ), 0 ) == 1 ; } \n",
      "VectorXZ function ( double arg0 ) { return Z_UNIT. ctrlShift ( arg0 ) ; } \n",
      "int function ( InputStream arg0, int arg1, int arg2 ) { int loc0 = 0 ; while ( loc0 < arg2 ) { int loc1 = ( ( int ) arg0. read ( arg0, loc0 ) ) \n",
      "void function ( ) { if ( iPredicates [ ] [ ]. evaluates ( ) ) { execute ( ) ; } } \n",
      "int function ( ) { int loc0 = 0 ; while ( ( loc0 < 1000 ) && ( inputFile. isFile ( ) ) ) { bytes. add ( readBytes ( inputFile ) ) ; loc0 ++ ; } return bytes. size ( ) ; }\n",
      "PolynomialFunction function ( double arg0 ) { return polynomials [ arg0 - 1 ] ; } \n",
      "void function ( ) { throw new RuntimeException ( \"srini_string\" ) ; } \n",
      "Atom function ( ) { return this. type ; } \n",
      "double function ( ) { return mu ; } \n",
      "DoubleListIterator function ( ListIterator arg0 ) { if ( arg0 == null ) { return null ; } return new DoubleListIterator ( arg0 ) ; } \n",
      "\n",
      "void function ( Class <? > arg0, Parser arg1 ) { if ( parsers. containsKey ( arg0 ) ) { parsers. put ( arg0, arg1 ) ; } else { parsers. put ( arg0, arg1 ) ; } }\n",
      "List < T > function ( Collection < T > arg0 ) { List < T > loc0 = new ArrayList < T > ( arg0 ) ; return Collections. unmodifiableList ( loc0 ) ; } \n",
      "int function ( ) { return kdcConfig. getTcpPort ( ) ; } \n",
      "KeyStroke function ( char arg0 ) { return get ( arg0, null ) ; } \n",
      "String function ( String arg0 ) { String loc0 = arg0 ; String loc1 = loc0. replace ('', '_' ) ; return loc1 ; } \n",
      "int function ( ModInteger arg0 ) { return ring. MIREM ( ring. MIREM ( arg0. getVal ( ), ring. MIREM ( ring. getSymmetricVal ( ), arg0. getSymmetricVal ( ) ) ) ) ; } \n",
      "AccountNumber32 function ( PartyIdentification32 arg0 ) { return AccountNumber32. odf ( bicRegex. match ( arg0. getBic ( ) ) ) ; } \n",
      "Unmarshaller function ( ) { return JAXB_CONTEXT. createUnmarshaller ( JAXB_CONTEXT. createXMLStreamReader ( new StringReader ( DEFAULT_NAMESPACE_URI ) ) ) ; } \n",
      "void function ( byte [ ] arg0, int arg1, int arg2 ) { int loc0 = arg2 % 3 ; while ( loc0 > 0 ) { arg0 [ arg1 ] = encodingTable [ arg0 [ arg1 ] ] ; arg0 [ arg1 + 1 ] = encodingTable [ arg0 [ arg1 + 1 ] ] ; arg0 [ arg1 +\n",
      "String function ( int arg0, String arg1 ) { return getMessage ( arg0, arg1, true ) ; } \n",
      "void function ( PropertyChangeListener arg0 ) { pcsDelegate. addPropertyChangeListener ( arg0 ) ; } \n",
      "void function ( ) { if ( _constructor == null ) { Class <? > loc0 = Class. forName ( \"srini_string\" ) ; if ( loc0!= null ) { try { _constructor = loc0. getDeclaredConstructor ( ) ) ; } catch ( SecurityException loc1 ) { loc1. printStackTrace ( ) ; } } } } \n",
      "int function ( short arg0 ) { return asInt ( ( arg0 & 0xffff ) >>> 8 ) | ( arg0 & 0xffff ) << 8 ; }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.split('### Response:\\n')[-1]\n",
    "    s = s.replace('\\n', '  ')\n",
    "    s = s.replace('<unk>', \" \")\n",
    "    s = ' '.join(s.split(' ')[:100])\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "    \n",
    "    if len(s) > 0 and s[0] == ' ':\n",
    "        s = s[1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "for s in tqdm.tqdm(res_list):\n",
    "    predict_list.append(preprocess(s))\n",
    "    \n",
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)\n",
    "\n",
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')\n",
    "\n",
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855131b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/alpaca-lora/t2c_concode/results/2023-04-29-18_08_01.txt\n",
      "INFO:__main__:BLEU: 19.67, EM: 10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)\n",
    "\n",
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')\n",
    "\n",
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1a69f",
   "metadata": {},
   "source": [
    "# len(res_list)\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a302c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = s.split('### Response:\\n')[-1]\n",
    "    s = s.replace('\\n', '  ')\n",
    "    s = s.replace('<unk>', \" \")\n",
    "    s = ' '.join(s.split(' ')[:100])\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "    \n",
    "    if len(s) > 0 and s[0] == ' ':\n",
    "        s = s[1:]\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "for s in tqdm.tqdm(res_list):\n",
    "    predict_list.append(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([i.split(\"### Response:\\n\")[-1]=='' for i in prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9221dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst[0])\n",
    "print(prompts[0])\n",
    "print(predict_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2d1267f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Integer function ( ) { return null == intervalQualifier ? null : intervalQualifier . getFractionalSecondPrecisionPreservingDefault ( ) ; }'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[8]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1068fa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Integer function ( ) { return precision ; } '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f7865a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'byte [ ] function ( Drawable arg0 ) { return bitmapToByte ( drawableToBitmap ( arg0 ) ) ; } '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "output_filename = str(datetime.now()).split('.')[0].replace(' ', '-').replace(':', '_')+'.txt'\n",
    "current_pred_name = \"/root/alpaca-lora/t2c_concode/results/%s\"%output_filename\n",
    "print(current_pred_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_pred_name, 'w+') as f:\n",
    "    f.write('\\n'.join(predict_list))\n",
    "#     f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112df6f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_pred_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_pred_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(command\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m      4\u001b[0m                         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[1;32m      5\u001b[0m result\u001b[38;5;241m.\u001b[39mstderr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_pred_name' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7edb81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 53.9, EM: 21.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = f\"python /root/CodeXGLUE/Text-Code/text-to-code/evaluator/evaluator.py -a /root/CodeXGLUE/Text-Code/text-to-code/evaluator/answers.json -p {current_pred_name}\"\n",
    "result = subprocess.run(command.split(' '), \n",
    "                        stderr=subprocess.PIPE)\n",
    "result.stderr\n",
    "results = result.stderr.decode()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee0662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 0.29, EM: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3199dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BLEU: 0.27, EM: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a753ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4b7555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 23.70 GiB total capacity; 13.80 GiB already allocated; 2.91 GiB free; 19.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig(max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m     full_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(full_output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:731\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 731\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1406\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1401\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1402\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1403\u001b[0m         )\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2201\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2201\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2209\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:770\u001b[0m, in \u001b[0;36mLLaMAForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    767\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    782\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:619\u001b[0m, in \u001b[0;36mLLaMAModel.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    613\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    614\u001b[0m         hidden_states,\n\u001b[1;32m    615\u001b[0m         attention_mask,\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:316\u001b[0m, in \u001b[0;36mLLaMADecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    313\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:253\u001b[0m, in \u001b[0;36mLLaMAAttention.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    250\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(attn_weights, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin))\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    254\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1845\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 23.70 GiB total capacity; 13.80 GiB already allocated; 2.91 GiB free; 19.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompts = [generate_prompt(instruction=inp['instruction'], \n",
    "#                            input=inp['input']) \n",
    "#            for inp in inputs\n",
    "#           ]\n",
    "tokenized_inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_output = model.generate(\n",
    "        **tokenized_inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "print(tokenizer.batch_decode(full_output, skip_special_tokens=False)[-1])\n",
    "\n",
    "#     # same tokenized input, but we index just into the parts for the second input\n",
    "#     single_output = model.generate(\n",
    "#         **{k: v[1:] for k, v in tokenized_inputs.items()},\n",
    "#         generation_config=generation_config\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3bd6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results  t2c_answers.json  t2c_dev.json  t2c_test.json\tt2c_train.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls t2c_concode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff57237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t2c_answers.json</th>\n",
       "      <th>t2c_train.json</th>\n",
       "      <th>t2c_test.json</th>\n",
       "      <th>t2c_dev.json</th>\n",
       "      <th>t2c_1000train.json</th>\n",
       "      <th>t2c_10000train.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t2c_answers.json</th>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_train.json</th>\n",
       "      <td>10</td>\n",
       "      <td>71262</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>981</td>\n",
       "      <td>9138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_test.json</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_dev.json</th>\n",
       "      <td>99</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>1935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_1000train.json</th>\n",
       "      <td>0</td>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>981</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2c_10000train.json</th>\n",
       "      <td>0</td>\n",
       "      <td>9138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "      <td>9138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     t2c_answers.json  t2c_train.json  t2c_test.json   \n",
       "t2c_answers.json                   99              10              0  \\\n",
       "t2c_train.json                     10           71262              0   \n",
       "t2c_test.json                       0               0              1   \n",
       "t2c_dev.json                       99             114              0   \n",
       "t2c_1000train.json                  0             981              0   \n",
       "t2c_10000train.json                 0            9138              0   \n",
       "\n",
       "                     t2c_dev.json  t2c_1000train.json  t2c_10000train.json  \n",
       "t2c_answers.json               99                   0                    0  \n",
       "t2c_train.json                114                 981                 9138  \n",
       "t2c_test.json                   0                   0                    0  \n",
       "t2c_dev.json                 1935                   0                    0  \n",
       "t2c_1000train.json              0                 981                  225  \n",
       "t2c_10000train.json             0                 225                 9138  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dct = {}\n",
    "for filename in os.listdir(\"t2c_concode\"):\n",
    "    if filename.endswith('.json'):\n",
    "        dct[filename] = json.load(open(f\"t2c_concode/{filename}\"))\n",
    "    \n",
    "df = {}\n",
    "keys = list(dct.keys())\n",
    "for key1 in keys:\n",
    "    df[key1] = {}\n",
    "    for key2 in keys:\n",
    "        lst1 = [i['output'] for i in dct[key2]]\n",
    "        lst2 = [i['output'] for i in dct[key1]]\n",
    "\n",
    "        df[key1][key2] = len(set(lst1)&set(lst2))\n",
    "\n",
    "pd.DataFrame(df)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b3ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dct.keys()\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e1445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "lst = set([i['output'] for i in dct['t2c_dev.json']])\n",
    "\n",
    "lst_new = [i for i in dct['t2c_train.json'] if i['output'] not in lst]\n",
    "lst_new = shuffle(lst_new)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d2bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(lst_new, open(\"t2c_concode/t2c_10000train.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f002ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(lst_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b71cc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcafc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
