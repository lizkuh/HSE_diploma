{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85712eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM as LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer as LLaMATokenizer\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from EvaluateTestSet import EvaluateTestSet\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c5141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = \"/root/data/t2c_train.json\"\n",
    "EXPERIMENTS_PATH = \"/root/experiments/\"\n",
    "experiment_config = {\n",
    "    \"experiment_name\": \"t2c_concode_220428_v14\",\n",
    "    \"fn_train_dataset\":  \"/root/data/t2c_train.json\",\n",
    "    \n",
    "    \"default_model\": \"decapoda-research/llama-7b-hf\",\n",
    "\n",
    "    # Setting for A100 - For 3090 \n",
    "    \"MICRO_BATCH_SIZE\": 2,#4 # 8  # change to 4 for 3090\n",
    "    \"BATCH_SIZE\": 10,#32#128\n",
    "\n",
    "    \"EPOCHS\": 10,#20  # paper uses 3\n",
    "    \"LEARNING_RATE\":  2e-4,  # from the original paper\n",
    "    \"CUTOFF_LEN\": 256,#384, # 256 accounts for about 96% of the data\n",
    "    \"LORA_R\": 4,\n",
    "    \"LORA_ALPHA\": 16,#*2\n",
    "    \"LORA_DROPOUT\": 0.05,\n",
    "\n",
    "    # Trainer config\n",
    "    \"warmup_steps\": 200,\n",
    "    \"fp16\": True,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"save_strategy\": 'steps',\n",
    "    \"save_steps\": 100,\n",
    "    \"seed\": 42,\n",
    "    \"logging_strategy\": 'steps',\n",
    "    \"report_to\": 'tensorboard',\n",
    "    \"mlm\": False,\n",
    "\n",
    "    \"truncation\": True,\n",
    "    \"padding\": \"max_length\",\n",
    "\n",
    "    \"config_use_cache\": False,\n",
    "#     \"resume_from_checkpoint\": True,\n",
    "    \"resume_from_checkpoint\": False,\n",
    "    \n",
    "    \"bleu_batch_size\": 5\n",
    "}\n",
    "\n",
    "experiment_config[\"GRADIENT_ACCUMULATION_STEPS\"] = experiment_config[\"BATCH_SIZE\"] // experiment_config[\"MICRO_BATCH_SIZE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301e6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment_path = os.path.join(EXPERIMENTS_PATH, \n",
    "                                       experiment_config[\"experiment_name\"]\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd32581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/experiments/t2c_concode_220428_v14'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_experiment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30a86511",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_config['resume_from_checkpoint'] or True:\n",
    "    if not os.path.exists(current_experiment_path):\n",
    "        raise ValueError(\"this experment does not exist\")\n",
    "    else:\n",
    "        fn_config = current_experiment_path + \"/experiment_config.json\"\n",
    "        if json.load(open(fn_config, \"r\")) != experiment_config:\n",
    "            raise ValueError(\"At previous time there was different config\")\n",
    "else:\n",
    "    if os.path.exists(current_experiment_path):\n",
    "        input(\"this experiment already was done\")\n",
    "#         json.dump(experiment_config, open(current_experiment_path+\"/experiment_config.json\", \"w+\"))\n",
    "# json.dump(current_experiment_path, open(\"experiment_config.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e1ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(experiment_config, open(current_experiment_path+\"/experiment_config.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce53571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(current_experiment_path) and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f89efc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076eeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lora_model_and_tokenizer(default_model,\n",
    "                             LORA_R,\n",
    "                             LORA_ALPHA,\n",
    "                             LORA_DROPOUT\n",
    "                            ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    model = LLaMAForCausalLM.from_pretrained(\n",
    "    default_model,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = LLaMATokenizer.from_pretrained(\n",
    "        default_model, add_eos_token=True\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "class MyCustomCallback(TensorBoardCallback):\n",
    "    log_bleu_steps_factor = 5\n",
    "    bleu_generation_max_new_tokens = 30\n",
    "    bleu_fn_test_data = \"temp/t2c_answers.json\"\n",
    "    bleu_fn_etalon = \"temp/answers.json\"\n",
    "    log_step = 0\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        super().on_log(args, state, control, logs=logs, **kwargs)\n",
    "        print(\"kwargs\", len(kwargs), kwargs.keys())\n",
    "        if self.tb_writer is not None:\n",
    "            print(state)\n",
    "            print(state.global_step)\n",
    "            print(self.log_step)\n",
    "            if (self.log_step % self.log_bleu_steps_factor ==0):\n",
    "                model = kwargs['model']\n",
    "                tokenizer = kwargs['tokenizer']\n",
    "\n",
    "                generation_config = GenerationConfig(max_new_tokens = self.bleu_generation_max_new_tokens,\n",
    "                                                     min_new_tokens = 5\n",
    "                                                    )\n",
    "                evaluator = EvaluateTestSet(generation_config = generation_config,\n",
    "                                        fn_test_data = self.bleu_fn_test_data,\n",
    "                                        fn_etalon = self.bleu_fn_etalon\n",
    "                                       )\n",
    "\n",
    "                metric_res = evaluator.evaluate(model=model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                               )\n",
    "                print(metric_res)\n",
    "                for key, val in metric_res.items():\n",
    "                    self.tb_writer.add_scalar(key, val, state.global_step)\n",
    "                self.tb_writer.flush()\n",
    "            self.log_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8e3fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22db1874a6a477cb51b9c158a2ac2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_lora_model_and_tokenizer(default_model = experiment_config[\"default_model\"],\n",
    "                                                 LORA_R = experiment_config[\"LORA_R\"],\n",
    "                                                 LORA_ALPHA = experiment_config[\"LORA_ALPHA\"],\n",
    "                                                 LORA_DROPOUT = experiment_config[\"LORA_DROPOUT\"]\n",
    "                                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee43aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-6f0c4e89fb84a2e8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9226d995c9e3465e839d7e8103807867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", \n",
    "                    data_files = experiment_config[\"fn_train_dataset\"]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61548e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_config[\"save_total_limit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba14c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  32/1000 07:16 < 3:54:41, 0.07 it/s, Epoch 0.31/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.712400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.305200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.01, global_step=1, max_steps=1000, num_train_epochs=10, total_flos=101553222451200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.08s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 20126.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 1.5754303790798663e-06, 'brevity_penalty': 5.829466373086881e-05, 'ratio': 0.09302325581395349, 'translation_length': 76, 'reference_length': 817, 'precisions_0': 0.03896103896103896, 'precisions_1': 0.02127659574468085, 'precisions_2': 0.023809523809523808, 'precisions_3': 0.02702702702702703}\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.02, global_step=2, max_steps=1000, num_train_epochs=10, total_flos=203106444902400.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "2\n",
      "1\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.03, global_step=3, max_steps=1000, num_train_epochs=10, total_flos=304659667353600.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "3\n",
      "2\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.04, global_step=4, max_steps=1000, num_train_epochs=10, total_flos=406212889804800.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "4\n",
      "3\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.05, global_step=5, max_steps=1000, num_train_epochs=10, total_flos=507766112256000.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "5\n",
      "4\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.06, global_step=6, max_steps=1000, num_train_epochs=10, total_flos=609319334707200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "6\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.08s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 71049.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 2.1392821230327552e-06, 'brevity_penalty': 7.679592536518248e-05, 'ratio': 0.09547123623011015, 'translation_length': 78, 'reference_length': 817, 'precisions_0': 0.05063291139240506, 'precisions_1': 0.02040816326530612, 'precisions_2': 0.022727272727272728, 'precisions_3': 0.02564102564102564}\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.07, global_step=7, max_steps=1000, num_train_epochs=10, total_flos=710872557158400.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "7\n",
      "6\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.08, global_step=8, max_steps=1000, num_train_epochs=10, total_flos=812425779609600.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "8\n",
      "7\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.09, global_step=9, max_steps=1000, num_train_epochs=10, total_flos=913979002060800.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "9\n",
      "8\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.1, global_step=10, max_steps=1000, num_train_epochs=10, total_flos=1015532224512000.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "10\n",
      "9\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.11, global_step=11, max_steps=1000, num_train_epochs=10, total_flos=1117085446963200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "11\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.11s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 67869.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 2.1392821230327552e-06, 'brevity_penalty': 7.679592536518248e-05, 'ratio': 0.09547123623011015, 'translation_length': 78, 'reference_length': 817, 'precisions_0': 0.05063291139240506, 'precisions_1': 0.02040816326530612, 'precisions_2': 0.022727272727272728, 'precisions_3': 0.02564102564102564}\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.12, global_step=12, max_steps=1000, num_train_epochs=10, total_flos=1218638669414400.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "12\n",
      "11\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.13, global_step=13, max_steps=1000, num_train_epochs=10, total_flos=1320191891865600.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "13\n",
      "12\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.14, global_step=14, max_steps=1000, num_train_epochs=10, total_flos=1421745114316800.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "14\n",
      "13\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.15, global_step=15, max_steps=1000, num_train_epochs=10, total_flos=1523298336768000.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "15\n",
      "14\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.16, global_step=16, max_steps=1000, num_train_epochs=10, total_flos=1624851559219200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "16\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.09s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 18289.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 8.272684649263504e-07, 'brevity_penalty': 2.734321190450097e-05, 'ratio': 0.08690330477356181, 'translation_length': 71, 'reference_length': 817, 'precisions_0': 0.041666666666666664, 'precisions_1': 0.023809523809523808, 'precisions_2': 0.02702702702702703, 'precisions_3': 0.03125}\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.17, global_step=17, max_steps=1000, num_train_epochs=10, total_flos=1726404781670400.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "17\n",
      "16\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.18, global_step=18, max_steps=1000, num_train_epochs=10, total_flos=1827958004121600.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "18\n",
      "17\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.19, global_step=19, max_steps=1000, num_train_epochs=10, total_flos=1929511226572800.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "19\n",
      "18\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.2, global_step=20, max_steps=1000, num_train_epochs=10, total_flos=2031064449024000.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "20\n",
      "19\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.21, global_step=21, max_steps=1000, num_train_epochs=10, total_flos=2132617671475200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "21\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.07s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 69060.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 1.3947481499636897e-06, 'brevity_penalty': 5.051029741138152e-05, 'ratio': 0.09179926560587515, 'translation_length': 75, 'reference_length': 817, 'precisions_0': 0.039473684210526314, 'precisions_1': 0.021739130434782608, 'precisions_2': 0.024390243902439025, 'precisions_3': 0.027777777777777776}\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.22, global_step=22, max_steps=1000, num_train_epochs=10, total_flos=2234170893926400.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "22\n",
      "21\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.23, global_step=23, max_steps=1000, num_train_epochs=10, total_flos=2335724116377600.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "23\n",
      "22\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.24, global_step=24, max_steps=1000, num_train_epochs=10, total_flos=2437277338828800.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "24\n",
      "23\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.25, global_step=25, max_steps=1000, num_train_epochs=10, total_flos=2538830561280000.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "25\n",
      "24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.26, global_step=26, max_steps=1000, num_train_epochs=10, total_flos=2640383783731200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}, {'loss': 2.4088, 'learning_rate': 2.3e-05, 'epoch': 0.26, 'step': 26}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "26\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.06s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 40316.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 1.3947481499636897e-06, 'brevity_penalty': 5.051029741138152e-05, 'ratio': 0.09179926560587515, 'translation_length': 75, 'reference_length': 817, 'precisions_0': 0.039473684210526314, 'precisions_1': 0.021739130434782608, 'precisions_2': 0.024390243902439025, 'precisions_3': 0.027777777777777776}\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.27, global_step=27, max_steps=1000, num_train_epochs=10, total_flos=2741937006182400.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}, {'loss': 2.4088, 'learning_rate': 2.3e-05, 'epoch': 0.26, 'step': 26}, {'loss': 2.3306, 'learning_rate': 2.4e-05, 'epoch': 0.27, 'step': 27}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "27\n",
      "26\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.28, global_step=28, max_steps=1000, num_train_epochs=10, total_flos=2843490228633600.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}, {'loss': 2.4088, 'learning_rate': 2.3e-05, 'epoch': 0.26, 'step': 26}, {'loss': 2.3306, 'learning_rate': 2.4e-05, 'epoch': 0.27, 'step': 27}, {'loss': 2.5992, 'learning_rate': 2.5e-05, 'epoch': 0.28, 'step': 28}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "28\n",
      "27\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.29, global_step=29, max_steps=1000, num_train_epochs=10, total_flos=2945043451084800.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}, {'loss': 2.4088, 'learning_rate': 2.3e-05, 'epoch': 0.26, 'step': 26}, {'loss': 2.3306, 'learning_rate': 2.4e-05, 'epoch': 0.27, 'step': 27}, {'loss': 2.5992, 'learning_rate': 2.5e-05, 'epoch': 0.28, 'step': 28}, {'loss': 2.4488, 'learning_rate': 2.6e-05, 'epoch': 0.29, 'step': 29}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "29\n",
      "28\n",
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.3, global_step=30, max_steps=1000, num_train_epochs=10, total_flos=3046596673536000.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}, {'loss': 2.4088, 'learning_rate': 2.3e-05, 'epoch': 0.26, 'step': 26}, {'loss': 2.3306, 'learning_rate': 2.4e-05, 'epoch': 0.27, 'step': 27}, {'loss': 2.5992, 'learning_rate': 2.5e-05, 'epoch': 0.28, 'step': 28}, {'loss': 2.4488, 'learning_rate': 2.6e-05, 'epoch': 0.29, 'step': 29}, {'loss': 2.3052, 'learning_rate': 2.7e-05, 'epoch': 0.3, 'step': 30}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "30\n",
      "29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs 6 dict_keys(['model', 'tokenizer', 'optimizer', 'lr_scheduler', 'train_dataloader', 'eval_dataloader'])\n",
      "TrainerState(epoch=0.31, global_step=31, max_steps=1000, num_train_epochs=10, total_flos=3148149895987200.0, log_history=[{'loss': 2.4916, 'learning_rate': 0.0, 'epoch': 0.01, 'step': 1}, {'loss': 2.4456, 'learning_rate': 0.0, 'epoch': 0.02, 'step': 2}, {'loss': 2.4694, 'learning_rate': 0.0, 'epoch': 0.03, 'step': 3}, {'loss': 2.5534, 'learning_rate': 1e-06, 'epoch': 0.04, 'step': 4}, {'loss': 2.7124, 'learning_rate': 2e-06, 'epoch': 0.05, 'step': 5}, {'loss': 2.49, 'learning_rate': 3e-06, 'epoch': 0.06, 'step': 6}, {'loss': 2.6936, 'learning_rate': 4e-06, 'epoch': 0.07, 'step': 7}, {'loss': 2.5669, 'learning_rate': 5e-06, 'epoch': 0.08, 'step': 8}, {'loss': 2.7878, 'learning_rate': 6e-06, 'epoch': 0.09, 'step': 9}, {'loss': 2.5453, 'learning_rate': 7e-06, 'epoch': 0.1, 'step': 10}, {'loss': 2.472, 'learning_rate': 8e-06, 'epoch': 0.11, 'step': 11}, {'loss': 2.5162, 'learning_rate': 9e-06, 'epoch': 0.12, 'step': 12}, {'loss': 2.7374, 'learning_rate': 1e-05, 'epoch': 0.13, 'step': 13}, {'loss': 2.4645, 'learning_rate': 1.1e-05, 'epoch': 0.14, 'step': 14}, {'loss': 2.915, 'learning_rate': 1.2e-05, 'epoch': 0.15, 'step': 15}, {'loss': 2.4362, 'learning_rate': 1.3e-05, 'epoch': 0.16, 'step': 16}, {'loss': 2.48, 'learning_rate': 1.4e-05, 'epoch': 0.17, 'step': 17}, {'loss': 2.4338, 'learning_rate': 1.5e-05, 'epoch': 0.18, 'step': 18}, {'loss': 2.3935, 'learning_rate': 1.6e-05, 'epoch': 0.19, 'step': 19}, {'loss': 2.3283, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2, 'step': 20}, {'loss': 2.5063, 'learning_rate': 1.8e-05, 'epoch': 0.21, 'step': 21}, {'loss': 2.4365, 'learning_rate': 1.9e-05, 'epoch': 0.22, 'step': 22}, {'loss': 2.5194, 'learning_rate': 2e-05, 'epoch': 0.23, 'step': 23}, {'loss': 2.6716, 'learning_rate': 2.1000000000000002e-05, 'epoch': 0.24, 'step': 24}, {'loss': 2.4599, 'learning_rate': 2.2e-05, 'epoch': 0.25, 'step': 25}, {'loss': 2.4088, 'learning_rate': 2.3e-05, 'epoch': 0.26, 'step': 26}, {'loss': 2.3306, 'learning_rate': 2.4e-05, 'epoch': 0.27, 'step': 27}, {'loss': 2.5992, 'learning_rate': 2.5e-05, 'epoch': 0.28, 'step': 28}, {'loss': 2.4488, 'learning_rate': 2.6e-05, 'epoch': 0.29, 'step': 29}, {'loss': 2.3052, 'learning_rate': 2.7e-05, 'epoch': 0.3, 'step': 30}, {'loss': 2.6489, 'learning_rate': 2.8e-05, 'epoch': 0.31, 'step': 31}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n",
      "31\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.11s/it]\n",
      "100%|████████████████████████████████████████| 30/30 [00:00<00:00, 67396.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM': 0.0, 'BLEU': 1.3947481499636897e-06, 'brevity_penalty': 5.051029741138152e-05, 'ratio': 0.09179926560587515, 'translation_length': 75, 'reference_length': 817, 'precisions_0': 0.039473684210526314, 'precisions_1': 0.021739130434782608, 'precisions_2': 0.024390243902439025, 'precisions_3': 0.027777777777777776}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m experiment_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_use_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print(len(trainer.optimizer.state['found_inf_per_device']))\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresume_from_checkpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(current_experiment_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2006\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2006\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2439\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;66;03m# Maybe delete some older checkpoints.\u001b[39;00m\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2439\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rotate_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2954\u001b[0m, in \u001b[0;36mTrainer._rotate_checkpoints\u001b[0;34m(self, use_mtime, output_dir)\u001b[0m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m checkpoints_to_be_deleted:\n\u001b[1;32m   2953\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting older checkpoint [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] due to args.save_total_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2954\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:725\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(fd)):\n\u001b[0;32m--> 725\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(fd)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:679\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopfd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "data = data.shuffle().map(\n",
    "    lambda data_point: tokenizer(\n",
    "        generate_prompt(data_point),\n",
    "        truncation=experiment_config[\"truncation\"],\n",
    "        max_length=experiment_config[\"CUTOFF_LEN\"],\n",
    "        padding=experiment_config[\"padding\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=experiment_config[\"MICRO_BATCH_SIZE\"],\n",
    "        gradient_accumulation_steps=experiment_config[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
    "        warmup_steps=experiment_config[\"warmup_steps\"],\n",
    "        num_train_epochs=experiment_config[\"EPOCHS\"],\n",
    "        learning_rate=experiment_config[\"LEARNING_RATE\"],\n",
    "        fp16=experiment_config[\"fp16\"],\n",
    "        logging_steps=experiment_config[\"logging_steps\"],\n",
    "        output_dir=current_experiment_path,#\"lora-alpaca\",\n",
    "        save_total_limit=experiment_config[\"save_total_limit\"],\n",
    "        save_strategy = experiment_config[\"save_strategy\"],\n",
    "        save_steps = experiment_config[\"save_steps\"],\n",
    "        seed=experiment_config[\"seed\"],\n",
    "        logging_dir=current_experiment_path,\n",
    "        logging_strategy=experiment_config[\"logging_strategy\"],\n",
    "        report_to=experiment_config[\"report_to\"]\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                               mlm=experiment_config[\"mlm\"]\n",
    "                                                              ),\n",
    "    callbacks = [MyCustomCallback]\n",
    ")\n",
    "model.config.use_cache = experiment_config[\"config_use_cache\"]\n",
    "# print(len(trainer.optimizer.state['found_inf_per_device']))\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=experiment_config[\"resume_from_checkpoint\"])\n",
    "\n",
    "model.save_pretrained(current_experiment_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2206204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(current_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f23142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683312104.0230427   checkpoint-30\r\n",
      "1683312104.035074    checkpoint-31\r\n",
      "1683312337.4801078   events.out.tfevents.1683312104.8d048d63ed1a.6054.0\r\n",
      "1683312337.4871686   events.out.tfevents.1683312104.8d048d63ed1a.6054.2\r\n",
      "1683312517.1667607   events.out.tfevents.1683312337.8d048d63ed1a.7269.0\r\n",
      "1683312517.1768987   events.out.tfevents.1683312337.8d048d63ed1a.7269.2\r\n",
      "1683312553.3835888   events.out.tfevents.1683312517.8d048d63ed1a.7269.4\r\n",
      "1683312553.3904636   events.out.tfevents.1683312517.8d048d63ed1a.7269.6\r\n",
      "1683312918.7901428   events.out.tfevents.1683312553.8d048d63ed1a.7392.0\r\n",
      "1683312918.7971816   events.out.tfevents.1683312553.8d048d63ed1a.7392.2\r\n",
      "adapter_config.json  events.out.tfevents.1683312918.8d048d63ed1a.7560.0\r\n",
      "adapter_model.bin    events.out.tfevents.1683312918.8d048d63ed1a.7560.2\r\n"
     ]
    }
   ],
   "source": [
    "!ls {current_experiment_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549f588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
